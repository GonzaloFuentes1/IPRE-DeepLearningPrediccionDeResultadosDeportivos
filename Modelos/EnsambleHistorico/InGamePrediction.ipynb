{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5de692a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from balu3.fs.sel    import sfs, clean\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from scipy.stats import binom, betabinom\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from joblib import Parallel, delayed\n",
    "from tensorflow.keras.layers import Masking, LSTM, Dense\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "210ca305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para la LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b934cb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data historica\n",
    "\n",
    "dfHistorico1 = pd.read_csv(\"dataHistórica/resultados_pasados_premier.csv\")\n",
    "dfHistorico1 = dfHistorico1.drop(\"Unnamed: 0\",axis = 1)\n",
    "dfHistorico1 = dfHistorico1.drop([\"Position_last_year_home\", \"Position_last_year_away\",\n",
    "                                  \"Points_last_year_home\", \"Points_last_year_away\"],axis = 1) # Para hacer el merge con el segundo.\n",
    "\n",
    "dfHistorico2 = pd.read_csv(\"dataHistórica/restoDeLigas.csv\")\n",
    "dfHistorico2 = dfHistorico2.drop(\"Unnamed: 0\",axis = 1)\n",
    "\n",
    "dfHistorico2['Date'] = pd.to_datetime(dfHistorico2['Date'], dayfirst=True, errors='coerce')\n",
    "dfHistorico2['Date'] = dfHistorico2['Date'].dt.strftime('%Y-%m-%d')\n",
    "dfHistorico2 = dfHistorico2[dfHistorico2[\"Date\"] >\"2010-01-01\"] # Seleccion solo de partidos de 2010 en adelante (porque el otro dataframe tiene info desde ahí)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c07e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHistorico = pd.concat([dfHistorico1, dfHistorico2], axis=0)\n",
    "dfHistorico = dfHistorico.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1524e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data tipo evento\n",
    "\n",
    "dfEvent = pd.read_csv(\"dataWhoScored/WhoScoredTeamPerMatchSpatial4x3TimeDiv5.csv\")\n",
    "dfEvent = dfEvent.drop(\"Unnamed: 0\",axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a459684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajuste manual de equipos:\n",
    "# Los que no tengan match son un string vacio.\n",
    "\n",
    "histToEventNameV1 = {\n",
    "   \"Arsenal\": \"Arsenal\", \"Aston Villa\": \"Aston Villa\", \"Barnsley\": \"\", \"Birmingham City\": \"Birmingham\", \"Blackburn\": \"Blackburn\",\"Blackpool\": \"Blackpool\",\n",
    "   \"Bolton\" : \"Bolton\", \"Bournemouth\": \"Bournemouth\", \"Bradford City\": \"\", \"Brentford\": \"Brentford\", \"Brighton\": \"Brighton\", \"Burnley\": \"Burnley\", \"Cardiff City\": \"Cardiff\",\n",
    "   \"Charlton Ath\": \"\", \"Chelsea\": \"Chelsea\", \"Coventry City\": \"\", \"Crystal Palace\": \"Crystal Palace\", \"Derby County\":\"\", \"Everton\": \"Everton\", \"Fulham\": \"Fulham\",\n",
    "   \"Huddersfield\": \"Huddersfield\", \"Hull City\": \"Hull\", \"Ipswich Town\": \"\", \"Leeds United\": \"Leeds\", \"Leicester City\": \"Leicester\", \"Liverpool\": \"Liverpool\",\n",
    "   \"Manchester City\": \"Man City\", \"Manchester Utd\": \"Man Utd\", \"Middlesbrough\": \"Middlesbrough\", \"Newcastle Utd\": \"Newcastle\", \"Norwich City\": \"Norwich\",\n",
    "   \"Nott'ham Forest\": \"Nottingham Forest\", \"Oldham Athletic\": \"\", \"Portsmouth\": \"\", \"QPR\": \"QPR\", \"Reading\": \"Reading\", \"Sheffield Utd\": \"Sheff Utd\", \"Sheffield Weds\": \"\",\n",
    "   \"Southampton\": \"Southampton\", \"Stoke City\": \"Stoke\", \"Sunderland\": \"Sunderland\", \"Swansea City\": \"Swansea\", \"Swindon Town\":  \"\", \"Tottenham\": \"Tottenham\", \"Watford\":\"Watford\",\n",
    "   \"West Brom\": \"WBA\", \"West Ham\": \"West Ham\", \"Wigan Athletic\": \"Wigan\", \"Wimbledon\": \"\", \"Wolves\": \"Wolves\"\n",
    "}\n",
    "len(histToEventNameV1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16d91e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "histToEventName = {\n",
    "   \"Arsenal\": \"Arsenal\", \"Aston Villa\": \"Aston Villa\", \"Barnsley\": \"\", \"Birmingham City\": \"Birmingham\", \"Blackburn\": \"Blackburn\",\"Blackpool\": \"Blackpool\",\n",
    "   \"Bolton\" : \"Bolton\", \"Bournemouth\": \"Bournemouth\", \"Bradford City\": \"\", \"Brentford\": \"Brentford\", \"Brighton\": \"Brighton\", \"Burnley\": \"Burnley\", \"Cardiff City\": \"Cardiff\",\n",
    "   \"Charlton Ath\": \"\", \"Chelsea\": \"Chelsea\", \"Coventry City\": \"\", \"Crystal Palace\": \"Crystal Palace\", \"Derby County\":\"\", \"Everton\": \"Everton\", \"Fulham\": \"Fulham\",\n",
    "   \"Huddersfield\": \"Huddersfield\", \"Hull City\": \"Hull\", \"Ipswich Town\": \"\", \"Leeds United\": \"Leeds\", \"Leicester City\": \"Leicester\", \"Liverpool\": \"Liverpool\",\n",
    "   \"Manchester City\": \"Man City\", \"Manchester Utd\": \"Man Utd\", \"Middlesbrough\": \"Middlesbrough\", \"Newcastle Utd\": \"Newcastle\", \"Norwich City\": \"Norwich\",\n",
    "   \"Nott'ham Forest\": \"Nottingham Forest\", \"Oldham Athletic\": \"\", \"Portsmouth\": \"\", \"QPR\": \"QPR\", \"Reading\": \"Reading\", \"Sheffield Utd\": \"Sheff Utd\", \"Sheffield Weds\": \"\",\n",
    "   \"Southampton\": \"Southampton\", \"Stoke City\": \"Stoke\", \"Sunderland\": \"Sunderland\", \"Swansea City\": \"Swansea\", \"Swindon Town\":  \"\", \"Tottenham\": \"Tottenham\", \"Watford\":\"Watford\",\n",
    "   \"West Brom\": \"WBA\", \"West Ham\": \"West Ham\", \"Wigan Athletic\": \"Wigan\", \"Wimbledon\": \"\", \"Wolves\": \"Wolves\", #V1\n",
    "   \"Aachen\":\"\",\"Ajaccio\":\"AC Ajaccio\", \"Alaves\":\"Deportivo Alaves\", \"Almeria\": \"Almeria\", \"Amiens\":\"Amiens\",\"Angers\":\"Angers\", \"Arles\":\"Arles-Avignon\", \"Ascoli\":\"\",\n",
    "   \"Atalanta\":\"Atalanta\", \"Ath Bilbao\":\"Athletic Club\", \"Ath Madrid\":\"Atletico\", \"Augsburg\":\"Augsburg\", \"Auxerre\":\"Auxerre\", \"Barcelona\":\"Barcelona\", \"Bari\":\"Bari\",\n",
    "   \"Bastia\":\"SC Bastia\", \"Bayern Munich\":\"Bayern\", \"Benevento\":\"Benevento\", \"Betis\":\"Real Betis\", \"Bielefeld\":\"Arminia Bielefeld\", \"Bochum\": \"Bochum\", \"Bologna\":\"Bologna\",\n",
    "   \"Bordeaux\":\"Bordeaux\", \"Boulogne\": \"\", \"Braunschweig\": \"Eintracht Braunschweig\", \"Brescia\":\"Brescia\", \"Brest\":\"Brest\", \"Cadiz\":\"Cadiz\", \"Caen\": \"Caen\", \"Cagliari\":\"Cagliari\",\n",
    "   \"Carpi\":\"AC Carpi\", \"Catania\":\"Catania\", \"Celta\":\"Celta Vigo\", \"Cesena\": \"Cesena\", \"Chievo\":\"Chievo\", \"Clermont\":\"Clermont Foot\", \"Cordoba\": \"Cordoba\", \"Cottbus\":\"\",\n",
    "   \"Cremonese\":\"Cremonese\", \"Crotone\":\"Crotone\", \"Darmstadt\":\"Darmstadt\", \"Dijon\": \"Dijon\", \"Dortmund\":\"Borussia Dortmund\", \"Duisburg\":\"\", \"Eibar\":\"Eibar\", \"Ein Frankfurt\":\"Eintracht Frankfurt\",\n",
    "   \"Elche\": \"Elche\", \"Empoli\": \"Empoli\", \"Espanol\": \"Espanyol\", \"Evian Thonon Gaillard\": \"Evian\", \"FC Koln\": \"FC Koln\", \"Fiorentina\": \"Fiorentina\", \"Fortuna Dusseldorf\": \"Fortuna Duesseldorf\",\n",
    "   \"Freiburg\":\"Freiburg\", \"Frosinone\": \"Frosinone\", \"Genoa\": \"Genoa\", \"Getafe\": \"Getafe\", \"Gimnastic\":\"\", \"Girona\": \"Girona\", \"Granada\": \"Granada\", \"Grenoble\":\"\",\n",
    "   \"Greuther Furth\": \"Greuther Fuerth\", \"Guingamp\": \"Guingamp\", \"Hamburg\":\"Hamburg\", \"Hannover\":\"Hannover\", \"Hansa Rostock\":\"\", \"Heidenheim\":\"FC Heidenheim\", \"Hercules\":\"Hercules\",\n",
    "   \"Hertha\":\"Hertha Berlin\", \"Hoffenheim\":\"Hoffenheim\", \"Huesca\": \"SD Huesca\", \"Ingolstadt\": \"Ingolstadt\", \"Inter\": \"Inter\", \"Juventus\":\"Juventus\", \"Kaiserslautern\":\"Kaiserslautern\",\n",
    "   \"Karlsruhe\":\"\", \"La Coruna\": \"Deportivo\", \"Las Palmas\": \"Las Palmas\", \"Lazio\": \"Lazio\", \"Le Havre\": \"Le Havre\", \"Le Mans\":\"\", \"Lecce\": \"Lecce\", \"Leganes\":\"Leganes\", \"Lens\": \"Lens\",\n",
    "   \"Levante\": \"Levante\", \"Leverkusen\": \"Leverkusen\", \"Lille\":\"Lille\", \"Livorno\": \"Livorno\", \"Lorient\":\"Lorient\", \"Lyon\": \"Lyon\", \"M'gladbach\": \"Borussia M.Gladbach\", \"Mainz\": \"Mainz\",\n",
    "   \"Malaga\":\"Malaga\", \"Mallorca\":\"Mallorca\", \"Messina\": \"\", \"Metz\": \"Metz\", \"Milan\": \"AC Milan\", \"Monaco\": \"Monaco\", \"Montpellier\": \"Montpellier\", \"Monza\": \"Monza\", \"Murcia\":\"\",\n",
    "   \"Nancy\":\"Nancy\", \"Nantes\":\"Nantes\", \"Napoli\":\"Napoli\", \"Nice\":\"Nice\", \"Nimes\":\"Nimes\", \"Novara\": \"Novara\", \"Numancia\":\"\", \"Nurnberg\":\"Nuernberg\", \"Osasuna\": \"Osasuna\",\n",
    "   \"Paderborn\": \"Paderborn\", \"Palermo\": \"Palermo\", \"Paris SG\": \"PSG\", \"Parma\": \"Parma Calcio 1913\", \"Pescara\":\"Pescara\", \"RB Leipzig\": \"RBL\",\"Real Madrid\":\"Real Madrid\", \n",
    "   \"Recreativo\":\"\", \"Reggina\":\"\", \"Reims\": \"Reims\", \"Rennes\":\"Rennes\", \"Roma\":\"Roma\", \"Salernitana\": \"Salernitana\", \"Sampdoria\":\"Sampdoria\", \"Santander\": \"Racing Santander\",\n",
    "   \"Sassuolo\":\"Sassuolo\", \"Schalke 04\": \"Schalke\", \"Sedan\":\"\", \"Sevilla\":\"Sevilla\", \"Siena\": \"Siena\", \"Sochaux\": \"Sochaux\", \"Sociedad\": \"Real Sociedad\", \"Sp Gijon\":\"Sporting Gijon\",\n",
    "   \"Spal\":\"SPAL 2013\", \"Spezia\": \"Spezia\", \"St Etienne\":\"Saint-Etienne\", \"St Pauli\":\"St. Pauli\", \"Strasbourg\":\"Strasbourg\", \"Stuttgart\": \"Stuttgart\", \"Tenerife\":\"\", \"Torino\":\"Torino\",\n",
    "   \"Toulouse\":\"Toulouse\", \"Treviso\":\"\", \"Troyes\":\"Troyes\", \"Udinese\": \"Udinese\", \"Union Berlin\":\"Union Berlin\", \"Valencia\": \"Valencia\", \"Valenciennes\": \"Valenciennes\", \"Valladolid\": \"Real Valladolid\",\n",
    "   \"Vallecano\": \"Rayo Vallecano\", \"Venezia\":\"Venezia\", \"Verona\":\"Verona\", \"Villarreal\": \"Villarreal\", \"Werder Bremen\": \"Werder Bremen\", \"Wolfsburg\": \"Wolfsburg\", \"Xerez\": \"\", \"Zaragoza\": \"Real Zaragoza\"\n",
    "}\n",
    "len(histToEventName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34545e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para buscar nombres\n",
    "\n",
    "a = dfHistorico[\"Team_home\"].unique().tolist()\n",
    "a.sort()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7402154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para buscar nombres\n",
    "\n",
    "a = dfEvent[\"Team\"].unique().tolist()\n",
    "a.sort()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270a03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres de data histórica a nombres de tipo evento.\n",
    "\n",
    "dfHistorico[\"Team_home\"] = dfHistorico[\"Team_home\"].map(histToEventName)\n",
    "dfHistorico[\"Team_away\"] = dfHistorico[\"Team_away\"].map(histToEventName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee6ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprobamos que no esten datos vacios\n",
    "\n",
    "dfHistorico.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cf3e487",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_17576\\1744700893.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mstringHome\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfHistorico\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTeam_home\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mstringAway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfHistorico\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTeam_away\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfHistorico\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdfAux\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfEvent\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdfEvent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDate\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mdate\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mdfHome\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfAux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdfAux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTeam\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstringHome\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_prefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Home\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[0mdfAway\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfAux\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdfAux\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTeam\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mstringAway\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_prefix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Away\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3880\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3882\u001b[0m         \u001b[1;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3883\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3884\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3885\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3886\u001b[0m         \u001b[1;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3887\u001b[0m         \u001b[1;31m# We interpret tuples as collections only for non-MultiIndex\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3941\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3942\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3943\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3944\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3945\u001b[1;33m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3946\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Primero revisamos si estan todos los partidos compatibles, para que sea más robusta la solución\n",
    "\n",
    "indexHistorical = []\n",
    "for i in tqdm(range(len(dfHistorico))):\n",
    "    stringHome = dfHistorico.iloc[i].Team_home\n",
    "    stringAway = dfHistorico.iloc[i].Team_away\n",
    "    date = dfHistorico.iloc[i].Date\n",
    "\n",
    "    dfAux = dfEvent[dfEvent.Date == date]\n",
    "\n",
    "    dfHome = dfAux[dfAux.Team == stringHome].add_prefix(\"Home\")\n",
    "    dfAway = dfAux[dfAux.Team == stringAway].add_prefix(\"Away\")\n",
    "\n",
    "    # dfHome.reset_index(drop=True, inplace=True)\n",
    "    # dfAway.reset_index(drop=True, inplace=True)\n",
    "    if len(dfHome) + len(dfAway) == 2: \n",
    "        indexHistorical.append(i)\n",
    "    # concat = pd.concat([dfHome, dfAway], axis = 1)\n",
    "    # test.append(concat)\n",
    "    # Buscamos los partidos que tenemos información\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7a0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe de partidos a buscar\n",
    "\n",
    "dfHistoricoCompatible = dfHistorico.iloc[indexHistorical].reset_index(drop=True)\n",
    "dfHistoricoCompatible = dfHistoricoCompatible.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92f213d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHistoricoCompatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4867c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHistoricoCompatible.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b54072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# goals_succ = dfEvent.filter(regex='Goal_Successful_bin').columns.tolist()\n",
    "# goals_unsucc = dfEvent.filter(regex='Goal_Unsuccessful_bin').columns.tolist()\n",
    "# goals_succ.extend(goals_unsucc)\n",
    "# goals = goals_succ\n",
    "\n",
    "# intervals = [\"0-45\",\"45-90\"]\n",
    "# intervals = [\"0-15\",\"15-30\",\"30-45\",\"45-60\",\"60-75\",\"75-90\"]\n",
    "intervals = [\"0-5\",\"5-10\",\"10-15\",\"15-20\",\"20-25\",\"25-30\", \"30-35\",\"35-40\",\"40-45\",\n",
    "             \"45-50\",\"50-55\",\"55-60\",\"60-65\",\"65-70\",\"70-75\", \"75-80\",\"80-85\",\"85-90\"]\n",
    "\n",
    "Xdata = []\n",
    "ydata = []\n",
    "for i in tqdm(range(len(dfHistoricoCompatible))):\n",
    "    stringHome = dfHistoricoCompatible.iloc[i].Team_home\n",
    "    stringAway = dfHistoricoCompatible.iloc[i].Team_away\n",
    "    date = dfHistoricoCompatible.iloc[i].Date\n",
    "\n",
    "    # dfAux = dfEvent[dfEvent.Date == date].drop(goals, axis=1) #Test sin los goles\n",
    "    dfAux = dfEvent[dfEvent.Date == date]\n",
    "\n",
    "    dfHome = dfAux[dfAux.Team == stringHome].add_prefix(\"Home\")\n",
    "    dfAway = dfAux[dfAux.Team == stringAway].add_prefix(\"Away\")\n",
    "\n",
    "    # Dropear valores redundantes\n",
    "    dfHome = dfHome.drop([\"HomeTeam\",\"HomeDate\",\"HomeGoals\",\"Homegame_id\"], axis = 1)\n",
    "    dfAway = dfAway.drop([\"AwayTeam\",\"AwayDate\",\"AwayGoals\",\"Awaygame_id\"], axis = 1)\n",
    "\n",
    "    dfHome.reset_index(drop=True, inplace=True)\n",
    "    dfAway.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    concat = pd.concat([dfHome, dfAway], axis = 1).reset_index(drop=True)\n",
    "    x = []\n",
    "    if len(concat) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        for inter in intervals:\n",
    "            intervals_columns = concat.filter(regex=f'{inter}$').columns.tolist()\n",
    "            hist = dfHistoricoCompatible.iloc[i:i+1].drop([\"Date\", \"Team_home\", \"Team_away\", \"result\", \"Goals_home\", \"Goals_away\", \"Wk\"], axis=1).values\n",
    "            x.append(np.append(concat[intervals_columns].values[0], hist))\n",
    "        ydata.append(dfHistoricoCompatible.iloc[i:i+1][\"result\"].values[0])\n",
    "        x = np.array(x)\n",
    "        Xdata.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4357a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para guardar el dataset y no correrlo varias veces \n",
    "# np.save(\"Space4x3Time5.npy\", np.array(Xdata))\n",
    "# np.save(\"LabelsSpace4x3Time5.npy\", np.array(ydata))\n",
    "\n",
    "Xdata = np.load(\"Space4x3Time5.npy\")\n",
    "ydata = np.load(\"LabelsSpace4x3Time5.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e74a68",
   "metadata": {},
   "source": [
    "# MLP solo datos históricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09aaf8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dfHistoricoCompatible.drop([\"Date\", \"Team_home\", \"Team_away\", \"result\", \"Goals_home\", \"Goals_away\", \"Wk\"], axis=1).values\n",
    "y = dfHistoricoCompatible.result\n",
    "\n",
    "# y = to_categorical(y, num_classes=3)\n",
    "\n",
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "split = int(0.8 * X.shape[0])\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "# scaler = StandardScaler()\n",
    "\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b587bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_norm.shape[1],)),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(3, activation='softmax')\n",
    "])\n",
    "# Compilar el modelo\n",
    "with tf.device('GPU:0'): # Usar gpu\n",
    "    model.compile(optimizer=Adam(learning_rate=0.01), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, min_lr=10^(-6))\n",
    "\n",
    "history = model.fit(X_train_norm, y_train, verbose=1, epochs=100, batch_size=8,\n",
    "                    validation_data=(X_test_norm, y_test), callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8f7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = TabNetClassifier(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-3),\n",
    "    scheduler_params={\"step_size\":50,\n",
    "                        \"gamma\":0.9},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax', # \"sparsemax\" \"entmax\"\n",
    "    )\n",
    "    \n",
    "clf.fit(X_train, y_train,\n",
    "    eval_set=[(X_train, y_train), (X_test, y_test)], \n",
    "    eval_name=['train', 'valid'], \n",
    "    eval_metric=['accuracy'], \n",
    "    max_epochs=1000 , patience=30, \n",
    "    batch_size=28, drop_last=False)\n",
    "\n",
    "pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaed6b73",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1e882018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data\n",
    "X = np.array(Xdata)\n",
    "y = np.array(ydata)\n",
    "y = to_categorical(y, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2e42354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22209, 18, 1270)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f34e393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22209, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a29de717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "split = int(0.8 * X.shape[0])\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f83e49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin clean\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_reshaped = X_train.reshape(-1, X.shape[2])\n",
    "# X_train_normalized = scaler.fit_transform(X_train_reshaped)\n",
    "# X_train_normalized = X_train_normalized.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# X_test_reshaped = X_test.reshape(-1, X.shape[2])\n",
    "# X_test_normalized = scaler.transform(X_test_reshaped)\n",
    "# X_test_normalized = X_test_normalized.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "\n",
    "# Con clean \n",
    "X_train_reshaped = X_train.reshape(-1, X.shape[2])\n",
    "X_test_reshaped = X_test.reshape(-1, X.shape[2])\n",
    "\n",
    "sclean  = clean(X_train_reshaped)      # indices of selected features\n",
    "X_train_reshaped = deepcopy(X_train_reshaped[:,sclean])\n",
    "X_test_reshaped = deepcopy(X_test_reshaped[:,sclean])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_reshaped)\n",
    "X_test_normalized = scaler.transform(X_test_reshaped)\n",
    "\n",
    "X_train_normalized = X_train_normalized.reshape(X_train.shape[0], X_train.shape[1], len(sclean))\n",
    "X_test_normalized = X_test_normalized.reshape(X_test.shape[0], X_test.shape[1], len(sclean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a287a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17767, 18, 766)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59fc8894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "24e295fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo LSTM\n",
    "model = Sequential()\n",
    "l2_recurrent_parameter = 1\n",
    "l_2_kernel_regularizer= 1\n",
    "# Primera capa LSTM con Dropout y regularización L2\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(X_train_normalized.shape[1], X_train_normalized.shape[2]),\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=True,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=True,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=False,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compilación del modelo\n",
    "with tf.device('GPU:0'): # Usar gpu\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=\"models\\InGame\",\n",
    "    monitor='val_loss',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=35, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0000000001)\n",
    "\n",
    "history = model.fit(X_train_normalized, y_train, verbose=1, epochs=1000, batch_size=16,\n",
    "                    validation_data=(X_test_normalized, y_test), callbacks=[early_stopping, reduce_lr, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "60231ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-5\n",
      "139/139 [==============================] - 2s 6ms/step - loss: 7.5779 - accuracy: 0.4500\n",
      "5-10\n",
      "139/139 [==============================] - 1s 5ms/step - loss: 14.4842 - accuracy: 0.3041\n",
      "10-15\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 18.7771 - accuracy: 0.3041\n",
      "15-20\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 8.4986 - accuracy: 0.3294\n",
      "20-25\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 3.8799 - accuracy: 0.4705\n",
      "25-30\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 2.1658 - accuracy: 0.4932\n",
      "30-35\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 1.9287 - accuracy: 0.5331\n",
      "35-40\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 1.9486 - accuracy: 0.5626\n",
      "40-45\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 1.9266 - accuracy: 0.5946\n",
      "45-50\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 1.8273 - accuracy: 0.6290\n",
      "50-55\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 1.6892 - accuracy: 0.6646\n",
      "55-60\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 1.5182 - accuracy: 0.6956\n",
      "60-65\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 1.3670 - accuracy: 0.7242\n",
      "65-70\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 1.1829 - accuracy: 0.7614\n",
      "70-75\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 0.9704 - accuracy: 0.8012\n",
      "75-80\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 0.7683 - accuracy: 0.8390\n",
      "80-85\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 0.5194 - accuracy: 0.8928\n",
      "85-90\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 0.1072 - accuracy: 0.9926\n"
     ]
    }
   ],
   "source": [
    "intervals = [\"0-5\",\"5-10\",\"10-15\",\"15-20\",\"20-25\",\"25-30\", \"30-35\",\"35-40\",\"40-45\",\n",
    "             \"45-50\",\"50-55\",\"55-60\",\"60-65\",\"65-70\",\"70-75\", \"75-80\",\"80-85\",\"85-90\"]\n",
    "loss, acc = [], []\n",
    "for i in range(1, X_test_normalized.shape[1]+1):\n",
    "    print(intervals[i-1])\n",
    "    aux = model.evaluate(X_test_normalized[:, :i, :], y_test)\n",
    "    loss.append(aux[0])\n",
    "    acc.append(aux[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c07d61",
   "metadata": {},
   "source": [
    "## Pruebas con padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26312ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import betabinom\n",
    "\n",
    "# Parámetros de la distribución beta-binomial\n",
    "n = X.shape[1]   # Número de ensayos\n",
    "a = 0.70*4  # Parámetro alpha de la distribución beta\n",
    "b = 0.30*4   # Parámetro beta de la distribución beta\n",
    "\n",
    "# Número de muestras aleatorias que deseas generar\n",
    "num_samples = X.shape[0]\n",
    "\n",
    "# Generar números aleatorios de una distribución beta-binomial\n",
    "samples = betabinom.rvs(n, a, b, size=num_samples)\n",
    "\n",
    "# Calcular y mostrar la media de las muestras generadas\n",
    "mean_samples = np.mean(samples)\n",
    "print(f\"Media de las muestras: {mean_samples}\")\n",
    "\n",
    "values, counts = np.unique(samples, return_counts=True)\n",
    "\n",
    "# Graficar un barplot de las muestras generadas\n",
    "plt.bar(values, counts / num_samples, color='b', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Títulos y etiquetas\n",
    "plt.title('Histograma de la distribución Beta-Binomial')\n",
    "plt.xlabel('Valores')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e028c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data\n",
    "XFullpad = np.array(Xdata)\n",
    "y = np.array(ydata)\n",
    "y = to_categorical(y, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97429197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "split = int(0.8 * XFullpad.shape[0])\n",
    "XFullpad_train, XFullpad_test = XFullpad[:split], XFullpad[split:]\n",
    "\n",
    "# # Sin clean\n",
    "# scaler = MinMaxScaler()\n",
    "# XFullpad_train_reshaped = XFullpad_train.reshape(-1, X.shape[2])\n",
    "# XFullpad_train_normalized = scaler.fit_transform(XFullpad_train_reshaped)\n",
    "# XFullpad_train_normalized = XFullpad_train_normalized.reshape(XFullpad_train.shape[0], XFullpad_train.shape[1], XFullpad_train.shape[2])\n",
    "\n",
    "# XFullpad_test_reshaped = XFullpad_test.reshape(-1, X.shape[2])\n",
    "# XFullpad_test_normalized = scaler.transform(XFullpad_test_reshaped)\n",
    "# XFullpad_test_normalized = XFullpad_test_normalized.reshape(XFullpad_test.shape[0], XFullpad_test.shape[1], XFullpad_test.shape[2])\n",
    "\n",
    "# Con clean \n",
    "XFullpad_train_reshaped = XFullpad_train.reshape(-1, X.shape[2])\n",
    "XFullpad_test_reshaped = XFullpad_test.reshape(-1, X.shape[2])\n",
    "\n",
    "sclean  = clean(XFullpad_train_reshaped)      # indices of selected features\n",
    "XFullpad_train_reshaped = deepcopy(XFullpad_train_reshaped[:,sclean])\n",
    "XFullpad_test_reshaped = deepcopy(XFullpad_test_reshaped[:,sclean])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "XFullpad_train_normalized = scaler.fit_transform(XFullpad_train_reshaped)\n",
    "XFullpad_test_normalized = scaler.transform(XFullpad_test_reshaped)\n",
    "\n",
    "XFullpad_train_normalized = XFullpad_train_normalized.reshape(XFullpad_train.shape[0], XFullpad_train.shape[1], len(sclean))\n",
    "XFullpad_test_normalized = XFullpad_test_normalized.reshape(XFullpad_test.shape[0], XFullpad_test.shape[1], len(sclean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfd5eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "XFullpad_train_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ab2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de la distribución binomial \n",
    "# n = X.shape[1]\n",
    "# p = 0.9\n",
    "# size = X.shape[0]\n",
    "# random_values = binom.rvs(n, p, size=size)\n",
    "\n",
    "# Parámetros de la distribución beta-binomial\n",
    "n = XFullpad.shape[1]   # Número de ensayos\n",
    "size = XFullpad.shape[0]\n",
    "a = 0.70*4  # Parámetro alpha de la distribución beta\n",
    "b = 0.30*4   # Parámetro beta de la distribución beta\n",
    "\n",
    "random_values = betabinom.rvs(n, a, b, size=size)\n",
    "\n",
    "# random_values = np.random.randint(1, n+1, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbedd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binom  and uniform\n",
    "Xpad_train = []\n",
    "ypad_train = []\n",
    "for i in range(split):\n",
    "    Xpad_train.append(XFullpad_train_normalized[i][:random_values[i]+1])\n",
    "    ypad_train.append(y[i])\n",
    "\n",
    "Xpad_test = []\n",
    "ypad_test = []\n",
    "for i in range(len(XFullpad)-split):\n",
    "    Xpad_test.append(XFullpad_test_normalized[i][:random_values[i+split]+1])\n",
    "    ypad_test.append(y[i+split])\n",
    "\n",
    "# Full\n",
    "# Xpad = []\n",
    "# ypad = []\n",
    "# for i in range(size):\n",
    "#     for j in range(n):\n",
    "#         Xpad.append(X[i][:j+1])\n",
    "#         ypad.append(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpad_train = pad_sequences(Xpad_train, padding='post', dtype='float64')\n",
    "Xpad_test = pad_sequences(Xpad_test, padding='post', dtype='float64')\n",
    "ypad_train = np.array(ypad_train)\n",
    "ypad_test = np.array(ypad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e6f703",
   "metadata": {},
   "outputs": [],
   "source": [
    "ypad_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262cd47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpad_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf7cb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "\n",
    "l2_recurrent_parameter = 0.01\n",
    "l_2_kernel_regularizer= 0.01\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=(Xpad_train.shape[1], Xpad_train.shape[2])))\n",
    "model.add(LSTM(256, return_sequences=True,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=True,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=True,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=False,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compilación del modelo\n",
    "with tf.device('GPU:0'): # Usar gpu\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=\"models\\Full\",\n",
    "#     monitor='val_accuracy',\n",
    "#     mode='max',\n",
    "#     save_best_only=True)\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=10^(-9))\n",
    "\n",
    "history = model.fit(Xpad_train, ypad_train, verbose=1, epochs=1000, batch_size=16,\n",
    "                    validation_data=(Xpad_test, ypad_test), callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beee1850",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = [], []\n",
    "for i in range(1, XFullpad_test_normalized.shape[1]+1):\n",
    "    print(intervals[i])\n",
    "    aux = model.evaluate(XFullpad_test_normalized[:, :i, :], ypad_test)\n",
    "    loss.append(aux[0])\n",
    "    acc.append(aux[1])\n",
    "print(np.mean(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
