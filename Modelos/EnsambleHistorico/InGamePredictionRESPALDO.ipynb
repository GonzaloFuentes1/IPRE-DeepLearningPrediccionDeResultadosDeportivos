{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5de692a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_tabnet.tab_model import TabNetClassifier\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from copy import deepcopy\n",
    "from tqdm import tqdm\n",
    "from balu3.fs.sel    import sfs, clean\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from tensorflow.keras import layers\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import randint\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from keras.utils import to_categorical\n",
    "from scipy.stats import binom, betabinom\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "210ca305",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Para la LSTM\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b934cb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gfuen\\AppData\\Local\\Temp\\ipykernel_4608\\788853108.py:11: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  dfHistorico2['Date'] = pd.to_datetime(dfHistorico2['Date'], dayfirst=True, errors='coerce')\n"
     ]
    }
   ],
   "source": [
    "# Data historica\n",
    "\n",
    "dfHistorico1 = pd.read_csv(\"dataHistórica/resultados_pasados_premier.csv\")\n",
    "dfHistorico1 = dfHistorico1.drop(\"Unnamed: 0\",axis = 1)\n",
    "dfHistorico1 = dfHistorico1.drop([\"Position_last_year_home\", \"Position_last_year_away\",\n",
    "                                  \"Points_last_year_home\", \"Points_last_year_away\"],axis = 1) # Para hacer el merge con el segundo.\n",
    "\n",
    "dfHistorico2 = pd.read_csv(\"dataHistórica/restoDeLigas.csv\")\n",
    "dfHistorico2 = dfHistorico2.drop(\"Unnamed: 0\",axis = 1)\n",
    "\n",
    "dfHistorico2['Date'] = pd.to_datetime(dfHistorico2['Date'], dayfirst=True, errors='coerce')\n",
    "dfHistorico2['Date'] = dfHistorico2['Date'].dt.strftime('%Y-%m-%d')\n",
    "dfHistorico2 = dfHistorico2[dfHistorico2[\"Date\"] >\"2010-01-01\"] # Seleccion solo de partidos de 2010 en adelante (porque el otro dataframe tiene info desde ahí)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c07e032",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHistorico = pd.concat([dfHistorico1, dfHistorico2], axis=0)\n",
    "dfHistorico = dfHistorico.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1524e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data tipo evento\n",
    "\n",
    "dfEvent = pd.read_csv(\"dataWhoScored/WhoScoredTeamPerMatchSpatial4x3TimeDiv5.csv\")\n",
    "dfEvent = dfEvent.drop(\"Unnamed: 0\",axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a459684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ajuste manual de equipos:\n",
    "# Los que no tengan match son un string vacio.\n",
    "\n",
    "histToEventNameV1 = {\n",
    "   \"Arsenal\": \"Arsenal\", \"Aston Villa\": \"Aston Villa\", \"Barnsley\": \"\", \"Birmingham City\": \"Birmingham\", \"Blackburn\": \"Blackburn\",\"Blackpool\": \"Blackpool\",\n",
    "   \"Bolton\" : \"Bolton\", \"Bournemouth\": \"Bournemouth\", \"Bradford City\": \"\", \"Brentford\": \"Brentford\", \"Brighton\": \"Brighton\", \"Burnley\": \"Burnley\", \"Cardiff City\": \"Cardiff\",\n",
    "   \"Charlton Ath\": \"\", \"Chelsea\": \"Chelsea\", \"Coventry City\": \"\", \"Crystal Palace\": \"Crystal Palace\", \"Derby County\":\"\", \"Everton\": \"Everton\", \"Fulham\": \"Fulham\",\n",
    "   \"Huddersfield\": \"Huddersfield\", \"Hull City\": \"Hull\", \"Ipswich Town\": \"\", \"Leeds United\": \"Leeds\", \"Leicester City\": \"Leicester\", \"Liverpool\": \"Liverpool\",\n",
    "   \"Manchester City\": \"Man City\", \"Manchester Utd\": \"Man Utd\", \"Middlesbrough\": \"Middlesbrough\", \"Newcastle Utd\": \"Newcastle\", \"Norwich City\": \"Norwich\",\n",
    "   \"Nott'ham Forest\": \"Nottingham Forest\", \"Oldham Athletic\": \"\", \"Portsmouth\": \"\", \"QPR\": \"QPR\", \"Reading\": \"Reading\", \"Sheffield Utd\": \"Sheff Utd\", \"Sheffield Weds\": \"\",\n",
    "   \"Southampton\": \"Southampton\", \"Stoke City\": \"Stoke\", \"Sunderland\": \"Sunderland\", \"Swansea City\": \"Swansea\", \"Swindon Town\":  \"\", \"Tottenham\": \"Tottenham\", \"Watford\":\"Watford\",\n",
    "   \"West Brom\": \"WBA\", \"West Ham\": \"West Ham\", \"Wigan Athletic\": \"Wigan\", \"Wimbledon\": \"\", \"Wolves\": \"Wolves\"\n",
    "}\n",
    "len(histToEventNameV1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a16d91e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "207"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "histToEventName = {\n",
    "   \"Arsenal\": \"Arsenal\", \"Aston Villa\": \"Aston Villa\", \"Barnsley\": \"\", \"Birmingham City\": \"Birmingham\", \"Blackburn\": \"Blackburn\",\"Blackpool\": \"Blackpool\",\n",
    "   \"Bolton\" : \"Bolton\", \"Bournemouth\": \"Bournemouth\", \"Bradford City\": \"\", \"Brentford\": \"Brentford\", \"Brighton\": \"Brighton\", \"Burnley\": \"Burnley\", \"Cardiff City\": \"Cardiff\",\n",
    "   \"Charlton Ath\": \"\", \"Chelsea\": \"Chelsea\", \"Coventry City\": \"\", \"Crystal Palace\": \"Crystal Palace\", \"Derby County\":\"\", \"Everton\": \"Everton\", \"Fulham\": \"Fulham\",\n",
    "   \"Huddersfield\": \"Huddersfield\", \"Hull City\": \"Hull\", \"Ipswich Town\": \"\", \"Leeds United\": \"Leeds\", \"Leicester City\": \"Leicester\", \"Liverpool\": \"Liverpool\",\n",
    "   \"Manchester City\": \"Man City\", \"Manchester Utd\": \"Man Utd\", \"Middlesbrough\": \"Middlesbrough\", \"Newcastle Utd\": \"Newcastle\", \"Norwich City\": \"Norwich\",\n",
    "   \"Nott'ham Forest\": \"Nottingham Forest\", \"Oldham Athletic\": \"\", \"Portsmouth\": \"\", \"QPR\": \"QPR\", \"Reading\": \"Reading\", \"Sheffield Utd\": \"Sheff Utd\", \"Sheffield Weds\": \"\",\n",
    "   \"Southampton\": \"Southampton\", \"Stoke City\": \"Stoke\", \"Sunderland\": \"Sunderland\", \"Swansea City\": \"Swansea\", \"Swindon Town\":  \"\", \"Tottenham\": \"Tottenham\", \"Watford\":\"Watford\",\n",
    "   \"West Brom\": \"WBA\", \"West Ham\": \"West Ham\", \"Wigan Athletic\": \"Wigan\", \"Wimbledon\": \"\", \"Wolves\": \"Wolves\", #V1\n",
    "   \"Aachen\":\"\",\"Ajaccio\":\"AC Ajaccio\", \"Alaves\":\"Deportivo Alaves\", \"Almeria\": \"Almeria\", \"Amiens\":\"Amiens\",\"Angers\":\"Angers\", \"Arles\":\"Arles-Avignon\", \"Ascoli\":\"\",\n",
    "   \"Atalanta\":\"Atalanta\", \"Ath Bilbao\":\"Athletic Club\", \"Ath Madrid\":\"Atletico\", \"Augsburg\":\"Augsburg\", \"Auxerre\":\"Auxerre\", \"Barcelona\":\"Barcelona\", \"Bari\":\"Bari\",\n",
    "   \"Bastia\":\"SC Bastia\", \"Bayern Munich\":\"Bayern\", \"Benevento\":\"Benevento\", \"Betis\":\"Real Betis\", \"Bielefeld\":\"Arminia Bielefeld\", \"Bochum\": \"Bochum\", \"Bologna\":\"Bologna\",\n",
    "   \"Bordeaux\":\"Bordeaux\", \"Boulogne\": \"\", \"Braunschweig\": \"Eintracht Braunschweig\", \"Brescia\":\"Brescia\", \"Brest\":\"Brest\", \"Cadiz\":\"Cadiz\", \"Caen\": \"Caen\", \"Cagliari\":\"Cagliari\",\n",
    "   \"Carpi\":\"AC Carpi\", \"Catania\":\"Catania\", \"Celta\":\"Celta Vigo\", \"Cesena\": \"Cesena\", \"Chievo\":\"Chievo\", \"Clermont\":\"Clermont Foot\", \"Cordoba\": \"Cordoba\", \"Cottbus\":\"\",\n",
    "   \"Cremonese\":\"Cremonese\", \"Crotone\":\"Crotone\", \"Darmstadt\":\"Darmstadt\", \"Dijon\": \"Dijon\", \"Dortmund\":\"Borussia Dortmund\", \"Duisburg\":\"\", \"Eibar\":\"Eibar\", \"Ein Frankfurt\":\"Eintracht Frankfurt\",\n",
    "   \"Elche\": \"Elche\", \"Empoli\": \"Empoli\", \"Espanol\": \"Espanyol\", \"Evian Thonon Gaillard\": \"Evian\", \"FC Koln\": \"FC Koln\", \"Fiorentina\": \"Fiorentina\", \"Fortuna Dusseldorf\": \"Fortuna Duesseldorf\",\n",
    "   \"Freiburg\":\"Freiburg\", \"Frosinone\": \"Frosinone\", \"Genoa\": \"Genoa\", \"Getafe\": \"Getafe\", \"Gimnastic\":\"\", \"Girona\": \"Girona\", \"Granada\": \"Granada\", \"Grenoble\":\"\",\n",
    "   \"Greuther Furth\": \"Greuther Fuerth\", \"Guingamp\": \"Guingamp\", \"Hamburg\":\"Hamburg\", \"Hannover\":\"Hannover\", \"Hansa Rostock\":\"\", \"Heidenheim\":\"FC Heidenheim\", \"Hercules\":\"Hercules\",\n",
    "   \"Hertha\":\"Hertha Berlin\", \"Hoffenheim\":\"Hoffenheim\", \"Huesca\": \"SD Huesca\", \"Ingolstadt\": \"Ingolstadt\", \"Inter\": \"Inter\", \"Juventus\":\"Juventus\", \"Kaiserslautern\":\"Kaiserslautern\",\n",
    "   \"Karlsruhe\":\"\", \"La Coruna\": \"Deportivo\", \"Las Palmas\": \"Las Palmas\", \"Lazio\": \"Lazio\", \"Le Havre\": \"Le Havre\", \"Le Mans\":\"\", \"Lecce\": \"Lecce\", \"Leganes\":\"Leganes\", \"Lens\": \"Lens\",\n",
    "   \"Levante\": \"Levante\", \"Leverkusen\": \"Leverkusen\", \"Lille\":\"Lille\", \"Livorno\": \"Livorno\", \"Lorient\":\"Lorient\", \"Lyon\": \"Lyon\", \"M'gladbach\": \"Borussia M.Gladbach\", \"Mainz\": \"Mainz\",\n",
    "   \"Malaga\":\"Malaga\", \"Mallorca\":\"Mallorca\", \"Messina\": \"\", \"Metz\": \"Metz\", \"Milan\": \"AC Milan\", \"Monaco\": \"Monaco\", \"Montpellier\": \"Montpellier\", \"Monza\": \"Monza\", \"Murcia\":\"\",\n",
    "   \"Nancy\":\"Nancy\", \"Nantes\":\"Nantes\", \"Napoli\":\"Napoli\", \"Nice\":\"Nice\", \"Nimes\":\"Nimes\", \"Novara\": \"Novara\", \"Numancia\":\"\", \"Nurnberg\":\"Nuernberg\", \"Osasuna\": \"Osasuna\",\n",
    "   \"Paderborn\": \"Paderborn\", \"Palermo\": \"Palermo\", \"Paris SG\": \"PSG\", \"Parma\": \"Parma Calcio 1913\", \"Pescara\":\"Pescara\", \"RB Leipzig\": \"RBL\",\"Real Madrid\":\"Real Madrid\", \n",
    "   \"Recreativo\":\"\", \"Reggina\":\"\", \"Reims\": \"Reims\", \"Rennes\":\"Rennes\", \"Roma\":\"Roma\", \"Salernitana\": \"Salernitana\", \"Sampdoria\":\"Sampdoria\", \"Santander\": \"Racing Santander\",\n",
    "   \"Sassuolo\":\"Sassuolo\", \"Schalke 04\": \"Schalke\", \"Sedan\":\"\", \"Sevilla\":\"Sevilla\", \"Siena\": \"Siena\", \"Sochaux\": \"Sochaux\", \"Sociedad\": \"Real Sociedad\", \"Sp Gijon\":\"Sporting Gijon\",\n",
    "   \"Spal\":\"SPAL 2013\", \"Spezia\": \"Spezia\", \"St Etienne\":\"Saint-Etienne\", \"St Pauli\":\"St. Pauli\", \"Strasbourg\":\"Strasbourg\", \"Stuttgart\": \"Stuttgart\", \"Tenerife\":\"\", \"Torino\":\"Torino\",\n",
    "   \"Toulouse\":\"Toulouse\", \"Treviso\":\"\", \"Troyes\":\"Troyes\", \"Udinese\": \"Udinese\", \"Union Berlin\":\"Union Berlin\", \"Valencia\": \"Valencia\", \"Valenciennes\": \"Valenciennes\", \"Valladolid\": \"Real Valladolid\",\n",
    "   \"Vallecano\": \"Rayo Vallecano\", \"Venezia\":\"Venezia\", \"Verona\":\"Verona\", \"Villarreal\": \"Villarreal\", \"Werder Bremen\": \"Werder Bremen\", \"Wolfsburg\": \"Wolfsburg\", \"Xerez\": \"\", \"Zaragoza\": \"Real Zaragoza\"\n",
    "}\n",
    "len(histToEventName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d34545e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ajaccio', 'Ajaccio GFCO', 'Alaves', 'Almeria', 'Amiens', 'Angers', 'Arles', 'Arsenal', 'Aston Villa', 'Atalanta', 'Ath Bilbao', 'Ath Madrid', 'Augsburg', 'Auxerre', 'Barcelona', 'Bari', 'Barnsley', 'Bastia', 'Bayern Munich', 'Benevento', 'Betis', 'Bielefeld', 'Birmingham City', 'Blackburn', 'Blackpool', 'Bochum', 'Bologna', 'Bolton', 'Bordeaux', 'Boulogne', 'Bournemouth', 'Bradford City', 'Braunschweig', 'Brentford', 'Brescia', 'Brest', 'Brighton', 'Burnley', 'Cadiz', 'Caen', 'Cagliari', 'Cardiff City', 'Carpi', 'Catania', 'Celta', 'Cesena', 'Charlton Ath', 'Chelsea', 'Chievo', 'Clermont', 'Cordoba', 'Coventry City', 'Cremonese', 'Crotone', 'Crystal Palace', 'Darmstadt', 'Derby County', 'Dijon', 'Dortmund', 'Eibar', 'Ein Frankfurt', 'Elche', 'Empoli', 'Espanol', 'Everton', 'Evian Thonon Gaillard', 'FC Koln', 'Fiorentina', 'Fortuna Dusseldorf', 'Freiburg', 'Frosinone', 'Fulham', 'Genoa', 'Getafe', 'Girona', 'Granada', 'Grenoble', 'Greuther Furth', 'Guingamp', 'Hamburg', 'Hannover', 'Heidenheim', 'Hercules', 'Hertha', 'Hoffenheim', 'Huddersfield', 'Huesca', 'Hull City', 'Ingolstadt', 'Inter', 'Ipswich Town', 'Juventus', 'Kaiserslautern', 'La Coruna', 'Las Palmas', 'Lazio', 'Le Havre', 'Le Mans', 'Lecce', 'Leeds United', 'Leganes', 'Leicester City', 'Lens', 'Levante', 'Leverkusen', 'Lille', 'Liverpool', 'Livorno', 'Lorient', 'Lyon', \"M'gladbach\", 'Mainz', 'Malaga', 'Mallorca', 'Manchester City', 'Manchester Utd', 'Marseille', 'Metz', 'Middlesbrough', 'Milan', 'Monaco', 'Montpellier', 'Monza', 'Nancy', 'Nantes', 'Napoli', 'Newcastle Utd', 'Nice', 'Nimes', 'Norwich City', \"Nott'ham Forest\", 'Novara', 'Nurnberg', 'Oldham Athletic', 'Osasuna', 'Paderborn', 'Palermo', 'Paris SG', 'Parma', 'Pescara', 'Portsmouth', 'QPR', 'RB Leipzig', 'Reading', 'Real Madrid', 'Reims', 'Rennes', 'Roma', 'Salernitana', 'Sampdoria', 'Santander', 'Sassuolo', 'Schalke 04', 'Sevilla', 'Sheffield Utd', 'Sheffield Weds', 'Siena', 'Sochaux', 'Sociedad', 'Southampton', 'Sp Gijon', 'Spal', 'Spezia', 'St Etienne', 'St Pauli', 'Stoke City', 'Strasbourg', 'Stuttgart', 'Sunderland', 'Swansea City', 'Swindon Town', 'Tenerife', 'Torino', 'Tottenham', 'Toulouse', 'Troyes', 'Udinese', 'Union Berlin', 'Valencia', 'Valenciennes', 'Valladolid', 'Vallecano', 'Venezia', 'Verona', 'Villarreal', 'Watford', 'Werder Bremen', 'West Brom', 'West Ham', 'Wigan Athletic', 'Wimbledon', 'Wolfsburg', 'Wolves', 'Xerez', 'Zaragoza']\n"
     ]
    }
   ],
   "source": [
    "# Para buscar nombres\n",
    "\n",
    "a = dfHistorico[\"Team_home\"].unique().tolist()\n",
    "a.sort()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7402154c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AC Ajaccio', 'AC Carpi', 'AC Milan', 'Almeria', 'Amiens', 'Angers', 'Arles-Avignon', 'Arminia Bielefeld', 'Arsenal', 'Aston Villa', 'Atalanta', 'Athletic Club', 'Atletico', 'Augsburg', 'Auxerre', 'Barcelona', 'Bari', 'Bayern', 'Benevento', 'Birmingham', 'Blackburn', 'Blackpool', 'Bochum', 'Bologna', 'Bolton', 'Bordeaux', 'Borussia Dortmund', 'Borussia M.Gladbach', 'Bournemouth', 'Brentford', 'Brescia', 'Brest', 'Brighton', 'Burnley', 'Cadiz', 'Caen', 'Cagliari', 'Cardiff', 'Catania', 'Celta Vigo', 'Cesena', 'Chelsea', 'Chievo', 'Clermont Foot', 'Cordoba', 'Cremonese', 'Crotone', 'Crystal Palace', 'Darmstadt', 'Deportivo', 'Deportivo Alaves', 'Dijon', 'Eibar', 'Eintracht Braunschweig', 'Eintracht Frankfurt', 'Elche', 'Empoli', 'Espanyol', 'Everton', 'Evian', 'FC Heidenheim', 'FC Koln', 'Fiorentina', 'Fortuna Duesseldorf', 'Freiburg', 'Frosinone', 'Fulham', 'GFC Ajaccio', 'Genoa', 'Getafe', 'Girona', 'Granada', 'Greuther Fuerth', 'Guingamp', 'Hamburg', 'Hannover', 'Hercules', 'Hertha Berlin', 'Hoffenheim', 'Huddersfield', 'Hull', 'Ingolstadt', 'Inter', 'Juventus', 'Kaiserslautern', 'Las Palmas', 'Lazio', 'Le Havre', 'Lecce', 'Leeds', 'Leganes', 'Leicester', 'Lens', 'Levante', 'Leverkusen', 'Lille', 'Liverpool', 'Livorno', 'Lorient', 'Luton', 'Lyon', 'Mainz', 'Malaga', 'Mallorca', 'Man City', 'Man Utd', 'Marseille', 'Metz', 'Middlesbrough', 'Monaco', 'Montpellier', 'Monza', 'Nancy', 'Nantes', 'Napoli', 'Newcastle', 'Nice', 'Nimes', 'Norwich', 'Nottingham Forest', 'Novara', 'Nuernberg', 'Osasuna', 'PSG', 'Paderborn', 'Palermo', 'Parma Calcio 1913', 'Pescara', 'QPR', 'RBL', 'Racing Santander', 'Rayo Vallecano', 'Reading', 'Real Betis', 'Real Madrid', 'Real Sociedad', 'Real Valladolid', 'Real Zaragoza', 'Reims', 'Rennes', 'Roma', 'SC Bastia', 'SD Huesca', 'SPAL 2013', 'Saint-Etienne', 'Salernitana', 'Sampdoria', 'Sassuolo', 'Schalke', 'Sevilla', 'Sheff Utd', 'Siena', 'Sochaux', 'Southampton', 'Spezia', 'Sporting Gijon', 'St. Pauli', 'Stoke', 'Strasbourg', 'Stuttgart', 'Sunderland', 'Swansea', 'Torino', 'Tottenham', 'Toulouse', 'Troyes', 'Udinese', 'Union Berlin', 'Valencia', 'Valenciennes', 'Verona', 'Villarreal', 'WBA', 'Watford', 'Werder Bremen', 'West Ham', 'Wigan', 'Wolfsburg', 'Wolves']\n"
     ]
    }
   ],
   "source": [
    "# Para buscar nombres\n",
    "\n",
    "a = dfEvent[\"Team\"].unique().tolist()\n",
    "a.sort()\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "270a03bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombres de data histórica a nombres de tipo evento.\n",
    "\n",
    "dfHistorico[\"Team_home\"] = dfHistorico[\"Team_home\"].map(histToEventName)\n",
    "dfHistorico[\"Team_away\"] = dfHistorico[\"Team_away\"].map(histToEventName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fee6ab73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wk                                 0\n",
       "Date                               0\n",
       "Team_home                        284\n",
       "Team_away                        281\n",
       "Goals_home                         0\n",
       "Goals_away                         0\n",
       "score_prom_home                    0\n",
       "score_prom_away                    0\n",
       "wins_home                          0\n",
       "wins_away                          0\n",
       "ties_home                          0\n",
       "ties_away                          0\n",
       "losses_home                        0\n",
       "losses_away                        0\n",
       "total_goals_home                   0\n",
       "total_goals_away                   0\n",
       "total_goals_conceded_home          0\n",
       "total_goals_conceded_away          0\n",
       "points_home                        0\n",
       "points_away                        0\n",
       "streak_home                        0\n",
       "streak_away                        0\n",
       "wins_in_last_5_matches_home        0\n",
       "wins_in_last_5_matches_away        0\n",
       "ties_in_last_5_matches_home        0\n",
       "ties_in_last_5_matches_away        0\n",
       "losses_in_last_5_matches_home      0\n",
       "losses_in_last_5_matches_away      0\n",
       "result                             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Comprobamos que no esten datos vacios\n",
    "\n",
    "dfHistorico.isnull().sum() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cf3e487",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 31979/31979 [04:21<00:00, 122.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Primero revisamos si estan todos los partidos compatibles, para que sea más robusta la solución\n",
    "\n",
    "indexHistorical = []\n",
    "for i in tqdm(range(len(dfHistorico))):\n",
    "    stringHome = dfHistorico.iloc[i].Team_home\n",
    "    stringAway = dfHistorico.iloc[i].Team_away\n",
    "    date = dfHistorico.iloc[i].Date\n",
    "\n",
    "    dfAux = dfEvent[dfEvent.Date == date]\n",
    "\n",
    "    dfHome = dfAux[dfAux.Team == stringHome].add_prefix(\"Home\")\n",
    "    dfAway = dfAux[dfAux.Team == stringAway].add_prefix(\"Away\")\n",
    "\n",
    "    # dfHome.reset_index(drop=True, inplace=True)\n",
    "    # dfAway.reset_index(drop=True, inplace=True)\n",
    "    if len(dfHome) + len(dfAway) == 2: \n",
    "        indexHistorical.append(i)\n",
    "    # concat = pd.concat([dfHome, dfAway], axis = 1)\n",
    "    # test.append(concat)\n",
    "    # Buscamos los partidos que tenemos información\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d7a0856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el dataframe de partidos a buscar\n",
    "\n",
    "dfHistoricoCompatible = dfHistorico.iloc[indexHistorical].reset_index(drop=True)\n",
    "dfHistoricoCompatible = dfHistoricoCompatible.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b92f213d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Wk</th>\n",
       "      <th>Date</th>\n",
       "      <th>Team_home</th>\n",
       "      <th>Team_away</th>\n",
       "      <th>Goals_home</th>\n",
       "      <th>Goals_away</th>\n",
       "      <th>score_prom_home</th>\n",
       "      <th>score_prom_away</th>\n",
       "      <th>wins_home</th>\n",
       "      <th>wins_away</th>\n",
       "      <th>...</th>\n",
       "      <th>points_away</th>\n",
       "      <th>streak_home</th>\n",
       "      <th>streak_away</th>\n",
       "      <th>wins_in_last_5_matches_home</th>\n",
       "      <th>wins_in_last_5_matches_away</th>\n",
       "      <th>ties_in_last_5_matches_home</th>\n",
       "      <th>ties_in_last_5_matches_away</th>\n",
       "      <th>losses_in_last_5_matches_home</th>\n",
       "      <th>losses_in_last_5_matches_away</th>\n",
       "      <th>result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-08-14</td>\n",
       "      <td>Bolton</td>\n",
       "      <td>Fulham</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-08-14</td>\n",
       "      <td>Wigan</td>\n",
       "      <td>Blackpool</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-08-14</td>\n",
       "      <td>Tottenham</td>\n",
       "      <td>Man City</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-08-14</td>\n",
       "      <td>Aston Villa</td>\n",
       "      <td>West Ham</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>2010-08-14</td>\n",
       "      <td>Chelsea</td>\n",
       "      <td>WBA</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22204</th>\n",
       "      <td>23.0</td>\n",
       "      <td>2024-02-23</td>\n",
       "      <td>Leverkusen</td>\n",
       "      <td>Mainz</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.590909</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>15.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22205</th>\n",
       "      <td>23.0</td>\n",
       "      <td>2024-02-24</td>\n",
       "      <td>Borussia M.Gladbach</td>\n",
       "      <td>Bochum</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>...</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22206</th>\n",
       "      <td>23.0</td>\n",
       "      <td>2024-02-24</td>\n",
       "      <td>Stuttgart</td>\n",
       "      <td>FC Koln</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.318182</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>...</td>\n",
       "      <td>16.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22207</th>\n",
       "      <td>23.0</td>\n",
       "      <td>2024-02-24</td>\n",
       "      <td>Union Berlin</td>\n",
       "      <td>FC Heidenheim</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>1.409091</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>27.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22208</th>\n",
       "      <td>23.0</td>\n",
       "      <td>2024-02-24</td>\n",
       "      <td>Werder Bremen</td>\n",
       "      <td>Darmstadt</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.409091</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22209 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Wk        Date            Team_home      Team_away  Goals_home  \\\n",
       "0       1.0  2010-08-14               Bolton         Fulham         0.0   \n",
       "1       1.0  2010-08-14                Wigan      Blackpool         0.0   \n",
       "2       1.0  2010-08-14            Tottenham       Man City         0.0   \n",
       "3       1.0  2010-08-14          Aston Villa       West Ham         3.0   \n",
       "4       1.0  2010-08-14              Chelsea            WBA         6.0   \n",
       "...     ...         ...                  ...            ...         ...   \n",
       "22204  23.0  2024-02-23           Leverkusen          Mainz         2.0   \n",
       "22205  23.0  2024-02-24  Borussia M.Gladbach         Bochum         5.0   \n",
       "22206  23.0  2024-02-24            Stuttgart        FC Koln         1.0   \n",
       "22207  23.0  2024-02-24         Union Berlin  FC Heidenheim         2.0   \n",
       "22208  23.0  2024-02-24        Werder Bremen      Darmstadt         1.0   \n",
       "\n",
       "       Goals_away  score_prom_home  score_prom_away  wins_home  wins_away  \\\n",
       "0             0.0         0.000000         0.000000        0.0        0.0   \n",
       "1             4.0         0.000000         0.000000        0.0        0.0   \n",
       "2             0.0         0.000000         0.000000        0.0        0.0   \n",
       "3             0.0         0.000000         0.000000        0.0        0.0   \n",
       "4             0.0         0.000000         0.000000        0.0        0.0   \n",
       "...           ...              ...              ...        ...        ...   \n",
       "22204         1.0         2.590909         0.772727       18.0        2.0   \n",
       "22205         2.0         1.636364         1.181818        5.0        5.0   \n",
       "22206         1.0         2.318182         0.681818       15.0        3.0   \n",
       "22207         2.0         0.954545         1.409091        7.0        7.0   \n",
       "22208         1.0         1.409091         1.045455        8.0        2.0   \n",
       "\n",
       "       ...  points_away  streak_home  streak_away  \\\n",
       "0      ...          0.0          0.0          0.0   \n",
       "1      ...          0.0          0.0          0.0   \n",
       "2      ...          0.0          0.0          0.0   \n",
       "3      ...          0.0          0.0          0.0   \n",
       "4      ...          0.0          0.0          0.0   \n",
       "...    ...          ...          ...          ...   \n",
       "22204  ...         15.0         22.0          1.0   \n",
       "22205  ...         25.0          0.0          3.0   \n",
       "22206  ...         16.0          4.0          0.0   \n",
       "22207  ...         27.0          2.0          0.0   \n",
       "22208  ...         12.0          1.0          0.0   \n",
       "\n",
       "       wins_in_last_5_matches_home  wins_in_last_5_matches_away  \\\n",
       "0                              0.0                          0.0   \n",
       "1                              0.0                          0.0   \n",
       "2                              0.0                          0.0   \n",
       "3                              0.0                          0.0   \n",
       "4                              0.0                          0.0   \n",
       "...                            ...                          ...   \n",
       "22204                          4.0                          1.0   \n",
       "22205                          0.0                          1.0   \n",
       "22206                          4.0                          1.0   \n",
       "22207                          3.0                          1.0   \n",
       "22208                          4.0                          0.0   \n",
       "\n",
       "       ties_in_last_5_matches_home  ties_in_last_5_matches_away  \\\n",
       "0                              0.0                          0.0   \n",
       "1                              0.0                          0.0   \n",
       "2                              0.0                          0.0   \n",
       "3                              0.0                          0.0   \n",
       "4                              0.0                          0.0   \n",
       "...                            ...                          ...   \n",
       "22204                          1.0                          1.0   \n",
       "22205                          2.0                          3.0   \n",
       "22206                          0.0                          2.0   \n",
       "22207                          1.0                          3.0   \n",
       "22208                          0.0                          2.0   \n",
       "\n",
       "       losses_in_last_5_matches_home  losses_in_last_5_matches_away  result  \n",
       "0                                0.0                            0.0       0  \n",
       "1                                0.0                            0.0       2  \n",
       "2                                0.0                            0.0       0  \n",
       "3                                0.0                            0.0       1  \n",
       "4                                0.0                            0.0       1  \n",
       "...                              ...                            ...     ...  \n",
       "22204                            0.0                            3.0       1  \n",
       "22205                            3.0                            1.0       1  \n",
       "22206                            1.0                            2.0       0  \n",
       "22207                            1.0                            1.0       0  \n",
       "22208                            1.0                            3.0       0  \n",
       "\n",
       "[22209 rows x 29 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfHistoricoCompatible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d4867c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wk                               0\n",
       "Date                             0\n",
       "Team_home                        0\n",
       "Team_away                        0\n",
       "Goals_home                       0\n",
       "Goals_away                       0\n",
       "score_prom_home                  0\n",
       "score_prom_away                  0\n",
       "wins_home                        0\n",
       "wins_away                        0\n",
       "ties_home                        0\n",
       "ties_away                        0\n",
       "losses_home                      0\n",
       "losses_away                      0\n",
       "total_goals_home                 0\n",
       "total_goals_away                 0\n",
       "total_goals_conceded_home        0\n",
       "total_goals_conceded_away        0\n",
       "points_home                      0\n",
       "points_away                      0\n",
       "streak_home                      0\n",
       "streak_away                      0\n",
       "wins_in_last_5_matches_home      0\n",
       "wins_in_last_5_matches_away      0\n",
       "ties_in_last_5_matches_home      0\n",
       "ties_in_last_5_matches_away      0\n",
       "losses_in_last_5_matches_home    0\n",
       "losses_in_last_5_matches_away    0\n",
       "result                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfHistoricoCompatible.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9b54072",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22209/22209 [1:02:00<00:00,  5.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# goals_succ = dfEvent.filter(regex='Goal_Successful_bin').columns.tolist()\n",
    "# goals_unsucc = dfEvent.filter(regex='Goal_Unsuccessful_bin').columns.tolist()\n",
    "# goals_succ.extend(goals_unsucc)\n",
    "# goals = goals_succ\n",
    "\n",
    "# intervals = [\"0-45\",\"45-90\"]\n",
    "# intervals = [\"0-15\",\"15-30\",\"30-45\",\"45-60\",\"60-75\",\"75-90\"]\n",
    "intervals = [\"0-5\",\"5-10\",\"10-15\",\"15-20\",\"20-25\",\"25-30\", \"30-35\",\"35-40\",\"40-45\",\n",
    "             \"45-50\",\"50-55\",\"55-60\",\"60-65\",\"65-70\",\"70-75\", \"75-80\",\"80-85\",\"85-90\"]\n",
    "\n",
    "Xdata = []\n",
    "ydata = []\n",
    "for i in tqdm(range(len(dfHistoricoCompatible))):\n",
    "    stringHome = dfHistoricoCompatible.iloc[i].Team_home\n",
    "    stringAway = dfHistoricoCompatible.iloc[i].Team_away\n",
    "    date = dfHistoricoCompatible.iloc[i].Date\n",
    "\n",
    "    # dfAux = dfEvent[dfEvent.Date == date].drop(goals, axis=1) #Test sin los goles\n",
    "    dfAux = dfEvent[dfEvent.Date == date]\n",
    "\n",
    "    dfHome = dfAux[dfAux.Team == stringHome].add_prefix(\"Home\")\n",
    "    dfAway = dfAux[dfAux.Team == stringAway].add_prefix(\"Away\")\n",
    "\n",
    "    # Dropear valores redundantes\n",
    "    dfHome = dfHome.drop([\"HomeTeam\",\"HomeDate\",\"HomeGoals\",\"Homegame_id\"], axis = 1)\n",
    "    dfAway = dfAway.drop([\"AwayTeam\",\"AwayDate\",\"AwayGoals\",\"Awaygame_id\"], axis = 1)\n",
    "\n",
    "    dfHome.reset_index(drop=True, inplace=True)\n",
    "    dfAway.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    concat = pd.concat([dfHome, dfAway], axis = 1).reset_index(drop=True)\n",
    "    x = []\n",
    "    if len(concat) == 0:\n",
    "        pass\n",
    "    else:\n",
    "        for inter in intervals:\n",
    "            intervals_columns = concat.filter(regex=f'{inter}$').columns.tolist()\n",
    "            hist = dfHistoricoCompatible.iloc[i:i+1].drop([\"Date\", \"Team_home\", \"Team_away\", \"result\", \"Goals_home\", \"Goals_away\", \"Wk\"], axis=1).values\n",
    "            x.append(np.append(concat[intervals_columns].values[0], hist))\n",
    "        ydata.append(dfHistoricoCompatible.iloc[i:i+1][\"result\"].values[0])\n",
    "        x = np.array(x)\n",
    "        Xdata.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4357a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para guardar el dataset y no correrlo varias veces \n",
    "np.save(\"Space4x3Time5.np\", np.array(Xdata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e882018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data\n",
    "X = np.array(Xdata)\n",
    "y = np.array(ydata)\n",
    "y = to_categorical(y, num_classes=3)\n",
    "\n",
    "# Mitad de los datos\n",
    "# X = np.array(Xdata)[:, :10, :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2e42354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22209, 18, 1270)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f34e393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22209, 3)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a29de717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "split = int(0.8 * X.shape[0])\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f83e49a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sin clean\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train_reshaped = X_train.reshape(-1, X.shape[2])\n",
    "# X_train_normalized = scaler.fit_transform(X_train_reshaped)\n",
    "# X_train_normalized = X_train_normalized.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2])\n",
    "\n",
    "# X_test_reshaped = X_test.reshape(-1, X.shape[2])\n",
    "# X_test_normalized = scaler.transform(X_test_reshaped)\n",
    "# X_test_normalized = X_test_normalized.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2])\n",
    "\n",
    "\n",
    "# Con clean \n",
    "X_train_reshaped = X_train.reshape(-1, X.shape[2])\n",
    "X_test_reshaped = X_test.reshape(-1, X.shape[2])\n",
    "\n",
    "sclean  = clean(X_train_reshaped)      # indices of selected features\n",
    "X_train_reshaped = deepcopy(X_train_reshaped[:,sclean])\n",
    "X_test_reshaped = deepcopy(X_test_reshaped[:,sclean])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_train_normalized = scaler.fit_transform(X_train_reshaped)\n",
    "X_test_normalized = scaler.transform(X_test_reshaped)\n",
    "\n",
    "X_train_normalized = X_train_normalized.reshape(X_train.shape[0], X_train.shape[1], len(sclean))\n",
    "X_test_normalized = X_test_normalized.reshape(X_test.shape[0], X_test.shape[1], len(sclean))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a287a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17767, 18, 766)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "59fc8894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "physical_devices = tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9a4e48ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Masking, LSTM, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "24e295fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1111/1111 [==============================] - 23s 17ms/step - loss: 44.8883 - accuracy: 0.4555 - val_loss: 2.6146 - val_accuracy: 0.3028 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 1.0811 - accuracy: 0.6159 - val_loss: 1.2657 - val_accuracy: 0.6751 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 1.0337 - accuracy: 0.6867 - val_loss: 1.5194 - val_accuracy: 0.3656 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.8251 - accuracy: 0.7301 - val_loss: 1.2749 - val_accuracy: 0.6655 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.7791 - accuracy: 0.7358 - val_loss: 0.6990 - val_accuracy: 0.7701 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.7653 - accuracy: 0.7433 - val_loss: 2.9550 - val_accuracy: 0.4570 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.7399 - accuracy: 0.7444 - val_loss: 2.1380 - val_accuracy: 0.3161 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.7246 - accuracy: 0.7511 - val_loss: 4.2008 - val_accuracy: 0.4500 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.7198 - accuracy: 0.7486 - val_loss: 1.7993 - val_accuracy: 0.5498 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.7122 - accuracy: 0.7529 - val_loss: 1.6385 - val_accuracy: 0.4286 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.7121 - accuracy: 0.7543 - val_loss: 0.6207 - val_accuracy: 0.7864 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.7169 - accuracy: 0.7538 - val_loss: 1.5128 - val_accuracy: 0.5293 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.6934 - accuracy: 0.7535 - val_loss: 2.0853 - val_accuracy: 0.3208 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.7027 - accuracy: 0.7514 - val_loss: 3.4579 - val_accuracy: 0.3170 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.6786 - accuracy: 0.7536 - val_loss: 9.9595 - val_accuracy: 0.3041 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.6829 - accuracy: 0.7595 - val_loss: 1.2110 - val_accuracy: 0.6715 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.6734 - accuracy: 0.7627 - val_loss: 2.0344 - val_accuracy: 0.5896 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.6777 - accuracy: 0.7616 - val_loss: 1.1219 - val_accuracy: 0.6898 - lr: 0.0010\n",
      "Epoch 19/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.6709 - accuracy: 0.7641 - val_loss: 0.7782 - val_accuracy: 0.6824 - lr: 0.0010\n",
      "Epoch 20/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.6651 - accuracy: 0.7658 - val_loss: 3.3007 - val_accuracy: 0.4584 - lr: 0.0010\n",
      "Epoch 21/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.6610 - accuracy: 0.7707 - val_loss: 4.8285 - val_accuracy: 0.4568 - lr: 0.0010\n",
      "Epoch 22/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.5099 - accuracy: 0.8200 - val_loss: 0.3836 - val_accuracy: 0.8926 - lr: 2.0000e-04\n",
      "Epoch 23/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.4572 - accuracy: 0.8367 - val_loss: 0.3655 - val_accuracy: 0.8820 - lr: 2.0000e-04\n",
      "Epoch 24/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.4519 - accuracy: 0.8387 - val_loss: 0.3712 - val_accuracy: 0.8845 - lr: 2.0000e-04\n",
      "Epoch 25/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.4393 - accuracy: 0.8480 - val_loss: 0.6180 - val_accuracy: 0.7562 - lr: 2.0000e-04\n",
      "Epoch 26/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.4304 - accuracy: 0.8509 - val_loss: 0.3250 - val_accuracy: 0.9111 - lr: 2.0000e-04\n",
      "Epoch 27/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.4222 - accuracy: 0.8525 - val_loss: 0.6072 - val_accuracy: 0.7271 - lr: 2.0000e-04\n",
      "Epoch 28/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.4109 - accuracy: 0.8606 - val_loss: 0.3318 - val_accuracy: 0.9070 - lr: 2.0000e-04\n",
      "Epoch 29/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.4090 - accuracy: 0.8627 - val_loss: 0.3159 - val_accuracy: 0.9027 - lr: 2.0000e-04\n",
      "Epoch 30/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.4110 - accuracy: 0.8635 - val_loss: 0.4697 - val_accuracy: 0.8098 - lr: 2.0000e-04\n",
      "Epoch 31/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.3945 - accuracy: 0.8725 - val_loss: 0.4854 - val_accuracy: 0.7794 - lr: 2.0000e-04\n",
      "Epoch 32/1000\n",
      "1111/1111 [==============================] - 21s 18ms/step - loss: 0.3977 - accuracy: 0.8696 - val_loss: 0.4289 - val_accuracy: 0.7999 - lr: 2.0000e-04\n",
      "Epoch 33/1000\n",
      "1111/1111 [==============================] - 21s 18ms/step - loss: 0.3959 - accuracy: 0.8722 - val_loss: 0.8378 - val_accuracy: 0.6855 - lr: 2.0000e-04\n",
      "Epoch 34/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.3912 - accuracy: 0.8734 - val_loss: 0.4386 - val_accuracy: 0.8122 - lr: 2.0000e-04\n",
      "Epoch 35/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.3897 - accuracy: 0.8771 - val_loss: 0.6252 - val_accuracy: 0.7229 - lr: 2.0000e-04\n",
      "Epoch 36/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.3720 - accuracy: 0.8843 - val_loss: 0.2988 - val_accuracy: 0.9102 - lr: 2.0000e-04\n",
      "Epoch 37/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.3769 - accuracy: 0.8813 - val_loss: 0.4531 - val_accuracy: 0.8307 - lr: 2.0000e-04\n",
      "Epoch 38/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.3667 - accuracy: 0.8868 - val_loss: 0.2417 - val_accuracy: 0.9548 - lr: 2.0000e-04\n",
      "Epoch 39/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3451 - accuracy: 0.9014 - val_loss: 0.3007 - val_accuracy: 0.9118 - lr: 2.0000e-04\n",
      "Epoch 40/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.3604 - accuracy: 0.8970 - val_loss: 1.0161 - val_accuracy: 0.6288 - lr: 2.0000e-04\n",
      "Epoch 41/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3491 - accuracy: 0.9030 - val_loss: 0.7949 - val_accuracy: 0.6810 - lr: 2.0000e-04\n",
      "Epoch 42/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3328 - accuracy: 0.9099 - val_loss: 0.2558 - val_accuracy: 0.9631 - lr: 2.0000e-04\n",
      "Epoch 43/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3406 - accuracy: 0.9105 - val_loss: 0.2022 - val_accuracy: 0.9748 - lr: 2.0000e-04\n",
      "Epoch 44/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3200 - accuracy: 0.9183 - val_loss: 0.2492 - val_accuracy: 0.9527 - lr: 2.0000e-04\n",
      "Epoch 45/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3302 - accuracy: 0.9129 - val_loss: 5.5616 - val_accuracy: 0.3624 - lr: 2.0000e-04\n",
      "Epoch 46/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3161 - accuracy: 0.9210 - val_loss: 0.2251 - val_accuracy: 0.9642 - lr: 2.0000e-04\n",
      "Epoch 47/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3064 - accuracy: 0.9235 - val_loss: 3.3516 - val_accuracy: 0.5374 - lr: 2.0000e-04\n",
      "Epoch 48/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3057 - accuracy: 0.9248 - val_loss: 0.7822 - val_accuracy: 0.7177 - lr: 2.0000e-04\n",
      "Epoch 49/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3020 - accuracy: 0.9267 - val_loss: 0.2652 - val_accuracy: 0.9453 - lr: 2.0000e-04\n",
      "Epoch 50/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.3028 - accuracy: 0.9291 - val_loss: 0.2861 - val_accuracy: 0.9372 - lr: 2.0000e-04\n",
      "Epoch 51/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.2929 - accuracy: 0.9336 - val_loss: 0.3569 - val_accuracy: 0.8577 - lr: 2.0000e-04\n",
      "Epoch 52/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.2973 - accuracy: 0.9332 - val_loss: 0.2659 - val_accuracy: 0.9491 - lr: 2.0000e-04\n",
      "Epoch 53/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.2830 - accuracy: 0.9387 - val_loss: 0.8163 - val_accuracy: 0.7105 - lr: 2.0000e-04\n",
      "Epoch 54/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.2258 - accuracy: 0.9619 - val_loss: 0.1930 - val_accuracy: 0.9752 - lr: 4.0000e-05\n",
      "Epoch 55/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.2173 - accuracy: 0.9646 - val_loss: 0.1873 - val_accuracy: 0.9692 - lr: 4.0000e-05\n",
      "Epoch 56/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.2102 - accuracy: 0.9662 - val_loss: 0.1593 - val_accuracy: 0.9872 - lr: 4.0000e-05\n",
      "Epoch 57/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.2021 - accuracy: 0.9692 - val_loss: 0.1880 - val_accuracy: 0.9784 - lr: 4.0000e-05\n",
      "Epoch 58/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1996 - accuracy: 0.9698 - val_loss: 0.1798 - val_accuracy: 0.9791 - lr: 4.0000e-05\n",
      "Epoch 59/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1987 - accuracy: 0.9688 - val_loss: 0.1608 - val_accuracy: 0.9773 - lr: 4.0000e-05\n",
      "Epoch 60/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1940 - accuracy: 0.9697 - val_loss: 0.1526 - val_accuracy: 0.9840 - lr: 4.0000e-05\n",
      "Epoch 61/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1894 - accuracy: 0.9699 - val_loss: 0.1488 - val_accuracy: 0.9854 - lr: 4.0000e-05\n",
      "Epoch 62/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1903 - accuracy: 0.9690 - val_loss: 0.1459 - val_accuracy: 0.9863 - lr: 4.0000e-05\n",
      "Epoch 63/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1793 - accuracy: 0.9721 - val_loss: 0.1364 - val_accuracy: 0.9854 - lr: 4.0000e-05\n",
      "Epoch 64/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1802 - accuracy: 0.9711 - val_loss: 0.1634 - val_accuracy: 0.9755 - lr: 4.0000e-05\n",
      "Epoch 65/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1809 - accuracy: 0.9719 - val_loss: 0.1620 - val_accuracy: 0.9840 - lr: 4.0000e-05\n",
      "Epoch 66/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1810 - accuracy: 0.9705 - val_loss: 0.1765 - val_accuracy: 0.9773 - lr: 4.0000e-05\n",
      "Epoch 67/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1772 - accuracy: 0.9726 - val_loss: 0.1503 - val_accuracy: 0.9863 - lr: 4.0000e-05\n",
      "Epoch 68/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1737 - accuracy: 0.9719 - val_loss: 0.1373 - val_accuracy: 0.9874 - lr: 4.0000e-05\n",
      "Epoch 69/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1678 - accuracy: 0.9748 - val_loss: 0.1522 - val_accuracy: 0.9820 - lr: 4.0000e-05\n",
      "Epoch 70/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1746 - accuracy: 0.9724 - val_loss: 0.1389 - val_accuracy: 0.9829 - lr: 4.0000e-05\n",
      "Epoch 71/1000\n",
      "1111/1111 [==============================] - 20s 18ms/step - loss: 0.1723 - accuracy: 0.9730 - val_loss: 0.1429 - val_accuracy: 0.9795 - lr: 4.0000e-05\n",
      "Epoch 72/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1642 - accuracy: 0.9761 - val_loss: 0.1340 - val_accuracy: 0.9869 - lr: 4.0000e-05\n",
      "Epoch 73/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1592 - accuracy: 0.9762 - val_loss: 0.2852 - val_accuracy: 0.8969 - lr: 4.0000e-05\n",
      "Epoch 74/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1700 - accuracy: 0.9730 - val_loss: 0.1418 - val_accuracy: 0.9824 - lr: 4.0000e-05\n",
      "Epoch 75/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1655 - accuracy: 0.9748 - val_loss: 0.1627 - val_accuracy: 0.9782 - lr: 4.0000e-05\n",
      "Epoch 76/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1659 - accuracy: 0.9735 - val_loss: 0.1204 - val_accuracy: 0.9896 - lr: 4.0000e-05\n",
      "Epoch 77/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1585 - accuracy: 0.9756 - val_loss: 0.3247 - val_accuracy: 0.8708 - lr: 4.0000e-05\n",
      "Epoch 78/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1639 - accuracy: 0.9735 - val_loss: 0.1436 - val_accuracy: 0.9788 - lr: 4.0000e-05\n",
      "Epoch 79/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1569 - accuracy: 0.9771 - val_loss: 0.1441 - val_accuracy: 0.9829 - lr: 4.0000e-05\n",
      "Epoch 80/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1587 - accuracy: 0.9755 - val_loss: 0.1196 - val_accuracy: 0.9887 - lr: 4.0000e-05\n",
      "Epoch 81/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1563 - accuracy: 0.9773 - val_loss: 0.1507 - val_accuracy: 0.9752 - lr: 4.0000e-05\n",
      "Epoch 82/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1566 - accuracy: 0.9752 - val_loss: 0.1206 - val_accuracy: 0.9896 - lr: 4.0000e-05\n",
      "Epoch 83/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1537 - accuracy: 0.9762 - val_loss: 0.1165 - val_accuracy: 0.9883 - lr: 4.0000e-05\n",
      "Epoch 84/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1521 - accuracy: 0.9764 - val_loss: 0.1254 - val_accuracy: 0.9842 - lr: 4.0000e-05\n",
      "Epoch 85/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1524 - accuracy: 0.9779 - val_loss: 0.1370 - val_accuracy: 0.9858 - lr: 4.0000e-05\n",
      "Epoch 86/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1508 - accuracy: 0.9770 - val_loss: 0.4746 - val_accuracy: 0.8492 - lr: 4.0000e-05\n",
      "Epoch 87/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1486 - accuracy: 0.9780 - val_loss: 0.1096 - val_accuracy: 0.9896 - lr: 4.0000e-05\n",
      "Epoch 88/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1582 - accuracy: 0.9760 - val_loss: 0.1482 - val_accuracy: 0.9714 - lr: 4.0000e-05\n",
      "Epoch 89/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1490 - accuracy: 0.9771 - val_loss: 0.4954 - val_accuracy: 0.8478 - lr: 4.0000e-05\n",
      "Epoch 90/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1481 - accuracy: 0.9778 - val_loss: 0.1128 - val_accuracy: 0.9894 - lr: 4.0000e-05\n",
      "Epoch 91/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1514 - accuracy: 0.9765 - val_loss: 0.1216 - val_accuracy: 0.9863 - lr: 4.0000e-05\n",
      "Epoch 92/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1466 - accuracy: 0.9788 - val_loss: 0.1208 - val_accuracy: 0.9883 - lr: 4.0000e-05\n",
      "Epoch 93/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1480 - accuracy: 0.9764 - val_loss: 0.1225 - val_accuracy: 0.9845 - lr: 4.0000e-05\n",
      "Epoch 94/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1431 - accuracy: 0.9797 - val_loss: 0.1581 - val_accuracy: 0.9748 - lr: 4.0000e-05\n",
      "Epoch 95/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1383 - accuracy: 0.9805 - val_loss: 0.1087 - val_accuracy: 0.9894 - lr: 4.0000e-05\n",
      "Epoch 96/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1460 - accuracy: 0.9769 - val_loss: 0.3683 - val_accuracy: 0.8820 - lr: 4.0000e-05\n",
      "Epoch 97/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1411 - accuracy: 0.9792 - val_loss: 0.1737 - val_accuracy: 0.9662 - lr: 4.0000e-05\n",
      "Epoch 98/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1451 - accuracy: 0.9781 - val_loss: 0.1279 - val_accuracy: 0.9815 - lr: 4.0000e-05\n",
      "Epoch 99/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1444 - accuracy: 0.9796 - val_loss: 0.1094 - val_accuracy: 0.9894 - lr: 4.0000e-05\n",
      "Epoch 100/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1482 - accuracy: 0.9766 - val_loss: 0.1504 - val_accuracy: 0.9694 - lr: 4.0000e-05\n",
      "Epoch 101/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1389 - accuracy: 0.9805 - val_loss: 0.1238 - val_accuracy: 0.9863 - lr: 4.0000e-05\n",
      "Epoch 102/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1414 - accuracy: 0.9788 - val_loss: 0.1136 - val_accuracy: 0.9883 - lr: 4.0000e-05\n",
      "Epoch 103/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1427 - accuracy: 0.9779 - val_loss: 0.1440 - val_accuracy: 0.9782 - lr: 4.0000e-05\n",
      "Epoch 104/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1418 - accuracy: 0.9783 - val_loss: 0.3020 - val_accuracy: 0.9068 - lr: 4.0000e-05\n",
      "Epoch 105/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1408 - accuracy: 0.9785 - val_loss: 0.1685 - val_accuracy: 0.9696 - lr: 4.0000e-05\n",
      "Epoch 106/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1238 - accuracy: 0.9851 - val_loss: 0.1007 - val_accuracy: 0.9912 - lr: 8.0000e-06\n",
      "Epoch 107/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1193 - accuracy: 0.9852 - val_loss: 0.0989 - val_accuracy: 0.9908 - lr: 8.0000e-06\n",
      "Epoch 108/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1177 - accuracy: 0.9860 - val_loss: 0.1002 - val_accuracy: 0.9914 - lr: 8.0000e-06\n",
      "Epoch 109/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1165 - accuracy: 0.9863 - val_loss: 0.0985 - val_accuracy: 0.9896 - lr: 8.0000e-06\n",
      "Epoch 110/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1196 - accuracy: 0.9853 - val_loss: 0.0986 - val_accuracy: 0.9908 - lr: 8.0000e-06\n",
      "Epoch 111/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1180 - accuracy: 0.9863 - val_loss: 0.1054 - val_accuracy: 0.9903 - lr: 8.0000e-06\n",
      "Epoch 112/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1158 - accuracy: 0.9855 - val_loss: 0.0975 - val_accuracy: 0.9910 - lr: 8.0000e-06\n",
      "Epoch 113/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1164 - accuracy: 0.9868 - val_loss: 0.0983 - val_accuracy: 0.9919 - lr: 8.0000e-06\n",
      "Epoch 114/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1192 - accuracy: 0.9856 - val_loss: 0.1061 - val_accuracy: 0.9908 - lr: 8.0000e-06\n",
      "Epoch 115/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1170 - accuracy: 0.9853 - val_loss: 0.0995 - val_accuracy: 0.9912 - lr: 8.0000e-06\n",
      "Epoch 116/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1135 - accuracy: 0.9874 - val_loss: 0.0950 - val_accuracy: 0.9917 - lr: 8.0000e-06\n",
      "Epoch 117/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1165 - accuracy: 0.9861 - val_loss: 0.0948 - val_accuracy: 0.9905 - lr: 8.0000e-06\n",
      "Epoch 118/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1152 - accuracy: 0.9854 - val_loss: 0.0945 - val_accuracy: 0.9912 - lr: 8.0000e-06\n",
      "Epoch 119/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1155 - accuracy: 0.9862 - val_loss: 0.1003 - val_accuracy: 0.9908 - lr: 8.0000e-06\n",
      "Epoch 120/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1134 - accuracy: 0.9862 - val_loss: 0.1095 - val_accuracy: 0.9896 - lr: 8.0000e-06\n",
      "Epoch 121/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1165 - accuracy: 0.9863 - val_loss: 0.0973 - val_accuracy: 0.9914 - lr: 8.0000e-06\n",
      "Epoch 122/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1160 - accuracy: 0.9868 - val_loss: 0.0945 - val_accuracy: 0.9910 - lr: 8.0000e-06\n",
      "Epoch 123/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1117 - accuracy: 0.9867 - val_loss: 0.0947 - val_accuracy: 0.9919 - lr: 8.0000e-06\n",
      "Epoch 124/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1105 - accuracy: 0.9880 - val_loss: 0.0960 - val_accuracy: 0.9908 - lr: 8.0000e-06\n",
      "Epoch 125/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1131 - accuracy: 0.9860 - val_loss: 0.0976 - val_accuracy: 0.9908 - lr: 8.0000e-06\n",
      "Epoch 126/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1104 - accuracy: 0.9870 - val_loss: 0.0985 - val_accuracy: 0.9921 - lr: 8.0000e-06\n",
      "Epoch 127/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1160 - accuracy: 0.9839 - val_loss: 0.0983 - val_accuracy: 0.9896 - lr: 8.0000e-06\n",
      "Epoch 128/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1173 - accuracy: 0.9854 - val_loss: 0.1002 - val_accuracy: 0.9903 - lr: 8.0000e-06\n",
      "Epoch 129/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1115 - accuracy: 0.9870 - val_loss: 0.0972 - val_accuracy: 0.9910 - lr: 1.6000e-06\n",
      "Epoch 130/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1110 - accuracy: 0.9873 - val_loss: 0.0930 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 131/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1122 - accuracy: 0.9876 - val_loss: 0.0950 - val_accuracy: 0.9912 - lr: 1.6000e-06\n",
      "Epoch 132/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1049 - accuracy: 0.9885 - val_loss: 0.0925 - val_accuracy: 0.9919 - lr: 1.6000e-06\n",
      "Epoch 133/1000\n",
      "1111/1111 [==============================] - 17s 15ms/step - loss: 0.1099 - accuracy: 0.9872 - val_loss: 0.0930 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 134/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1058 - accuracy: 0.9887 - val_loss: 0.0927 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 135/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1115 - accuracy: 0.9859 - val_loss: 0.0922 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 136/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1079 - accuracy: 0.9880 - val_loss: 0.0920 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 137/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1138 - accuracy: 0.9864 - val_loss: 0.0918 - val_accuracy: 0.9910 - lr: 1.6000e-06\n",
      "Epoch 138/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1110 - accuracy: 0.9867 - val_loss: 0.0924 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 139/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1086 - accuracy: 0.9871 - val_loss: 0.0920 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 140/1000\n",
      "1111/1111 [==============================] - 17s 16ms/step - loss: 0.1098 - accuracy: 0.9875 - val_loss: 0.0913 - val_accuracy: 0.9919 - lr: 1.6000e-06\n",
      "Epoch 141/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1100 - accuracy: 0.9862 - val_loss: 0.0914 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 142/1000\n",
      "1111/1111 [==============================] - 23s 21ms/step - loss: 0.1113 - accuracy: 0.9871 - val_loss: 0.0922 - val_accuracy: 0.9921 - lr: 1.6000e-06\n",
      "Epoch 143/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1029 - accuracy: 0.9895 - val_loss: 0.0964 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 144/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1130 - accuracy: 0.9873 - val_loss: 0.0917 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 145/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1064 - accuracy: 0.9872 - val_loss: 0.0918 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 146/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1060 - accuracy: 0.9883 - val_loss: 0.0921 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 147/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1051 - accuracy: 0.9882 - val_loss: 0.0906 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 148/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1094 - accuracy: 0.9873 - val_loss: 0.0913 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 149/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1049 - accuracy: 0.9886 - val_loss: 0.0917 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 150/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1093 - accuracy: 0.9869 - val_loss: 0.0920 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 151/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1072 - accuracy: 0.9874 - val_loss: 0.0925 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 152/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1069 - accuracy: 0.9876 - val_loss: 0.0915 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 153/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1074 - accuracy: 0.9869 - val_loss: 0.0904 - val_accuracy: 0.9912 - lr: 1.6000e-06\n",
      "Epoch 154/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1073 - accuracy: 0.9881 - val_loss: 0.0949 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 155/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1047 - accuracy: 0.9881 - val_loss: 0.0927 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 156/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.1060 - accuracy: 0.9880 - val_loss: 0.0915 - val_accuracy: 0.9921 - lr: 1.6000e-06\n",
      "Epoch 157/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.1083 - accuracy: 0.9870 - val_loss: 0.0908 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 158/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1032 - accuracy: 0.9893 - val_loss: 0.0919 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 159/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1063 - accuracy: 0.9878 - val_loss: 0.0909 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 160/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1087 - accuracy: 0.9880 - val_loss: 0.0915 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 161/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1076 - accuracy: 0.9877 - val_loss: 0.0901 - val_accuracy: 0.9919 - lr: 1.6000e-06\n",
      "Epoch 162/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1076 - accuracy: 0.9877 - val_loss: 0.0899 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 163/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1061 - accuracy: 0.9881 - val_loss: 0.0907 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 164/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.1097 - accuracy: 0.9855 - val_loss: 0.0909 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 165/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.1078 - accuracy: 0.9871 - val_loss: 0.0904 - val_accuracy: 0.9919 - lr: 1.6000e-06\n",
      "Epoch 166/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.1080 - accuracy: 0.9872 - val_loss: 0.0903 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 167/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1043 - accuracy: 0.9886 - val_loss: 0.0917 - val_accuracy: 0.9912 - lr: 1.6000e-06\n",
      "Epoch 168/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1093 - accuracy: 0.9869 - val_loss: 0.0892 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 169/1000\n",
      "1111/1111 [==============================] - 18s 16ms/step - loss: 0.1060 - accuracy: 0.9880 - val_loss: 0.0904 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 170/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.1065 - accuracy: 0.9873 - val_loss: 0.0924 - val_accuracy: 0.9912 - lr: 1.6000e-06\n",
      "Epoch 171/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.1100 - accuracy: 0.9867 - val_loss: 0.0903 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 172/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1062 - accuracy: 0.9880 - val_loss: 0.0901 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 173/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.1080 - accuracy: 0.9875 - val_loss: 0.0898 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 174/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1064 - accuracy: 0.9887 - val_loss: 0.0908 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 175/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1071 - accuracy: 0.9873 - val_loss: 0.0898 - val_accuracy: 0.9912 - lr: 1.6000e-06\n",
      "Epoch 176/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.1083 - accuracy: 0.9877 - val_loss: 0.0931 - val_accuracy: 0.9914 - lr: 1.6000e-06\n",
      "Epoch 177/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1087 - accuracy: 0.9874 - val_loss: 0.0924 - val_accuracy: 0.9917 - lr: 1.6000e-06\n",
      "Epoch 178/1000\n",
      "1111/1111 [==============================] - 18s 17ms/step - loss: 0.1086 - accuracy: 0.9872 - val_loss: 0.0903 - val_accuracy: 0.9912 - lr: 1.6000e-06\n",
      "Epoch 179/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1026 - accuracy: 0.9882 - val_loss: 0.0914 - val_accuracy: 0.9914 - lr: 3.2000e-07\n",
      "Epoch 180/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1028 - accuracy: 0.9881 - val_loss: 0.0903 - val_accuracy: 0.9917 - lr: 3.2000e-07\n",
      "Epoch 181/1000\n",
      "1111/1111 [==============================] - 19s 17ms/step - loss: 0.1044 - accuracy: 0.9894 - val_loss: 0.0901 - val_accuracy: 0.9917 - lr: 3.2000e-07\n",
      "Epoch 182/1000\n",
      "1111/1111 [==============================] - 22s 19ms/step - loss: 0.1034 - accuracy: 0.9892 - val_loss: 0.0912 - val_accuracy: 0.9914 - lr: 3.2000e-07\n",
      "Epoch 183/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1025 - accuracy: 0.9887 - val_loss: 0.0902 - val_accuracy: 0.9917 - lr: 3.2000e-07\n",
      "Epoch 184/1000\n",
      "1111/1111 [==============================] - 23s 21ms/step - loss: 0.1042 - accuracy: 0.9884 - val_loss: 0.0901 - val_accuracy: 0.9917 - lr: 3.2000e-07\n",
      "Epoch 185/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.1046 - accuracy: 0.9878 - val_loss: 0.0897 - val_accuracy: 0.9917 - lr: 3.2000e-07\n",
      "Epoch 186/1000\n",
      "1111/1111 [==============================] - 22s 19ms/step - loss: 0.1061 - accuracy: 0.9880 - val_loss: 0.0901 - val_accuracy: 0.9917 - lr: 3.2000e-07\n",
      "Epoch 187/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.1128 - accuracy: 0.9843 - val_loss: 0.0905 - val_accuracy: 0.9917 - lr: 3.2000e-07\n",
      "Epoch 188/1000\n",
      "1111/1111 [==============================] - 22s 19ms/step - loss: 0.1046 - accuracy: 0.9884 - val_loss: 0.0904 - val_accuracy: 0.9917 - lr: 3.2000e-07\n",
      "Epoch 189/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1049 - accuracy: 0.9878 - val_loss: 0.0903 - val_accuracy: 0.9917 - lr: 6.4000e-08\n",
      "Epoch 190/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.1060 - accuracy: 0.9887 - val_loss: 0.0904 - val_accuracy: 0.9914 - lr: 6.4000e-08\n",
      "Epoch 191/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.1064 - accuracy: 0.9881 - val_loss: 0.0899 - val_accuracy: 0.9917 - lr: 6.4000e-08\n",
      "Epoch 192/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1081 - accuracy: 0.9870 - val_loss: 0.0905 - val_accuracy: 0.9919 - lr: 6.4000e-08\n",
      "Epoch 193/1000\n",
      "1111/1111 [==============================] - 22s 19ms/step - loss: 0.1043 - accuracy: 0.9889 - val_loss: 0.0898 - val_accuracy: 0.9917 - lr: 6.4000e-08\n",
      "Epoch 194/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1022 - accuracy: 0.9892 - val_loss: 0.0901 - val_accuracy: 0.9917 - lr: 6.4000e-08\n",
      "Epoch 195/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1039 - accuracy: 0.9881 - val_loss: 0.0898 - val_accuracy: 0.9917 - lr: 6.4000e-08\n",
      "Epoch 196/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.1032 - accuracy: 0.9882 - val_loss: 0.0921 - val_accuracy: 0.9914 - lr: 6.4000e-08\n",
      "Epoch 197/1000\n",
      "1111/1111 [==============================] - 22s 19ms/step - loss: 0.1043 - accuracy: 0.9873 - val_loss: 0.0907 - val_accuracy: 0.9917 - lr: 6.4000e-08\n",
      "Epoch 198/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1060 - accuracy: 0.9878 - val_loss: 0.0904 - val_accuracy: 0.9914 - lr: 6.4000e-08\n",
      "Epoch 199/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.1022 - accuracy: 0.9890 - val_loss: 0.0904 - val_accuracy: 0.9914 - lr: 1.2800e-08\n",
      "Epoch 200/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.1065 - accuracy: 0.9883 - val_loss: 0.0915 - val_accuracy: 0.9917 - lr: 1.2800e-08\n",
      "Epoch 201/1000\n",
      "1111/1111 [==============================] - 23s 20ms/step - loss: 0.1037 - accuracy: 0.9887 - val_loss: 0.0906 - val_accuracy: 0.9914 - lr: 1.2800e-08\n",
      "Epoch 202/1000\n",
      "1111/1111 [==============================] - 22s 19ms/step - loss: 0.1104 - accuracy: 0.9879 - val_loss: 0.0906 - val_accuracy: 0.9914 - lr: 1.2800e-08\n",
      "Epoch 203/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.1077 - accuracy: 0.9876 - val_loss: 0.0901 - val_accuracy: 0.9917 - lr: 1.2800e-08\n",
      "Epoch 204/1000\n",
      "1111/1111 [==============================] - 22s 19ms/step - loss: 0.1049 - accuracy: 0.9875 - val_loss: 0.0901 - val_accuracy: 0.9917 - lr: 1.2800e-08\n",
      "Epoch 205/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.1092 - accuracy: 0.9873 - val_loss: 0.0908 - val_accuracy: 0.9912 - lr: 1.2800e-08\n",
      "Epoch 206/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1065 - accuracy: 0.9876 - val_loss: 0.0900 - val_accuracy: 0.9917 - lr: 1.2800e-08\n",
      "Epoch 207/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1017 - accuracy: 0.9895 - val_loss: 0.0900 - val_accuracy: 0.9919 - lr: 1.2800e-08\n",
      "Epoch 208/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1063 - accuracy: 0.9883 - val_loss: 0.0899 - val_accuracy: 0.9917 - lr: 1.2800e-08\n",
      "Epoch 209/1000\n",
      "1111/1111 [==============================] - 22s 19ms/step - loss: 0.1044 - accuracy: 0.9879 - val_loss: 0.0896 - val_accuracy: 0.9919 - lr: 2.5600e-09\n",
      "Epoch 210/1000\n",
      "1111/1111 [==============================] - 22s 19ms/step - loss: 0.1044 - accuracy: 0.9878 - val_loss: 0.0907 - val_accuracy: 0.9917 - lr: 2.5600e-09\n",
      "Epoch 211/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1074 - accuracy: 0.9875 - val_loss: 0.0906 - val_accuracy: 0.9917 - lr: 2.5600e-09\n",
      "Epoch 212/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1034 - accuracy: 0.9892 - val_loss: 0.0898 - val_accuracy: 0.9917 - lr: 2.5600e-09\n",
      "Epoch 213/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1103 - accuracy: 0.9854 - val_loss: 0.0905 - val_accuracy: 0.9917 - lr: 2.5600e-09\n",
      "Epoch 214/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1046 - accuracy: 0.9877 - val_loss: 0.0913 - val_accuracy: 0.9914 - lr: 2.5600e-09\n",
      "Epoch 215/1000\n",
      "1111/1111 [==============================] - 21s 19ms/step - loss: 0.1060 - accuracy: 0.9875 - val_loss: 0.0900 - val_accuracy: 0.9917 - lr: 2.5600e-09\n",
      "Epoch 216/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1022 - accuracy: 0.9891 - val_loss: 0.0904 - val_accuracy: 0.9914 - lr: 2.5600e-09\n",
      "Epoch 217/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1104 - accuracy: 0.9871 - val_loss: 0.0902 - val_accuracy: 0.9917 - lr: 2.5600e-09\n",
      "Epoch 218/1000\n",
      "1111/1111 [==============================] - 22s 20ms/step - loss: 0.1055 - accuracy: 0.9876 - val_loss: 0.0907 - val_accuracy: 0.9917 - lr: 2.5600e-09\n"
     ]
    }
   ],
   "source": [
    "# Definir el modelo LSTM\n",
    "model = Sequential()\n",
    "l2_recurrent_parameter = 1\n",
    "l_2_kernel_regularizer= 1\n",
    "# Primera capa LSTM con Dropout y regularización L2\n",
    "model.add(LSTM(256, return_sequences=True, input_shape=(X_train_normalized.shape[1], X_train_normalized.shape[2]),\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=True,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=True,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=False,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compilación del modelo\n",
    "with tf.device('GPU:0'): # Usar gpu\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=\"models\\Full\",\n",
    "#     monitor='val_accuracy',\n",
    "#     mode='max',\n",
    "#     save_best_only=True)\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=10, min_lr=0.0000000001)\n",
    "\n",
    "history = model.fit(X_train_normalized, y_train, verbose=1, epochs=1000, batch_size=16,\n",
    "                    validation_data=(X_test_normalized, y_test), callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "60231ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-5\n",
      "139/139 [==============================] - 2s 7ms/step - loss: 5.2524 - accuracy: 0.4604\n",
      "5-10\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 7.8394 - accuracy: 0.4845\n",
      "10-15\n",
      "139/139 [==============================] - 1s 6ms/step - loss: 8.3760 - accuracy: 0.5036\n",
      "15-20\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 7.7167 - accuracy: 0.5209\n",
      "20-25\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 6.7070 - accuracy: 0.5358\n",
      "25-30\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 5.8453 - accuracy: 0.5511\n",
      "30-35\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 4.9963 - accuracy: 0.5646\n",
      "35-40\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 4.1898 - accuracy: 0.5835\n",
      "40-45\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 3.5135 - accuracy: 0.5997\n",
      "45-50\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 2.8880 - accuracy: 0.6195\n",
      "50-55\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 2.4145 - accuracy: 0.6407\n",
      "55-60\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 2.0228 - accuracy: 0.6907\n",
      "60-65\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 1.7527 - accuracy: 0.7240\n",
      "65-70\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 1.5048 - accuracy: 0.7605\n",
      "70-75\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 1.2133 - accuracy: 0.8008\n",
      "75-80\n",
      "139/139 [==============================] - 1s 7ms/step - loss: 0.9645 - accuracy: 0.8393\n",
      "80-85\n",
      "139/139 [==============================] - 1s 8ms/step - loss: 0.6360 - accuracy: 0.8928\n",
      "85-90\n",
      "139/139 [==============================] - 1s 8ms/step - loss: 0.0892 - accuracy: 0.9914\n"
     ]
    }
   ],
   "source": [
    "loss, acc = [], []\n",
    "for i in range(1, X_test_normalized.shape[1]+1):\n",
    "    print(intervals[i-1])\n",
    "    aux = model.evaluate(X_test_normalized[:, :i, :], y_test)\n",
    "    loss.append(aux[0])\n",
    "    acc.append(aux[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c07d61",
   "metadata": {},
   "source": [
    "## Pruebas con padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "26312ad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Media de las muestras: 12.585438335809807\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import betabinom\n",
    "\n",
    "# Parámetros de la distribución beta-binomial\n",
    "n = X.shape[1]   # Número de ensayos\n",
    "a = 0.70*4  # Parámetro alpha de la distribución beta\n",
    "b = 0.30*4   # Parámetro beta de la distribución beta\n",
    "\n",
    "# Número de muestras aleatorias que deseas generar\n",
    "num_samples = X.shape[0]\n",
    "\n",
    "# Generar números aleatorios de una distribución beta-binomial\n",
    "samples = betabinom.rvs(n, a, b, size=num_samples)\n",
    "\n",
    "# Calcular y mostrar la media de las muestras generadas\n",
    "mean_samples = np.mean(samples)\n",
    "print(f\"Media de las muestras: {mean_samples}\")\n",
    "\n",
    "values, counts = np.unique(samples, return_counts=True)\n",
    "\n",
    "# Graficar un barplot de las muestras generadas\n",
    "plt.bar(values, counts / num_samples, color='b', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Títulos y etiquetas\n",
    "plt.title('Histograma de la distribución Beta-Binomial')\n",
    "plt.xlabel('Valores')\n",
    "plt.ylabel('Frecuencia')\n",
    "\n",
    "# Mostrar la gráfica\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "e028c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data\n",
    "XFullpad = np.array(Xdata)\n",
    "y = np.array(ydata)\n",
    "y = to_categorical(y, num_classes=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "97429197",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "\n",
    "split = int(0.8 * XFullpad.shape[0])\n",
    "XFullpad_train, XFullpad_test = XFullpad[:split], XFullpad[split:]\n",
    "\n",
    "# # Sin clean\n",
    "# scaler = MinMaxScaler()\n",
    "# XFullpad_train_reshaped = XFullpad_train.reshape(-1, X.shape[2])\n",
    "# XFullpad_train_normalized = scaler.fit_transform(XFullpad_train_reshaped)\n",
    "# XFullpad_train_normalized = XFullpad_train_normalized.reshape(XFullpad_train.shape[0], XFullpad_train.shape[1], XFullpad_train.shape[2])\n",
    "\n",
    "# XFullpad_test_reshaped = XFullpad_test.reshape(-1, X.shape[2])\n",
    "# XFullpad_test_normalized = scaler.transform(XFullpad_test_reshaped)\n",
    "# XFullpad_test_normalized = XFullpad_test_normalized.reshape(XFullpad_test.shape[0], XFullpad_test.shape[1], XFullpad_test.shape[2])\n",
    "\n",
    "# Con clean \n",
    "XFullpad_train_reshaped = XFullpad_train.reshape(-1, X.shape[2])\n",
    "XFullpad_test_reshaped = XFullpad_test.reshape(-1, X.shape[2])\n",
    "\n",
    "sclean  = clean(XFullpad_train_reshaped)      # indices of selected features\n",
    "XFullpad_train_reshaped = deepcopy(XFullpad_train_reshaped[:,sclean])\n",
    "XFullpad_test_reshaped = deepcopy(XFullpad_test_reshaped[:,sclean])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "XFullpad_train_normalized = scaler.fit_transform(XFullpad_train_reshaped)\n",
    "XFullpad_test_normalized = scaler.transform(XFullpad_test_reshaped)\n",
    "\n",
    "XFullpad_train_normalized = XFullpad_train_normalized.reshape(XFullpad_train.shape[0], XFullpad_train.shape[1], len(sclean))\n",
    "XFullpad_test_normalized = XFullpad_test_normalized.reshape(XFullpad_test.shape[0], XFullpad_test.shape[1], len(sclean))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "fbfd5eda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17767, 18, 766)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "XFullpad_train_normalized.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e0ab2690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parámetros de la distribución binomial \n",
    "# n = X.shape[1]\n",
    "# p = 0.9\n",
    "# size = X.shape[0]\n",
    "# random_values = binom.rvs(n, p, size=size)\n",
    "\n",
    "# Parámetros de la distribución beta-binomial\n",
    "n = XFullpad.shape[1]   # Número de ensayos\n",
    "size = XFullpad.shape[0]\n",
    "a = 0.70*4  # Parámetro alpha de la distribución beta\n",
    "b = 0.30*4   # Parámetro beta de la distribución beta\n",
    "\n",
    "random_values = betabinom.rvs(n, a, b, size=size)\n",
    "\n",
    "# random_values = np.random.randint(1, n+1, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6bbedd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Binom  and uniform\n",
    "Xpad_train = []\n",
    "ypad_train = []\n",
    "for i in range(split):\n",
    "    Xpad_train.append(XFullpad_train_normalized[i][:random_values[i]+1])\n",
    "    ypad_train.append(y[i])\n",
    "\n",
    "Xpad_test = []\n",
    "ypad_test = []\n",
    "for i in range(len(XFullpad)-split):\n",
    "    Xpad_test.append(XFullpad_test_normalized[i][:random_values[i+split]+1])\n",
    "    ypad_test.append(y[i+split])\n",
    "\n",
    "# Full\n",
    "# Xpad = []\n",
    "# ypad = []\n",
    "# for i in range(size):\n",
    "#     for j in range(n):\n",
    "#         Xpad.append(X[i][:j+1])\n",
    "#         ypad.append(y[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "843a0de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Xpad_train = pad_sequences(Xpad_train, padding='post', dtype='float64')\n",
    "Xpad_test = pad_sequences(Xpad_test, padding='post', dtype='float64')\n",
    "ypad_train = np.array(ypad_train)\n",
    "ypad_test = np.array(ypad_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "31e6f703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17767, 3)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ypad_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "262cd47f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17767, 18, 766)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xpad_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "bcf7cb75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1111/1111 [==============================] - 46s 32ms/step - loss: 3.2970 - accuracy: 0.5336 - val_loss: 1.6510 - val_accuracy: 0.4430 - lr: 0.0010\n",
      "Epoch 2/1000\n",
      "1111/1111 [==============================] - 32s 29ms/step - loss: 1.2033 - accuracy: 0.5698 - val_loss: 1.0740 - val_accuracy: 0.6594 - lr: 0.0010\n",
      "Epoch 3/1000\n",
      "1111/1111 [==============================] - 33s 29ms/step - loss: 0.9345 - accuracy: 0.6410 - val_loss: 0.8355 - val_accuracy: 0.6713 - lr: 0.0010\n",
      "Epoch 4/1000\n",
      "1111/1111 [==============================] - 33s 29ms/step - loss: 0.8431 - accuracy: 0.6703 - val_loss: 0.9012 - val_accuracy: 0.6083 - lr: 0.0010\n",
      "Epoch 5/1000\n",
      "1111/1111 [==============================] - 32s 29ms/step - loss: 0.7967 - accuracy: 0.6873 - val_loss: 0.7852 - val_accuracy: 0.6974 - lr: 0.0010\n",
      "Epoch 6/1000\n",
      "1111/1111 [==============================] - 33s 29ms/step - loss: 0.7764 - accuracy: 0.6951 - val_loss: 0.7291 - val_accuracy: 0.7132 - lr: 0.0010\n",
      "Epoch 7/1000\n",
      "1111/1111 [==============================] - 33s 29ms/step - loss: 0.7625 - accuracy: 0.6979 - val_loss: 0.7406 - val_accuracy: 0.6902 - lr: 0.0010\n",
      "Epoch 8/1000\n",
      "1111/1111 [==============================] - 32s 29ms/step - loss: 0.7463 - accuracy: 0.7017 - val_loss: 1.0200 - val_accuracy: 0.5383 - lr: 0.0010\n",
      "Epoch 9/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.7350 - accuracy: 0.7045 - val_loss: 0.6730 - val_accuracy: 0.7357 - lr: 0.0010\n",
      "Epoch 10/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.7233 - accuracy: 0.7089 - val_loss: 0.7750 - val_accuracy: 0.7080 - lr: 0.0010\n",
      "Epoch 11/1000\n",
      "1111/1111 [==============================] - 32s 29ms/step - loss: 0.7285 - accuracy: 0.7056 - val_loss: 0.7175 - val_accuracy: 0.7368 - lr: 0.0010\n",
      "Epoch 12/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.7157 - accuracy: 0.7146 - val_loss: 0.6723 - val_accuracy: 0.7341 - lr: 0.0010\n",
      "Epoch 13/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.7160 - accuracy: 0.7128 - val_loss: 0.9575 - val_accuracy: 0.6252 - lr: 0.0010\n",
      "Epoch 14/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.7092 - accuracy: 0.7116 - val_loss: 0.9469 - val_accuracy: 0.5720 - lr: 0.0010\n",
      "Epoch 15/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.7059 - accuracy: 0.7148 - val_loss: 0.6979 - val_accuracy: 0.7258 - lr: 0.0010\n",
      "Epoch 16/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.7079 - accuracy: 0.7163 - val_loss: 0.7490 - val_accuracy: 0.6961 - lr: 0.0010\n",
      "Epoch 17/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.7080 - accuracy: 0.7167 - val_loss: 0.8747 - val_accuracy: 0.5723 - lr: 0.0010\n",
      "Epoch 18/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6741 - accuracy: 0.7285 - val_loss: 0.8154 - val_accuracy: 0.6362 - lr: 5.0000e-04\n",
      "Epoch 19/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6690 - accuracy: 0.7332 - val_loss: 0.7900 - val_accuracy: 0.6531 - lr: 5.0000e-04\n",
      "Epoch 20/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6675 - accuracy: 0.7345 - val_loss: 0.6966 - val_accuracy: 0.6972 - lr: 5.0000e-04\n",
      "Epoch 21/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6677 - accuracy: 0.7318 - val_loss: 0.6476 - val_accuracy: 0.7461 - lr: 5.0000e-04\n",
      "Epoch 22/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6615 - accuracy: 0.7344 - val_loss: 0.6972 - val_accuracy: 0.7211 - lr: 5.0000e-04\n",
      "Epoch 23/1000\n",
      "1111/1111 [==============================] - 33s 29ms/step - loss: 0.6603 - accuracy: 0.7344 - val_loss: 0.7018 - val_accuracy: 0.7571 - lr: 5.0000e-04\n",
      "Epoch 24/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6633 - accuracy: 0.7323 - val_loss: 0.6823 - val_accuracy: 0.7537 - lr: 5.0000e-04\n",
      "Epoch 25/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6637 - accuracy: 0.7321 - val_loss: 0.6444 - val_accuracy: 0.7485 - lr: 5.0000e-04\n",
      "Epoch 26/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6630 - accuracy: 0.7354 - val_loss: 0.7956 - val_accuracy: 0.6285 - lr: 5.0000e-04\n",
      "Epoch 27/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6530 - accuracy: 0.7379 - val_loss: 0.6956 - val_accuracy: 0.7217 - lr: 5.0000e-04\n",
      "Epoch 28/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6592 - accuracy: 0.7360 - val_loss: 0.6510 - val_accuracy: 0.7265 - lr: 5.0000e-04\n",
      "Epoch 29/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6535 - accuracy: 0.7357 - val_loss: 0.7072 - val_accuracy: 0.6905 - lr: 5.0000e-04\n",
      "Epoch 30/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6552 - accuracy: 0.7362 - val_loss: 0.7068 - val_accuracy: 0.7337 - lr: 5.0000e-04\n",
      "Epoch 31/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.6308 - accuracy: 0.7510 - val_loss: 0.7471 - val_accuracy: 0.6641 - lr: 2.5000e-04\n",
      "Epoch 32/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.6362 - accuracy: 0.7469 - val_loss: 0.8289 - val_accuracy: 0.5889 - lr: 2.5000e-04\n",
      "Epoch 33/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.6305 - accuracy: 0.7513 - val_loss: 0.6261 - val_accuracy: 0.7449 - lr: 2.5000e-04\n",
      "Epoch 34/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6283 - accuracy: 0.7502 - val_loss: 0.6234 - val_accuracy: 0.7609 - lr: 2.5000e-04\n",
      "Epoch 35/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.6316 - accuracy: 0.7495 - val_loss: 0.6435 - val_accuracy: 0.7521 - lr: 2.5000e-04\n",
      "Epoch 36/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.6263 - accuracy: 0.7489 - val_loss: 0.7850 - val_accuracy: 0.6637 - lr: 2.5000e-04\n",
      "Epoch 37/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6277 - accuracy: 0.7475 - val_loss: 0.6297 - val_accuracy: 0.7501 - lr: 2.5000e-04\n",
      "Epoch 38/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6306 - accuracy: 0.7473 - val_loss: 0.6535 - val_accuracy: 0.7524 - lr: 2.5000e-04\n",
      "Epoch 39/1000\n",
      "1111/1111 [==============================] - 35s 31ms/step - loss: 0.6250 - accuracy: 0.7509 - val_loss: 0.6352 - val_accuracy: 0.7488 - lr: 2.5000e-04\n",
      "Epoch 40/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6109 - accuracy: 0.7574 - val_loss: 0.5964 - val_accuracy: 0.7672 - lr: 1.2500e-04\n",
      "Epoch 41/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6125 - accuracy: 0.7601 - val_loss: 0.5937 - val_accuracy: 0.7674 - lr: 1.2500e-04\n",
      "Epoch 42/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6115 - accuracy: 0.7593 - val_loss: 0.6322 - val_accuracy: 0.7481 - lr: 1.2500e-04\n",
      "Epoch 43/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6100 - accuracy: 0.7605 - val_loss: 0.6474 - val_accuracy: 0.7452 - lr: 1.2500e-04\n",
      "Epoch 44/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6081 - accuracy: 0.7558 - val_loss: 0.6377 - val_accuracy: 0.7344 - lr: 1.2500e-04\n",
      "Epoch 45/1000\n",
      "1111/1111 [==============================] - 35s 31ms/step - loss: 0.6082 - accuracy: 0.7593 - val_loss: 0.5953 - val_accuracy: 0.7654 - lr: 1.2500e-04\n",
      "Epoch 46/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6079 - accuracy: 0.7579 - val_loss: 0.6306 - val_accuracy: 0.7530 - lr: 1.2500e-04\n",
      "Epoch 47/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.6027 - accuracy: 0.7600 - val_loss: 0.6008 - val_accuracy: 0.7614 - lr: 6.2500e-05\n",
      "Epoch 48/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5969 - accuracy: 0.7651 - val_loss: 0.5970 - val_accuracy: 0.7645 - lr: 6.2500e-05\n",
      "Epoch 49/1000\n",
      "1111/1111 [==============================] - 36s 33ms/step - loss: 0.5991 - accuracy: 0.7624 - val_loss: 0.5903 - val_accuracy: 0.7647 - lr: 6.2500e-05\n",
      "Epoch 50/1000\n",
      "1111/1111 [==============================] - 35s 31ms/step - loss: 0.5974 - accuracy: 0.7654 - val_loss: 0.5988 - val_accuracy: 0.7647 - lr: 6.2500e-05\n",
      "Epoch 51/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5993 - accuracy: 0.7644 - val_loss: 0.6054 - val_accuracy: 0.7638 - lr: 6.2500e-05\n",
      "Epoch 52/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5936 - accuracy: 0.7668 - val_loss: 0.5938 - val_accuracy: 0.7672 - lr: 6.2500e-05\n",
      "Epoch 53/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5911 - accuracy: 0.7668 - val_loss: 0.6011 - val_accuracy: 0.7632 - lr: 6.2500e-05\n",
      "Epoch 54/1000\n",
      "1111/1111 [==============================] - 35s 32ms/step - loss: 0.5936 - accuracy: 0.7648 - val_loss: 0.5859 - val_accuracy: 0.7677 - lr: 6.2500e-05\n",
      "Epoch 55/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.5916 - accuracy: 0.7657 - val_loss: 0.5973 - val_accuracy: 0.7645 - lr: 6.2500e-05\n",
      "Epoch 56/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5884 - accuracy: 0.7665 - val_loss: 0.5912 - val_accuracy: 0.7670 - lr: 6.2500e-05\n",
      "Epoch 57/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5895 - accuracy: 0.7670 - val_loss: 0.6152 - val_accuracy: 0.7553 - lr: 6.2500e-05\n",
      "Epoch 58/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5935 - accuracy: 0.7666 - val_loss: 0.5865 - val_accuracy: 0.7681 - lr: 6.2500e-05\n",
      "Epoch 59/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5863 - accuracy: 0.7694 - val_loss: 0.5883 - val_accuracy: 0.7674 - lr: 6.2500e-05\n",
      "Epoch 60/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.5844 - accuracy: 0.7711 - val_loss: 0.5928 - val_accuracy: 0.7654 - lr: 3.1250e-05\n",
      "Epoch 61/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5822 - accuracy: 0.7733 - val_loss: 0.5875 - val_accuracy: 0.7672 - lr: 3.1250e-05\n",
      "Epoch 62/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.5872 - accuracy: 0.7703 - val_loss: 0.5851 - val_accuracy: 0.7652 - lr: 3.1250e-05\n",
      "Epoch 63/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5822 - accuracy: 0.7720 - val_loss: 0.5890 - val_accuracy: 0.7659 - lr: 3.1250e-05\n",
      "Epoch 64/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5846 - accuracy: 0.7714 - val_loss: 0.5896 - val_accuracy: 0.7663 - lr: 3.1250e-05\n",
      "Epoch 65/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5800 - accuracy: 0.7758 - val_loss: 0.5852 - val_accuracy: 0.7674 - lr: 3.1250e-05\n",
      "Epoch 66/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5799 - accuracy: 0.7702 - val_loss: 0.5882 - val_accuracy: 0.7656 - lr: 3.1250e-05\n",
      "Epoch 67/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5770 - accuracy: 0.7741 - val_loss: 0.5927 - val_accuracy: 0.7638 - lr: 3.1250e-05\n",
      "Epoch 68/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.5805 - accuracy: 0.7721 - val_loss: 0.5886 - val_accuracy: 0.7665 - lr: 1.5625e-05\n",
      "Epoch 69/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5762 - accuracy: 0.7734 - val_loss: 0.5871 - val_accuracy: 0.7677 - lr: 1.5625e-05\n",
      "Epoch 70/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5755 - accuracy: 0.7754 - val_loss: 0.5847 - val_accuracy: 0.7641 - lr: 1.5625e-05\n",
      "Epoch 71/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5800 - accuracy: 0.7735 - val_loss: 0.5853 - val_accuracy: 0.7656 - lr: 1.5625e-05\n",
      "Epoch 72/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5748 - accuracy: 0.7743 - val_loss: 0.5921 - val_accuracy: 0.7643 - lr: 1.5625e-05\n",
      "Epoch 73/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5785 - accuracy: 0.7723 - val_loss: 0.5876 - val_accuracy: 0.7650 - lr: 1.5625e-05\n",
      "Epoch 74/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5745 - accuracy: 0.7753 - val_loss: 0.5857 - val_accuracy: 0.7672 - lr: 1.5625e-05\n",
      "Epoch 75/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5739 - accuracy: 0.7755 - val_loss: 0.5879 - val_accuracy: 0.7663 - lr: 1.5625e-05\n",
      "Epoch 76/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.5753 - accuracy: 0.7762 - val_loss: 0.5860 - val_accuracy: 0.7665 - lr: 7.8125e-06\n",
      "Epoch 77/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5742 - accuracy: 0.7781 - val_loss: 0.5841 - val_accuracy: 0.7686 - lr: 7.8125e-06\n",
      "Epoch 78/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5744 - accuracy: 0.7740 - val_loss: 0.5845 - val_accuracy: 0.7668 - lr: 7.8125e-06\n",
      "Epoch 79/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5747 - accuracy: 0.7753 - val_loss: 0.5854 - val_accuracy: 0.7656 - lr: 7.8125e-06\n",
      "Epoch 80/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5762 - accuracy: 0.7772 - val_loss: 0.5860 - val_accuracy: 0.7663 - lr: 7.8125e-06\n",
      "Epoch 81/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5801 - accuracy: 0.7755 - val_loss: 0.5867 - val_accuracy: 0.7690 - lr: 7.8125e-06\n",
      "Epoch 82/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5752 - accuracy: 0.7751 - val_loss: 0.5847 - val_accuracy: 0.7672 - lr: 7.8125e-06\n",
      "Epoch 83/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5708 - accuracy: 0.7768 - val_loss: 0.5859 - val_accuracy: 0.7670 - lr: 3.9063e-06\n",
      "Epoch 84/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5743 - accuracy: 0.7756 - val_loss: 0.5844 - val_accuracy: 0.7663 - lr: 3.9063e-06\n",
      "Epoch 85/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5748 - accuracy: 0.7762 - val_loss: 0.5843 - val_accuracy: 0.7668 - lr: 3.9063e-06\n",
      "Epoch 86/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5738 - accuracy: 0.7723 - val_loss: 0.5861 - val_accuracy: 0.7683 - lr: 3.9063e-06\n",
      "Epoch 87/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5760 - accuracy: 0.7760 - val_loss: 0.5865 - val_accuracy: 0.7674 - lr: 3.9063e-06\n",
      "Epoch 88/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.5745 - accuracy: 0.7737 - val_loss: 0.5845 - val_accuracy: 0.7686 - lr: 1.9531e-06\n",
      "Epoch 89/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5721 - accuracy: 0.7750 - val_loss: 0.5840 - val_accuracy: 0.7656 - lr: 1.9531e-06\n",
      "Epoch 90/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5743 - accuracy: 0.7769 - val_loss: 0.5852 - val_accuracy: 0.7672 - lr: 1.9531e-06\n",
      "Epoch 91/1000\n",
      "1111/1111 [==============================] - 36s 32ms/step - loss: 0.5695 - accuracy: 0.7755 - val_loss: 0.5846 - val_accuracy: 0.7688 - lr: 1.9531e-06\n",
      "Epoch 92/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.5739 - accuracy: 0.7742 - val_loss: 0.5854 - val_accuracy: 0.7656 - lr: 1.9531e-06\n",
      "Epoch 93/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5744 - accuracy: 0.7746 - val_loss: 0.5849 - val_accuracy: 0.7677 - lr: 9.7656e-07\n",
      "Epoch 94/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5744 - accuracy: 0.7758 - val_loss: 0.5846 - val_accuracy: 0.7663 - lr: 9.7656e-07\n",
      "Epoch 95/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5723 - accuracy: 0.7735 - val_loss: 0.5847 - val_accuracy: 0.7688 - lr: 9.7656e-07\n",
      "Epoch 96/1000\n",
      "1111/1111 [==============================] - 33s 29ms/step - loss: 0.5663 - accuracy: 0.7755 - val_loss: 0.5845 - val_accuracy: 0.7679 - lr: 9.7656e-07\n",
      "Epoch 97/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.5695 - accuracy: 0.7740 - val_loss: 0.5847 - val_accuracy: 0.7665 - lr: 9.7656e-07\n",
      "Epoch 98/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5713 - accuracy: 0.7778 - val_loss: 0.5856 - val_accuracy: 0.7663 - lr: 4.8828e-07\n",
      "Epoch 99/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5739 - accuracy: 0.7750 - val_loss: 0.5843 - val_accuracy: 0.7665 - lr: 4.8828e-07\n",
      "Epoch 100/1000\n",
      "1111/1111 [==============================] - 35s 31ms/step - loss: 0.5690 - accuracy: 0.7781 - val_loss: 0.5847 - val_accuracy: 0.7663 - lr: 4.8828e-07\n",
      "Epoch 101/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5695 - accuracy: 0.7744 - val_loss: 0.5848 - val_accuracy: 0.7677 - lr: 4.8828e-07\n",
      "Epoch 102/1000\n",
      "1111/1111 [==============================] - 33s 30ms/step - loss: 0.5725 - accuracy: 0.7770 - val_loss: 0.5846 - val_accuracy: 0.7683 - lr: 4.8828e-07\n",
      "Epoch 103/1000\n",
      "1111/1111 [==============================] - 35s 31ms/step - loss: 0.5756 - accuracy: 0.7742 - val_loss: 0.5861 - val_accuracy: 0.7674 - lr: 2.4414e-07\n",
      "Epoch 104/1000\n",
      "1111/1111 [==============================] - 34s 30ms/step - loss: 0.5739 - accuracy: 0.7732 - val_loss: 0.5842 - val_accuracy: 0.7679 - lr: 2.4414e-07\n",
      "Epoch 105/1000\n",
      "1111/1111 [==============================] - 34s 31ms/step - loss: 0.5725 - accuracy: 0.7748 - val_loss: 0.5848 - val_accuracy: 0.7663 - lr: 2.4414e-07\n",
      "Epoch 106/1000\n",
      "1111/1111 [==============================] - 35s 32ms/step - loss: 0.5726 - accuracy: 0.7735 - val_loss: 0.5847 - val_accuracy: 0.7677 - lr: 2.4414e-07\n",
      "Epoch 107/1000\n",
      "1111/1111 [==============================] - 32s 29ms/step - loss: 0.5712 - accuracy: 0.7777 - val_loss: 0.5843 - val_accuracy: 0.7672 - lr: 2.4414e-07\n",
      "Epoch 108/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5697 - accuracy: 0.7763 - val_loss: 0.5851 - val_accuracy: 0.7674 - lr: 1.2207e-07\n",
      "Epoch 109/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5733 - accuracy: 0.7735 - val_loss: 0.5847 - val_accuracy: 0.7672 - lr: 1.2207e-07\n",
      "Epoch 110/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5720 - accuracy: 0.7768 - val_loss: 0.5842 - val_accuracy: 0.7677 - lr: 1.2207e-07\n",
      "Epoch 111/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5717 - accuracy: 0.7762 - val_loss: 0.5844 - val_accuracy: 0.7668 - lr: 1.2207e-07\n",
      "Epoch 112/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5706 - accuracy: 0.7772 - val_loss: 0.5842 - val_accuracy: 0.7668 - lr: 1.2207e-07\n",
      "Epoch 113/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5736 - accuracy: 0.7735 - val_loss: 0.5848 - val_accuracy: 0.7672 - lr: 6.1035e-08\n",
      "Epoch 114/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5717 - accuracy: 0.7742 - val_loss: 0.5846 - val_accuracy: 0.7668 - lr: 6.1035e-08\n",
      "Epoch 115/1000\n",
      "1111/1111 [==============================] - 32s 29ms/step - loss: 0.5705 - accuracy: 0.7764 - val_loss: 0.5847 - val_accuracy: 0.7668 - lr: 6.1035e-08\n",
      "Epoch 116/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5733 - accuracy: 0.7760 - val_loss: 0.5848 - val_accuracy: 0.7668 - lr: 6.1035e-08\n",
      "Epoch 117/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5753 - accuracy: 0.7743 - val_loss: 0.5850 - val_accuracy: 0.7670 - lr: 6.1035e-08\n",
      "Epoch 118/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5716 - accuracy: 0.7759 - val_loss: 0.5850 - val_accuracy: 0.7659 - lr: 3.0518e-08\n",
      "Epoch 119/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5720 - accuracy: 0.7740 - val_loss: 0.5847 - val_accuracy: 0.7683 - lr: 3.0518e-08\n",
      "Epoch 120/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5706 - accuracy: 0.7766 - val_loss: 0.5842 - val_accuracy: 0.7663 - lr: 3.0518e-08\n",
      "Epoch 121/1000\n",
      "1111/1111 [==============================] - 31s 27ms/step - loss: 0.5747 - accuracy: 0.7746 - val_loss: 0.5843 - val_accuracy: 0.7668 - lr: 3.0518e-08\n",
      "Epoch 122/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5691 - accuracy: 0.7779 - val_loss: 0.5845 - val_accuracy: 0.7670 - lr: 3.0518e-08\n",
      "Epoch 123/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5729 - accuracy: 0.7764 - val_loss: 0.5845 - val_accuracy: 0.7656 - lr: 1.5259e-08\n",
      "Epoch 124/1000\n",
      "1111/1111 [==============================] - 32s 28ms/step - loss: 0.5692 - accuracy: 0.7769 - val_loss: 0.5846 - val_accuracy: 0.7677 - lr: 1.5259e-08\n",
      "Epoch 125/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5674 - accuracy: 0.7775 - val_loss: 0.5845 - val_accuracy: 0.7659 - lr: 1.5259e-08\n",
      "Epoch 126/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5707 - accuracy: 0.7787 - val_loss: 0.5837 - val_accuracy: 0.7677 - lr: 1.5259e-08\n",
      "Epoch 127/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5720 - accuracy: 0.7738 - val_loss: 0.5845 - val_accuracy: 0.7665 - lr: 1.5259e-08\n",
      "Epoch 128/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5752 - accuracy: 0.7769 - val_loss: 0.5849 - val_accuracy: 0.7670 - lr: 1.5259e-08\n",
      "Epoch 129/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5687 - accuracy: 0.7774 - val_loss: 0.5841 - val_accuracy: 0.7663 - lr: 1.5259e-08\n",
      "Epoch 130/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5717 - accuracy: 0.7745 - val_loss: 0.5843 - val_accuracy: 0.7670 - lr: 1.5259e-08\n",
      "Epoch 131/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5714 - accuracy: 0.7749 - val_loss: 0.5843 - val_accuracy: 0.7677 - lr: 1.5259e-08\n",
      "Epoch 132/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5693 - accuracy: 0.7782 - val_loss: 0.5849 - val_accuracy: 0.7681 - lr: 7.6294e-09\n",
      "Epoch 133/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5705 - accuracy: 0.7766 - val_loss: 0.5842 - val_accuracy: 0.7661 - lr: 7.6294e-09\n",
      "Epoch 134/1000\n",
      "1111/1111 [==============================] - 32s 29ms/step - loss: 0.5723 - accuracy: 0.7746 - val_loss: 0.5848 - val_accuracy: 0.7652 - lr: 7.6294e-09\n",
      "Epoch 135/1000\n",
      "1111/1111 [==============================] - 31s 27ms/step - loss: 0.5726 - accuracy: 0.7754 - val_loss: 0.5843 - val_accuracy: 0.7672 - lr: 7.6294e-09\n",
      "Epoch 136/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5658 - accuracy: 0.7745 - val_loss: 0.5842 - val_accuracy: 0.7670 - lr: 7.6294e-09\n",
      "Epoch 137/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5736 - accuracy: 0.7756 - val_loss: 0.5848 - val_accuracy: 0.7656 - lr: 3.8147e-09\n",
      "Epoch 138/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5734 - accuracy: 0.7724 - val_loss: 0.5834 - val_accuracy: 0.7686 - lr: 3.8147e-09\n",
      "Epoch 139/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5733 - accuracy: 0.7781 - val_loss: 0.5851 - val_accuracy: 0.7701 - lr: 3.8147e-09\n",
      "Epoch 140/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5688 - accuracy: 0.7781 - val_loss: 0.5848 - val_accuracy: 0.7668 - lr: 3.8147e-09\n",
      "Epoch 141/1000\n",
      "1111/1111 [==============================] - 31s 27ms/step - loss: 0.5663 - accuracy: 0.7769 - val_loss: 0.5840 - val_accuracy: 0.7679 - lr: 3.8147e-09\n",
      "Epoch 142/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5725 - accuracy: 0.7782 - val_loss: 0.5841 - val_accuracy: 0.7663 - lr: 3.8147e-09\n",
      "Epoch 143/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5710 - accuracy: 0.7768 - val_loss: 0.5845 - val_accuracy: 0.7665 - lr: 3.8147e-09\n",
      "Epoch 144/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5694 - accuracy: 0.7749 - val_loss: 0.5846 - val_accuracy: 0.7665 - lr: 1.9073e-09\n",
      "Epoch 145/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5719 - accuracy: 0.7759 - val_loss: 0.5845 - val_accuracy: 0.7665 - lr: 1.9073e-09\n",
      "Epoch 146/1000\n",
      "1111/1111 [==============================] - 31s 27ms/step - loss: 0.5716 - accuracy: 0.7763 - val_loss: 0.5844 - val_accuracy: 0.7661 - lr: 1.9073e-09\n",
      "Epoch 147/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5709 - accuracy: 0.7770 - val_loss: 0.5844 - val_accuracy: 0.7665 - lr: 1.9073e-09\n",
      "Epoch 148/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5690 - accuracy: 0.7758 - val_loss: 0.5844 - val_accuracy: 0.7659 - lr: 1.9073e-09\n",
      "Epoch 149/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5726 - accuracy: 0.7777 - val_loss: 0.5848 - val_accuracy: 0.7663 - lr: 9.5367e-10\n",
      "Epoch 150/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5727 - accuracy: 0.7777 - val_loss: 0.5852 - val_accuracy: 0.7656 - lr: 9.5367e-10\n",
      "Epoch 151/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5674 - accuracy: 0.7777 - val_loss: 0.5845 - val_accuracy: 0.7668 - lr: 9.5367e-10\n",
      "Epoch 152/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5726 - accuracy: 0.7746 - val_loss: 0.5848 - val_accuracy: 0.7654 - lr: 9.5367e-10\n",
      "Epoch 153/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5689 - accuracy: 0.7771 - val_loss: 0.5849 - val_accuracy: 0.7670 - lr: 9.5367e-10\n",
      "Epoch 154/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5736 - accuracy: 0.7772 - val_loss: 0.5846 - val_accuracy: 0.7668 - lr: 4.7684e-10\n",
      "Epoch 155/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5726 - accuracy: 0.7738 - val_loss: 0.5861 - val_accuracy: 0.7663 - lr: 4.7684e-10\n",
      "Epoch 156/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5712 - accuracy: 0.7763 - val_loss: 0.5840 - val_accuracy: 0.7677 - lr: 4.7684e-10\n",
      "Epoch 157/1000\n",
      "1111/1111 [==============================] - 31s 27ms/step - loss: 0.5690 - accuracy: 0.7773 - val_loss: 0.5839 - val_accuracy: 0.7677 - lr: 4.7684e-10\n",
      "Epoch 158/1000\n",
      "1111/1111 [==============================] - 31s 27ms/step - loss: 0.5684 - accuracy: 0.7756 - val_loss: 0.5855 - val_accuracy: 0.7674 - lr: 4.7684e-10\n",
      "Epoch 159/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5707 - accuracy: 0.7731 - val_loss: 0.5854 - val_accuracy: 0.7656 - lr: 2.3842e-10\n",
      "Epoch 160/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5682 - accuracy: 0.7776 - val_loss: 0.5844 - val_accuracy: 0.7663 - lr: 2.3842e-10\n",
      "Epoch 161/1000\n",
      "1111/1111 [==============================] - 31s 27ms/step - loss: 0.5713 - accuracy: 0.7757 - val_loss: 0.5847 - val_accuracy: 0.7656 - lr: 2.3842e-10\n",
      "Epoch 162/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5708 - accuracy: 0.7749 - val_loss: 0.5850 - val_accuracy: 0.7665 - lr: 2.3842e-10\n",
      "Epoch 163/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5699 - accuracy: 0.7769 - val_loss: 0.5851 - val_accuracy: 0.7656 - lr: 2.3842e-10\n",
      "Epoch 164/1000\n",
      "1111/1111 [==============================] - 31s 27ms/step - loss: 0.5720 - accuracy: 0.7746 - val_loss: 0.5848 - val_accuracy: 0.7668 - lr: 1.1921e-10\n",
      "Epoch 165/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5728 - accuracy: 0.7744 - val_loss: 0.5856 - val_accuracy: 0.7652 - lr: 1.1921e-10\n",
      "Epoch 166/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5693 - accuracy: 0.7752 - val_loss: 0.5842 - val_accuracy: 0.7670 - lr: 1.1921e-10\n",
      "Epoch 167/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5708 - accuracy: 0.7755 - val_loss: 0.5841 - val_accuracy: 0.7665 - lr: 1.1921e-10\n",
      "Epoch 168/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5685 - accuracy: 0.7766 - val_loss: 0.5850 - val_accuracy: 0.7674 - lr: 1.1921e-10\n",
      "Epoch 169/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5691 - accuracy: 0.7780 - val_loss: 0.5846 - val_accuracy: 0.7670 - lr: 5.9605e-11\n",
      "Epoch 170/1000\n",
      "1111/1111 [==============================] - 31s 27ms/step - loss: 0.5701 - accuracy: 0.7775 - val_loss: 0.5846 - val_accuracy: 0.7683 - lr: 5.9605e-11\n",
      "Epoch 171/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5745 - accuracy: 0.7744 - val_loss: 0.5845 - val_accuracy: 0.7665 - lr: 5.9605e-11\n",
      "Epoch 172/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5715 - accuracy: 0.7764 - val_loss: 0.5849 - val_accuracy: 0.7656 - lr: 5.9605e-11\n",
      "Epoch 173/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5692 - accuracy: 0.7775 - val_loss: 0.5844 - val_accuracy: 0.7661 - lr: 5.9605e-11\n",
      "Epoch 174/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5727 - accuracy: 0.7759 - val_loss: 0.5840 - val_accuracy: 0.7681 - lr: 2.9802e-11\n",
      "Epoch 175/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5709 - accuracy: 0.7751 - val_loss: 0.5849 - val_accuracy: 0.7668 - lr: 2.9802e-11\n",
      "Epoch 176/1000\n",
      "1111/1111 [==============================] - 32s 28ms/step - loss: 0.5696 - accuracy: 0.7786 - val_loss: 0.5847 - val_accuracy: 0.7663 - lr: 2.9802e-11\n",
      "Epoch 177/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5704 - accuracy: 0.7778 - val_loss: 0.5844 - val_accuracy: 0.7656 - lr: 2.9802e-11\n",
      "Epoch 178/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5746 - accuracy: 0.7758 - val_loss: 0.5843 - val_accuracy: 0.7668 - lr: 2.9802e-11\n",
      "Epoch 179/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5721 - accuracy: 0.7770 - val_loss: 0.5845 - val_accuracy: 0.7665 - lr: 1.4901e-11\n",
      "Epoch 180/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5672 - accuracy: 0.7782 - val_loss: 0.5847 - val_accuracy: 0.7661 - lr: 1.4901e-11\n",
      "Epoch 181/1000\n",
      "1111/1111 [==============================] - 31s 27ms/step - loss: 0.5731 - accuracy: 0.7758 - val_loss: 0.5842 - val_accuracy: 0.7679 - lr: 1.4901e-11\n",
      "Epoch 182/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5715 - accuracy: 0.7776 - val_loss: 0.5845 - val_accuracy: 0.7656 - lr: 1.4901e-11\n",
      "Epoch 183/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5732 - accuracy: 0.7777 - val_loss: 0.5846 - val_accuracy: 0.7677 - lr: 1.4901e-11\n",
      "Epoch 184/1000\n",
      "1111/1111 [==============================] - 31s 28ms/step - loss: 0.5657 - accuracy: 0.7762 - val_loss: 0.5847 - val_accuracy: 0.7659 - lr: 7.4506e-12\n",
      "Epoch 185/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5691 - accuracy: 0.7795 - val_loss: 0.5845 - val_accuracy: 0.7670 - lr: 7.4506e-12\n",
      "Epoch 186/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5690 - accuracy: 0.7754 - val_loss: 0.5852 - val_accuracy: 0.7661 - lr: 7.4506e-12\n",
      "Epoch 187/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5729 - accuracy: 0.7774 - val_loss: 0.5846 - val_accuracy: 0.7663 - lr: 7.4506e-12\n",
      "Epoch 188/1000\n",
      "1111/1111 [==============================] - 30s 27ms/step - loss: 0.5728 - accuracy: 0.7760 - val_loss: 0.5842 - val_accuracy: 0.7668 - lr: 7.4506e-12\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
    "\n",
    "\n",
    "l2_recurrent_parameter = 0.01\n",
    "l_2_kernel_regularizer= 0.01\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Masking(mask_value=0., input_shape=(Xpad_train.shape[1], Xpad_train.shape[2])))\n",
    "model.add(LSTM(256, return_sequences=True,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=True,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=True,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(128,activation=\"relu\"))\n",
    "model.add(LSTM(256, return_sequences=False,\n",
    "               kernel_regularizer=l2(l_2_kernel_regularizer), recurrent_regularizer=l2(l2_recurrent_parameter)))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "# Compilación del modelo\n",
    "with tf.device('GPU:0'): # Usar gpu\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=\"models\\Full\",\n",
    "#     monitor='val_accuracy',\n",
    "#     mode='max',\n",
    "#     save_best_only=True)\n",
    "# Callbacks\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=50, restore_best_weights=True)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=10^(-9))\n",
    "\n",
    "history = model.fit(Xpad_train, ypad_train, verbose=1, epochs=1000, batch_size=16,\n",
    "                    validation_data=(Xpad_test, ypad_test), callbacks=[early_stopping, reduce_lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "beee1850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-10\n",
      "139/139 [==============================] - 5s 9ms/step - loss: 1.2446 - accuracy: 0.4500\n",
      "10-15\n",
      "139/139 [==============================] - 1s 8ms/step - loss: 1.0686 - accuracy: 0.4858\n",
      "15-20\n",
      "139/139 [==============================] - 1s 8ms/step - loss: 1.0168 - accuracy: 0.5115\n",
      "20-25\n",
      "139/139 [==============================] - 1s 8ms/step - loss: 0.9922 - accuracy: 0.5304\n",
      "25-30\n",
      "139/139 [==============================] - 1s 9ms/step - loss: 0.9668 - accuracy: 0.5520\n",
      "30-35\n",
      "139/139 [==============================] - 1s 9ms/step - loss: 0.9446 - accuracy: 0.5608\n",
      "35-40\n",
      "139/139 [==============================] - 1s 8ms/step - loss: 0.9197 - accuracy: 0.5734\n",
      "40-45\n",
      "139/139 [==============================] - 1s 9ms/step - loss: 0.8924 - accuracy: 0.5882\n",
      "45-50\n",
      "139/139 [==============================] - 1s 9ms/step - loss: 0.8634 - accuracy: 0.6074\n",
      "50-55\n",
      "139/139 [==============================] - 1s 9ms/step - loss: 0.8249 - accuracy: 0.6391\n",
      "55-60\n",
      "139/139 [==============================] - 1s 9ms/step - loss: 0.7827 - accuracy: 0.6670\n",
      "60-65\n",
      "139/139 [==============================] - 1s 9ms/step - loss: 0.7408 - accuracy: 0.6959\n",
      "65-70\n",
      "139/139 [==============================] - 1s 10ms/step - loss: 0.6970 - accuracy: 0.7220\n",
      "70-75\n",
      "139/139 [==============================] - 1s 9ms/step - loss: 0.6469 - accuracy: 0.7571\n",
      "75-80\n",
      "139/139 [==============================] - 1s 9ms/step - loss: 0.5782 - accuracy: 0.7927\n",
      "80-85\n",
      "139/139 [==============================] - 1s 9ms/step - loss: 0.5088 - accuracy: 0.8307\n",
      "85-90\n",
      "139/139 [==============================] - 1s 10ms/step - loss: 0.3977 - accuracy: 0.8805\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[92], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m loss, acc \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, XFullpad_test_normalized\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mintervals\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m      4\u001b[0m     aux \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(XFullpad_test_normalized[:, :i, :], ypad_test)\n\u001b[0;32m      5\u001b[0m     loss\u001b[38;5;241m.\u001b[39mappend(aux[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "loss, acc = [], []\n",
    "for i in range(1, XFullpad_test_normalized.shape[1]+1):\n",
    "    print(intervals[i])\n",
    "    aux = model.evaluate(XFullpad_test_normalized[:, :i, :], ypad_test)\n",
    "    loss.append(aux[0])\n",
    "    acc.append(aux[1])\n",
    "print(np.mean(acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
