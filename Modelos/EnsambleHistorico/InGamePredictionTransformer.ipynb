{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be586fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cb1f3f",
   "metadata": {},
   "source": [
    "# Tratamiento de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e353c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar si hay GPU disponible\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Cargar datos desde archivos .npy\n",
    "X = np.load(\"InGameDataOrdenada\\Space4x3Time5.npy\")\n",
    "y = np.load(\"InGameDataOrdenada\\LabelsSpace4x3Time5.npy\")\n",
    "\n",
    "# Convertir los datos a tensores de PyTorch\n",
    "X = torch.tensor(X, dtype=torch.float32)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Preprocesamiento: Normalización de los datos\n",
    "scaler = MinMaxScaler()\n",
    "n_samples, sequence_length, n_features = X.shape\n",
    "X_flat = X.view(-1, n_features)\n",
    "X_flat = torch.tensor(scaler.fit_transform(X_flat), dtype=torch.float32)\n",
    "X = X_flat.view(n_samples, sequence_length, n_features)\n",
    "\n",
    "# División de los datos en 65% entrenamiento, 15% validación y 20% prueba\n",
    "train_size = int(0.65 * n_samples)\n",
    "val_size = int(0.15 * n_samples)\n",
    "test_size = n_samples - train_size - val_size\n",
    "\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size + val_size], y[train_size:train_size + val_size]\n",
    "X_test, y_test = X[train_size + val_size:], y[train_size + val_size:]\n",
    "\n",
    "# Mover los datos a la GPU si está disponible\n",
    "X_train, X_val, X_test = X_train.to(device), X_val.to(device), X_test.to(device)\n",
    "y_train, y_val, y_test = y_train.to(device), y_val.to(device), y_test.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f10c04",
   "metadata": {},
   "source": [
    "# Simple+L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82517af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_heads=4, num_layers=2, hidden_dim=128, dropout=0.1, sequence_length=18):\n",
    "        super(TransformerClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, hidden_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, sequence_length, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x.mean(dim=1)  # Global average pooling\n",
    "        x = self.fc_out(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675fa5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de los hiperparámetros\n",
    "input_dim = n_features\n",
    "num_classes = len(torch.unique(y))\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "hidden_dim = 256\n",
    "dropout = 0.1\n",
    "sequence_length = 18\n",
    "\n",
    "# Crear el modelo con los nuevos hiperparámetros\n",
    "model = TransformerClassifier(input_dim, num_classes, num_heads, num_layers, hidden_dim, dropout, sequence_length).to(device)\n",
    "\n",
    "# Hiperparámetros de entrenamiento\n",
    "learning_rate = 0.0001\n",
    "batch_size = 8\n",
    "num_epochs = 1000\n",
    "patience = 50  # Para early stopping\n",
    "weight_decay = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Early stopping y guardado del mejor modelo\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Evaluar en el conjunto de validación\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in zip(X_val, y_val):\n",
    "            inputs, labels = inputs.unsqueeze(0), labels.unsqueeze(0)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(X_val)}, Val Acc: {val_acc * 100:.2f}%')\n",
    "\n",
    "    # Guardar el mejor modelo\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Cargar el mejor modelo\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': running_loss,\n",
    "}, 'models/ModeloSimple.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2970cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo en función de la longitud de la secuencia\n",
    "model.eval()\n",
    "accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq_len in range(1, sequence_length + 1):\n",
    "        test_outputs = model(X_test[:, :seq_len, :])\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Sequence Length: {seq_len}, Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630b129c",
   "metadata": {},
   "source": [
    "# Padding + Regularizador L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f6c947",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, max_length, incomplete_prob=0.5):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.max_length = max_length\n",
    "        self.incomplete_prob = incomplete_prob\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        input_seq = self.X[idx]\n",
    "        target_seq = self.y[idx]\n",
    "        seq_length = input_seq.size(0)\n",
    "        \n",
    "        if torch.rand(1).item() < self.incomplete_prob:\n",
    "            seq_length = torch.randint(1, input_seq.size(0), (1,)).item()\n",
    "        \n",
    "        input_seq = input_seq[:seq_length]\n",
    "        target_seq = target_seq\n",
    "        \n",
    "        # Rellenar con padding si es necesario\n",
    "        padded_input_seq = torch.zeros(self.max_length, input_seq.size(1))\n",
    "        padded_input_seq[:seq_length] = input_seq\n",
    "        \n",
    "        return padded_input_seq, target_seq, seq_length\n",
    "\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, base_loss_fn, incomplete_weight=1.0):\n",
    "        super(CustomLoss, self).__init__()\n",
    "        self.base_loss_fn = base_loss_fn\n",
    "        self.incomplete_weight = incomplete_weight\n",
    "    \n",
    "    def forward(self, predictions, targets, sequence_lengths, max_length):\n",
    "        base_loss = self.base_loss_fn(predictions, targets)\n",
    "        \n",
    "        incomplete_mask = (sequence_lengths < max_length).float()\n",
    "        \n",
    "        regularization_loss = incomplete_mask.mean() * self.incomplete_weight\n",
    "        \n",
    "        total_loss = base_loss + regularization_loss\n",
    "        \n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93d84d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de los hiperparámetros\n",
    "input_dim = n_features\n",
    "num_classes = len(torch.unique(y))\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "hidden_dim = 256\n",
    "dropout = 0.1\n",
    "sequence_length = 18\n",
    "\n",
    "# Crear el modelo con los nuevos hiperparámetros\n",
    "model = TransformerClassifier(input_dim, num_classes, num_heads, num_layers, hidden_dim, dropout, sequence_length).to(device)\n",
    "\n",
    "# Hiperparámetros de entrenamiento\n",
    "learning_rate = 0.0001\n",
    "batch_size = 8\n",
    "num_epochs = 1000\n",
    "patience = 50  # Para early stopping\n",
    "incomplete_weight = 1.25\n",
    "weight_decay = 0.01\n",
    "incomplete_prob = 0.75\n",
    "\n",
    "criterion = CustomLoss(nn.CrossEntropyLoss(), incomplete_weight=incomplete_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Early stopping y guardado del mejor modelo\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train, max_length=sequence_length, incomplete_prob=incomplete_prob)\n",
    "val_dataset = CustomDataset(X_val, y_val, max_length=sequence_length, incomplete_prob=incomplete_prob)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels, seq_lengths in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels, seq_lengths, sequence_length)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Evaluar en el conjunto de validación\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, seq_lengths in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels, seq_lengths, sequence_length)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}, Val Acc: {val_acc * 100:.2f}%')\n",
    "\n",
    "    # Guardar el mejor modelo\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    # Ajustar la tasa de aprendizaje basada en la pérdida de validación\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Cargar el mejor modelo\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': running_loss,\n",
    "}, 'models/ModeloPaddingL2.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80dd8489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluación del modelo en función de la longitud de la secuencia\n",
    "model.eval()\n",
    "accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq_len in range(1, sequence_length + 1):\n",
    "        test_outputs = model(X_test[:, :seq_len, :])\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Sequence Length: {seq_len}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a09cc36",
   "metadata": {},
   "source": [
    "# Regularizador LSTM + L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee01ad93",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes, num_heads=4, num_layers=2, hidden_dim=128, lstm_hidden_dim=64, dropout=0.1, sequence_length=18):\n",
    "        super(TransformerLSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, lstm_hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.embedding = nn.Linear(lstm_hidden_dim * 2, hidden_dim)\n",
    "        self.pos_encoder = nn.Parameter(torch.zeros(1, sequence_length, hidden_dim))\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(hidden_dim, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.embedding(x)\n",
    "        x = x + self.pos_encoder[:, :x.size(1), :]\n",
    "        x = self.transformer_encoder(x, src_key_padding_mask=src_key_padding_mask)\n",
    "        x = self.dropout(x.mean(dim=1))  # Global average pooling con dropout\n",
    "        x = self.fc_out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095f907e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración de los hiperparámetros\n",
    "input_dim = n_features\n",
    "num_classes = len(torch.unique(y))\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "hidden_dim = 256\n",
    "lstm_hidden_dim = 128\n",
    "dropout = 0.1\n",
    "sequence_length = 18\n",
    "\n",
    "# Crear el modelo con los nuevos hiperparámetros\n",
    "model = TransformerLSTMClassifier(input_dim, num_classes, num_heads, num_layers, hidden_dim, lstm_hidden_dim, dropout, sequence_length).to(device)\n",
    "\n",
    "# Hiperparámetros de entrenamiento\n",
    "learning_rate = 0.0001\n",
    "batch_size = 8\n",
    "num_epochs = 1000\n",
    "patience = 50  # Para early stopping\n",
    "weight_decay = 0.01  # Regularización L2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Early stopping y guardado del mejor modelo\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Evaluar en el conjunto de validación\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in zip(X_val, y_val):\n",
    "            inputs, labels = inputs.unsqueeze(0), labels.unsqueeze(0)\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(X_val)}, Val Acc: {val_acc * 100:.2f}%')\n",
    "\n",
    "    # Guardar el mejor modelo\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    # Adjust learning rate based on validation loss\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Cargar el mejor modelo\n",
    "model.load_state_dict(best_model_wts)\n",
    "\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': running_loss,\n",
    "}, 'models/ModeloLTSML2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af241cb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Length: 1, Accuracy: 28.99%\n",
      "Sequence Length: 2, Accuracy: 34.10%\n",
      "Sequence Length: 3, Accuracy: 37.43%\n",
      "Sequence Length: 4, Accuracy: 41.44%\n",
      "Sequence Length: 5, Accuracy: 44.86%\n",
      "Sequence Length: 6, Accuracy: 48.26%\n",
      "Sequence Length: 7, Accuracy: 51.65%\n",
      "Sequence Length: 8, Accuracy: 55.03%\n",
      "Sequence Length: 9, Accuracy: 57.87%\n",
      "Sequence Length: 10, Accuracy: 61.81%\n",
      "Sequence Length: 11, Accuracy: 65.61%\n",
      "Sequence Length: 12, Accuracy: 68.78%\n",
      "Sequence Length: 13, Accuracy: 71.98%\n",
      "Sequence Length: 14, Accuracy: 75.83%\n",
      "Sequence Length: 15, Accuracy: 79.92%\n",
      "Sequence Length: 16, Accuracy: 83.59%\n",
      "Sequence Length: 17, Accuracy: 88.99%\n",
      "Sequence Length: 18, Accuracy: 99.08%\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo en función de la longitud de la secuencia\n",
    "model.eval()\n",
    "accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq_len in range(1, sequence_length + 1):\n",
    "        test_outputs = model(X_test[:, :seq_len, :])\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Sequence Length: {seq_len}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6ba384",
   "metadata": {},
   "source": [
    "# Regularización LSTM + L2 + Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9a169725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000, Loss: 1.9678275684900892, Val Loss: 1.9036529521681245, Val Acc: 50.29%\n",
      "Epoch 2/1000, Loss: 1.7566033567087802, Val Loss: 1.7085897440641014, Val Acc: 64.64%\n",
      "Epoch 3/1000, Loss: 1.664201238776178, Val Loss: 1.7332869194947347, Val Acc: 62.32%\n",
      "Epoch 4/1000, Loss: 1.6566128669187963, Val Loss: 1.6286559091739443, Val Acc: 68.09%\n",
      "Epoch 5/1000, Loss: 1.6363911732164447, Val Loss: 1.7317043379250008, Val Acc: 63.25%\n",
      "Epoch 6/1000, Loss: 1.6377385176119712, Val Loss: 1.605908765009943, Val Acc: 69.35%\n",
      "Epoch 7/1000, Loss: 1.6288631382411207, Val Loss: 1.6090242422455086, Val Acc: 69.08%\n",
      "Epoch 8/1000, Loss: 1.6222143826887547, Val Loss: 1.6460261438942214, Val Acc: 68.00%\n",
      "Epoch 9/1000, Loss: 1.6169576401195367, Val Loss: 1.6501141067225786, Val Acc: 68.39%\n",
      "Epoch 10/1000, Loss: 1.5973092316590518, Val Loss: 1.6163308498354843, Val Acc: 69.08%\n",
      "Epoch 11/1000, Loss: 1.6068216298425626, Val Loss: 1.6298259108114, Val Acc: 68.93%\n",
      "Epoch 12/1000, Loss: 1.5847147391938767, Val Loss: 1.6380358180324752, Val Acc: 67.85%\n",
      "Epoch 13/1000, Loss: 1.593227904985486, Val Loss: 1.6152000113249751, Val Acc: 70.25%\n",
      "Epoch 14/1000, Loss: 1.597862966420578, Val Loss: 1.6148575138749683, Val Acc: 69.23%\n",
      "Epoch 15/1000, Loss: 1.6044314234045405, Val Loss: 1.6277555367184948, Val Acc: 69.65%\n",
      "Epoch 16/1000, Loss: 1.5841602658631069, Val Loss: 1.571478499680885, Val Acc: 71.00%\n",
      "Epoch 17/1000, Loss: 1.5859418007475516, Val Loss: 1.5814389675280796, Val Acc: 71.36%\n",
      "Epoch 18/1000, Loss: 1.5821550445048103, Val Loss: 1.5966835178930288, Val Acc: 68.99%\n",
      "Epoch 19/1000, Loss: 1.5766718283914793, Val Loss: 1.6044674242994292, Val Acc: 70.61%\n",
      "Epoch 20/1000, Loss: 1.5757932484645263, Val Loss: 1.6329908460578275, Val Acc: 68.21%\n",
      "Epoch 21/1000, Loss: 1.5818970241705137, Val Loss: 1.5964489113274543, Val Acc: 70.94%\n",
      "Epoch 22/1000, Loss: 1.57643375320778, Val Loss: 1.6046586708020736, Val Acc: 69.86%\n",
      "Epoch 23/1000, Loss: 1.5703741055446319, Val Loss: 1.5604716034636015, Val Acc: 72.02%\n",
      "Epoch 24/1000, Loss: 1.5757095862623727, Val Loss: 1.5889298672321206, Val Acc: 71.03%\n",
      "Epoch 25/1000, Loss: 1.5690083475159147, Val Loss: 1.5842200910158235, Val Acc: 71.24%\n",
      "Epoch 26/1000, Loss: 1.5587229653408652, Val Loss: 1.57437006956537, Val Acc: 71.36%\n",
      "Epoch 27/1000, Loss: 1.5738527770187716, Val Loss: 1.6507068479281137, Val Acc: 66.59%\n",
      "Epoch 28/1000, Loss: 1.5688587213652285, Val Loss: 1.590786842867037, Val Acc: 69.98%\n",
      "Epoch 29/1000, Loss: 1.5648302301970876, Val Loss: 1.555734425281254, Val Acc: 72.17%\n",
      "Epoch 30/1000, Loss: 1.5596217365311122, Val Loss: 1.5611317420237274, Val Acc: 71.51%\n",
      "Epoch 31/1000, Loss: 1.5700785382467624, Val Loss: 1.5671470097128228, Val Acc: 71.30%\n",
      "Epoch 32/1000, Loss: 1.5569400704633496, Val Loss: 1.6004218437335798, Val Acc: 70.73%\n",
      "Epoch 33/1000, Loss: 1.5722049223914372, Val Loss: 1.5708733254508764, Val Acc: 72.95%\n",
      "Epoch 34/1000, Loss: 1.567238347715288, Val Loss: 1.5695564648712037, Val Acc: 71.84%\n",
      "Epoch 35/1000, Loss: 1.5514193109858399, Val Loss: 1.6223650375144927, Val Acc: 68.27%\n",
      "Epoch 36/1000, Loss: 1.5541314715823968, Val Loss: 1.5739648738380128, Val Acc: 72.95%\n",
      "Epoch 37/1000, Loss: 1.552748192281274, Val Loss: 1.6149447313221201, Val Acc: 69.71%\n",
      "Epoch 38/1000, Loss: 1.5587411677903416, Val Loss: 1.573544406561656, Val Acc: 71.42%\n",
      "Epoch 39/1000, Loss: 1.5626013961855394, Val Loss: 1.5666049145585967, Val Acc: 72.14%\n",
      "Epoch 40/1000, Loss: 1.5564254885234992, Val Loss: 1.5548895345106795, Val Acc: 71.90%\n",
      "Epoch 41/1000, Loss: 1.553615447903604, Val Loss: 1.575575690248502, Val Acc: 72.17%\n",
      "Epoch 42/1000, Loss: 1.5484777917326982, Val Loss: 1.605166699467923, Val Acc: 69.50%\n",
      "Epoch 43/1000, Loss: 1.568341269139768, Val Loss: 1.620867448635103, Val Acc: 70.46%\n",
      "Epoch 44/1000, Loss: 1.54457008173261, Val Loss: 1.5296333842272027, Val Acc: 72.83%\n",
      "Epoch 45/1000, Loss: 1.5556266876964358, Val Loss: 1.5675551827865701, Val Acc: 72.29%\n",
      "Epoch 46/1000, Loss: 1.5526917907669935, Val Loss: 1.5952194187674404, Val Acc: 70.22%\n",
      "Epoch 47/1000, Loss: 1.5528239337030871, Val Loss: 1.5865331067913988, Val Acc: 71.09%\n",
      "Epoch 48/1000, Loss: 1.5477501029618228, Val Loss: 1.5763710650842004, Val Acc: 71.51%\n",
      "Epoch 49/1000, Loss: 1.5594407739731744, Val Loss: 1.5465273392763206, Val Acc: 71.63%\n",
      "Epoch 50/1000, Loss: 1.5449822201788261, Val Loss: 1.571465534729538, Val Acc: 72.41%\n",
      "Epoch 51/1000, Loss: 1.5554620901963718, Val Loss: 1.5542429530833317, Val Acc: 72.50%\n",
      "Epoch 52/1000, Loss: 1.5601159176998192, Val Loss: 1.5610375200956823, Val Acc: 73.07%\n",
      "Epoch 53/1000, Loss: 1.5517439137865632, Val Loss: 1.5580254005419125, Val Acc: 71.96%\n",
      "Epoch 54/1000, Loss: 1.5535772783109025, Val Loss: 1.5558540380727408, Val Acc: 72.08%\n",
      "Epoch 55/1000, Loss: 1.5578724463560574, Val Loss: 1.5549892079969252, Val Acc: 71.57%\n",
      "Epoch 56/1000, Loss: 1.5402518761620296, Val Loss: 1.5526241766845195, Val Acc: 73.70%\n",
      "Epoch 57/1000, Loss: 1.5342825021439972, Val Loss: 1.568858220978026, Val Acc: 72.92%\n",
      "Epoch 58/1000, Loss: 1.546754912689452, Val Loss: 1.5777588905259146, Val Acc: 72.98%\n",
      "Epoch 59/1000, Loss: 1.5369066126954192, Val Loss: 1.5569835386122752, Val Acc: 72.98%\n",
      "Epoch 60/1000, Loss: 1.5376667992560158, Val Loss: 1.572657155471586, Val Acc: 72.71%\n",
      "Epoch 61/1000, Loss: 1.529964747694721, Val Loss: 1.5600876850901517, Val Acc: 72.71%\n",
      "Epoch 62/1000, Loss: 1.5412480472362602, Val Loss: 1.5339014747236521, Val Acc: 73.97%\n",
      "Epoch 63/1000, Loss: 1.544278454417337, Val Loss: 1.6082063356242977, Val Acc: 71.57%\n",
      "Epoch 64/1000, Loss: 1.5356219060863485, Val Loss: 1.571260350864969, Val Acc: 72.59%\n",
      "Epoch 65/1000, Loss: 1.5457989597254513, Val Loss: 1.541479870176279, Val Acc: 74.21%\n",
      "Epoch 66/1000, Loss: 1.5303076833404001, Val Loss: 1.570686000133399, Val Acc: 72.38%\n",
      "Epoch 67/1000, Loss: 1.5340043124069467, Val Loss: 1.5476765958180945, Val Acc: 72.71%\n",
      "Epoch 68/1000, Loss: 1.5252660716339492, Val Loss: 1.5567735544097907, Val Acc: 73.37%\n",
      "Epoch 69/1000, Loss: 1.5339030713420825, Val Loss: 1.548774160381269, Val Acc: 72.68%\n",
      "Epoch 70/1000, Loss: 1.528619084018089, Val Loss: 1.5378468046530662, Val Acc: 73.04%\n",
      "Epoch 71/1000, Loss: 1.5306863373501478, Val Loss: 1.574384160573362, Val Acc: 72.56%\n",
      "Epoch 72/1000, Loss: 1.532961191199823, Val Loss: 1.5430836006038615, Val Acc: 72.98%\n",
      "Epoch 73/1000, Loss: 1.5156887276846287, Val Loss: 1.5363882279962748, Val Acc: 72.68%\n",
      "Epoch 74/1000, Loss: 1.52668825678548, Val Loss: 1.5548233482620772, Val Acc: 72.71%\n",
      "Epoch 75/1000, Loss: 1.5332629879921096, Val Loss: 1.5483638837310958, Val Acc: 73.19%\n",
      "Epoch 76/1000, Loss: 1.5415560851466954, Val Loss: 1.5731249345336553, Val Acc: 73.16%\n",
      "Epoch 77/1000, Loss: 1.5306602241755192, Val Loss: 1.5711648769149513, Val Acc: 72.29%\n",
      "Epoch 78/1000, Loss: 1.5333026769583906, Val Loss: 1.5164268887681827, Val Acc: 73.70%\n",
      "Epoch 79/1000, Loss: 1.530274533300849, Val Loss: 1.5365307250325264, Val Acc: 73.46%\n",
      "Epoch 80/1000, Loss: 1.5206876371623406, Val Loss: 1.5661707326793035, Val Acc: 72.35%\n",
      "Epoch 81/1000, Loss: 1.5428529808726006, Val Loss: 1.565597356679034, Val Acc: 72.32%\n",
      "Epoch 82/1000, Loss: 1.5330428285116635, Val Loss: 1.5659953591294131, Val Acc: 73.37%\n",
      "Epoch 83/1000, Loss: 1.5209034793759977, Val Loss: 1.531814978063657, Val Acc: 73.67%\n",
      "Epoch 84/1000, Loss: 1.5444900215497637, Val Loss: 1.5439286819296822, Val Acc: 73.67%\n",
      "Epoch 85/1000, Loss: 1.5365047117184405, Val Loss: 1.507868701715423, Val Acc: 73.88%\n",
      "Epoch 86/1000, Loss: 1.5232347075958992, Val Loss: 1.5412730280762261, Val Acc: 73.49%\n",
      "Epoch 87/1000, Loss: 1.512390950957824, Val Loss: 1.5486342987503374, Val Acc: 72.74%\n",
      "Epoch 88/1000, Loss: 1.5283424515308106, Val Loss: 1.5695825585076093, Val Acc: 72.80%\n",
      "Epoch 89/1000, Loss: 1.5277434489238295, Val Loss: 1.5444221578384152, Val Acc: 72.59%\n",
      "Epoch 90/1000, Loss: 1.5266330832259476, Val Loss: 1.5380387852191102, Val Acc: 73.16%\n",
      "Epoch 91/1000, Loss: 1.5209769094749832, Val Loss: 1.5414838377906444, Val Acc: 73.82%\n",
      "Epoch 92/1000, Loss: 1.5258718556314295, Val Loss: 1.537500929026624, Val Acc: 73.91%\n",
      "Epoch 93/1000, Loss: 1.5219327732796815, Val Loss: 1.5662653661944352, Val Acc: 73.07%\n",
      "Epoch 94/1000, Loss: 1.5279411492585475, Val Loss: 1.5568479412123029, Val Acc: 72.68%\n",
      "Epoch 95/1000, Loss: 1.524177355003489, Val Loss: 1.5651852505844457, Val Acc: 72.98%\n",
      "Epoch 96/1000, Loss: 1.5426542060857333, Val Loss: 1.5147191445203994, Val Acc: 73.88%\n",
      "Epoch 97/1000, Loss: 1.5049474871389754, Val Loss: 1.5642780800836606, Val Acc: 73.19%\n",
      "Epoch 98/1000, Loss: 1.5251449460798354, Val Loss: 1.5719698711221832, Val Acc: 72.68%\n",
      "Epoch 99/1000, Loss: 1.5305405621713548, Val Loss: 1.550001591792772, Val Acc: 72.68%\n",
      "Epoch 100/1000, Loss: 1.523678568178927, Val Loss: 1.5402158227126832, Val Acc: 73.88%\n",
      "Epoch 101/1000, Loss: 1.5309042786462155, Val Loss: 1.5296692491885737, Val Acc: 73.88%\n",
      "Epoch 102/1000, Loss: 1.5113450832148998, Val Loss: 1.5311429671565493, Val Acc: 73.76%\n",
      "Epoch 103/1000, Loss: 1.5215278393674094, Val Loss: 1.5499454241804156, Val Acc: 72.98%\n",
      "Epoch 104/1000, Loss: 1.4974081050821288, Val Loss: 1.544339313970023, Val Acc: 73.19%\n",
      "Epoch 105/1000, Loss: 1.514001247509695, Val Loss: 1.549982720521064, Val Acc: 72.53%\n",
      "Epoch 106/1000, Loss: 1.5424696940134106, Val Loss: 1.529865283447052, Val Acc: 73.04%\n",
      "Epoch 107/1000, Loss: 1.5293541755867797, Val Loss: 1.574024564869475, Val Acc: 73.13%\n",
      "Epoch 108/1000, Loss: 1.5306350172556669, Val Loss: 1.5564025097452483, Val Acc: 72.68%\n",
      "Epoch 109/1000, Loss: 1.5129765444184935, Val Loss: 1.5387974376322258, Val Acc: 74.27%\n",
      "Epoch 110/1000, Loss: 1.5192361424834444, Val Loss: 1.5333438662641596, Val Acc: 74.12%\n",
      "Epoch 111/1000, Loss: 1.5334885166952814, Val Loss: 1.5327131606984288, Val Acc: 74.00%\n",
      "Epoch 112/1000, Loss: 1.5172697636972174, Val Loss: 1.5189511911779539, Val Acc: 73.73%\n",
      "Epoch 113/1000, Loss: 1.523569017582653, Val Loss: 1.5498243109783538, Val Acc: 72.71%\n",
      "Epoch 114/1000, Loss: 1.5247887050841324, Val Loss: 1.5832504300151673, Val Acc: 72.50%\n",
      "Epoch 115/1000, Loss: 1.529673584534108, Val Loss: 1.5358394790469876, Val Acc: 73.16%\n",
      "Epoch 116/1000, Loss: 1.5152416385772156, Val Loss: 1.5619950957745674, Val Acc: 72.80%\n",
      "Epoch 117/1000, Loss: 1.524240975548356, Val Loss: 1.5479368399405964, Val Acc: 72.77%\n",
      "Epoch 118/1000, Loss: 1.5266920518346772, Val Loss: 1.5685166370069488, Val Acc: 72.65%\n",
      "Epoch 119/1000, Loss: 1.5236055581358332, Val Loss: 1.5478729207962294, Val Acc: 72.50%\n",
      "Epoch 120/1000, Loss: 1.5274004703083197, Val Loss: 1.5211909097914569, Val Acc: 73.25%\n",
      "Epoch 121/1000, Loss: 1.5226856194209524, Val Loss: 1.5526633255212845, Val Acc: 73.01%\n",
      "Epoch 122/1000, Loss: 1.5227979400646654, Val Loss: 1.5517734431321297, Val Acc: 71.93%\n",
      "Epoch 123/1000, Loss: 1.5230514125645656, Val Loss: 1.5257678787932132, Val Acc: 73.70%\n",
      "Epoch 124/1000, Loss: 1.5143782797117313, Val Loss: 1.5117089140791922, Val Acc: 72.83%\n",
      "Epoch 125/1000, Loss: 1.518961556408544, Val Loss: 1.544845689609948, Val Acc: 73.19%\n",
      "Epoch 126/1000, Loss: 1.5209006885247218, Val Loss: 1.5336744689419972, Val Acc: 73.97%\n",
      "Epoch 127/1000, Loss: 1.5175601055747585, Val Loss: 1.5377792479041792, Val Acc: 73.61%\n",
      "Epoch 128/1000, Loss: 1.5113030096335425, Val Loss: 1.5445912490705698, Val Acc: 72.02%\n",
      "Epoch 129/1000, Loss: 1.5222321396058947, Val Loss: 1.551280747258389, Val Acc: 72.62%\n",
      "Epoch 130/1000, Loss: 1.5185296643803985, Val Loss: 1.5347840167470617, Val Acc: 73.58%\n",
      "Epoch 131/1000, Loss: 1.5159705751985724, Val Loss: 1.5428827000621024, Val Acc: 73.22%\n",
      "Epoch 132/1000, Loss: 1.5324492414407123, Val Loss: 1.534182242534345, Val Acc: 72.59%\n",
      "Epoch 133/1000, Loss: 1.5232707629573643, Val Loss: 1.551465717992262, Val Acc: 73.70%\n",
      "Epoch 134/1000, Loss: 1.525511968812784, Val Loss: 1.565357687426155, Val Acc: 72.95%\n",
      "Epoch 135/1000, Loss: 1.5212604405477106, Val Loss: 1.5547397716908697, Val Acc: 72.56%\n",
      "Epoch 136/1000, Loss: 1.5216038023996221, Val Loss: 1.5669915580426939, Val Acc: 72.11%\n",
      "Epoch 137/1000, Loss: 1.5174435601174996, Val Loss: 1.540766820824214, Val Acc: 72.86%\n",
      "Epoch 138/1000, Loss: 1.5159030428553553, Val Loss: 1.5601364922998373, Val Acc: 73.55%\n",
      "Epoch 139/1000, Loss: 1.521665778946018, Val Loss: 1.5341998239628714, Val Acc: 72.98%\n",
      "Epoch 140/1000, Loss: 1.5084753391980465, Val Loss: 1.5520164087167196, Val Acc: 72.98%\n",
      "Epoch 141/1000, Loss: 1.5335119418655405, Val Loss: 1.5594804576543482, Val Acc: 72.50%\n",
      "Epoch 142/1000, Loss: 1.5169642233617417, Val Loss: 1.5517596734125496, Val Acc: 73.10%\n",
      "Epoch 143/1000, Loss: 1.5277172331664701, Val Loss: 1.5880171620838044, Val Acc: 72.92%\n",
      "Epoch 144/1000, Loss: 1.5200861792485139, Val Loss: 1.5252997244630688, Val Acc: 74.30%\n",
      "Epoch 145/1000, Loss: 1.517523346152002, Val Loss: 1.520564012335513, Val Acc: 74.21%\n",
      "Epoch 146/1000, Loss: 1.5110569935777016, Val Loss: 1.5572785575789678, Val Acc: 74.00%\n",
      "Epoch 147/1000, Loss: 1.504170510544341, Val Loss: 1.547901125800028, Val Acc: 72.98%\n",
      "Epoch 148/1000, Loss: 1.5101452442747734, Val Loss: 1.5536973090515487, Val Acc: 72.59%\n",
      "Epoch 149/1000, Loss: 1.511761154139471, Val Loss: 1.5642125047695965, Val Acc: 72.29%\n",
      "Epoch 150/1000, Loss: 1.5124476493395598, Val Loss: 1.5132600074620222, Val Acc: 74.09%\n",
      "Epoch 151/1000, Loss: 1.5192149523057437, Val Loss: 1.5766422932543296, Val Acc: 72.20%\n",
      "Epoch 152/1000, Loss: 1.507826393991296, Val Loss: 1.5481844881822395, Val Acc: 72.29%\n",
      "Epoch 153/1000, Loss: 1.5152148915294794, Val Loss: 1.5327407668687667, Val Acc: 72.98%\n",
      "Epoch 154/1000, Loss: 1.5264686078493614, Val Loss: 1.52634483848151, Val Acc: 73.19%\n",
      "Epoch 155/1000, Loss: 1.5270369785314122, Val Loss: 1.5771202885551592, Val Acc: 71.48%\n",
      "Epoch 156/1000, Loss: 1.512234756969679, Val Loss: 1.5693316430792381, Val Acc: 73.10%\n",
      "Epoch 157/1000, Loss: 1.5206879814741023, Val Loss: 1.5575237556307224, Val Acc: 72.26%\n",
      "Epoch 158/1000, Loss: 1.52842749693222, Val Loss: 1.5470212402927963, Val Acc: 73.49%\n",
      "Epoch 159/1000, Loss: 1.5258382592009705, Val Loss: 1.5379749746442632, Val Acc: 72.62%\n",
      "Epoch 160/1000, Loss: 1.5174369241227073, Val Loss: 1.5540947073933782, Val Acc: 72.68%\n",
      "Epoch 161/1000, Loss: 1.5119893925589538, Val Loss: 1.539260519313325, Val Acc: 72.98%\n",
      "Epoch 162/1000, Loss: 1.527378525686066, Val Loss: 1.559550733176814, Val Acc: 71.87%\n",
      "Epoch 163/1000, Loss: 1.5184509658417213, Val Loss: 1.5446499576827943, Val Acc: 73.64%\n",
      "Epoch 164/1000, Loss: 1.5346193541119963, Val Loss: 1.515639534124007, Val Acc: 74.81%\n",
      "Epoch 165/1000, Loss: 1.517327648814035, Val Loss: 1.5552263739388636, Val Acc: 73.76%\n",
      "Epoch 166/1000, Loss: 1.5180975696551833, Val Loss: 1.550655808566581, Val Acc: 72.50%\n",
      "Epoch 167/1000, Loss: 1.5250588366777282, Val Loss: 1.5302377673878216, Val Acc: 73.28%\n",
      "Epoch 168/1000, Loss: 1.5005035609254547, Val Loss: 1.5418345800959907, Val Acc: 72.56%\n",
      "Epoch 169/1000, Loss: 1.5280311714579193, Val Loss: 1.5605017045904819, Val Acc: 72.95%\n",
      "Epoch 170/1000, Loss: 1.5166880543459815, Val Loss: 1.5179803982027806, Val Acc: 74.60%\n",
      "Epoch 171/1000, Loss: 1.516767243797429, Val Loss: 1.5366966097813262, Val Acc: 73.61%\n",
      "Epoch 172/1000, Loss: 1.5140794963552682, Val Loss: 1.5302254849492756, Val Acc: 73.79%\n",
      "Epoch 173/1000, Loss: 1.5109715408235376, Val Loss: 1.5607312705454617, Val Acc: 72.83%\n",
      "Epoch 174/1000, Loss: 1.5119444370269775, Val Loss: 1.5510939037230822, Val Acc: 72.47%\n",
      "Epoch 175/1000, Loss: 1.5133303948716774, Val Loss: 1.5343940618415246, Val Acc: 73.76%\n",
      "Epoch 176/1000, Loss: 1.5081547255000909, Val Loss: 1.5406038353927836, Val Acc: 73.40%\n",
      "Epoch 177/1000, Loss: 1.536505036638054, Val Loss: 1.5180214193267911, Val Acc: 74.18%\n",
      "Epoch 178/1000, Loss: 1.5226090767376972, Val Loss: 1.5344742496553814, Val Acc: 72.50%\n",
      "Epoch 179/1000, Loss: 1.5279638590740034, Val Loss: 1.5602284817751517, Val Acc: 72.68%\n",
      "Epoch 180/1000, Loss: 1.5245049650649285, Val Loss: 1.5398985906150935, Val Acc: 73.64%\n",
      "Epoch 181/1000, Loss: 1.5346374414964397, Val Loss: 1.5196033100159405, Val Acc: 74.33%\n",
      "Epoch 182/1000, Loss: 1.5130790164596155, Val Loss: 1.5408783209804873, Val Acc: 73.31%\n",
      "Epoch 183/1000, Loss: 1.505777445485057, Val Loss: 1.5063056437397842, Val Acc: 74.00%\n",
      "Epoch 184/1000, Loss: 1.5323030473783075, Val Loss: 1.549233129790852, Val Acc: 73.07%\n",
      "Epoch 185/1000, Loss: 1.509429404197307, Val Loss: 1.5379733589265487, Val Acc: 73.34%\n",
      "Epoch 186/1000, Loss: 1.5223745340950932, Val Loss: 1.5572327094866614, Val Acc: 72.62%\n",
      "Epoch 187/1000, Loss: 1.531396442354551, Val Loss: 1.5325646083620774, Val Acc: 73.49%\n",
      "Epoch 188/1000, Loss: 1.5249773656562424, Val Loss: 1.5481390081522108, Val Acc: 74.00%\n",
      "Epoch 189/1000, Loss: 1.5111929816387366, Val Loss: 1.5149589801779195, Val Acc: 73.58%\n",
      "Epoch 190/1000, Loss: 1.52172042989995, Val Loss: 1.5538264857632884, Val Acc: 73.52%\n",
      "Epoch 191/1000, Loss: 1.5061303388544067, Val Loss: 1.522029396560053, Val Acc: 73.79%\n",
      "Epoch 192/1000, Loss: 1.5262720081284435, Val Loss: 1.5680537371878838, Val Acc: 72.74%\n",
      "Epoch 193/1000, Loss: 1.5085914378846452, Val Loss: 1.54326371325076, Val Acc: 73.58%\n",
      "Epoch 194/1000, Loss: 1.5144584759615796, Val Loss: 1.5372895525676273, Val Acc: 73.94%\n",
      "Epoch 195/1000, Loss: 1.5289048206608051, Val Loss: 1.5428612143786429, Val Acc: 73.10%\n",
      "Epoch 196/1000, Loss: 1.5223249030245307, Val Loss: 1.5159875505510676, Val Acc: 74.36%\n",
      "Epoch 197/1000, Loss: 1.5099838579790743, Val Loss: 1.5367769661697062, Val Acc: 73.91%\n",
      "Epoch 198/1000, Loss: 1.5349336644949345, Val Loss: 1.5054452585724685, Val Acc: 75.14%\n",
      "Epoch 199/1000, Loss: 1.526812623998465, Val Loss: 1.5369267831298226, Val Acc: 72.98%\n",
      "Epoch 200/1000, Loss: 1.5158249954602725, Val Loss: 1.536376227042096, Val Acc: 73.55%\n",
      "Epoch 201/1000, Loss: 1.5276350512398906, Val Loss: 1.530345810749345, Val Acc: 73.22%\n",
      "Epoch 202/1000, Loss: 1.5317852197591617, Val Loss: 1.578350566638968, Val Acc: 72.26%\n",
      "Epoch 203/1000, Loss: 1.5102719044668853, Val Loss: 1.5560593925872659, Val Acc: 73.16%\n",
      "Epoch 204/1000, Loss: 1.5169714729376447, Val Loss: 1.5249242956909776, Val Acc: 73.04%\n",
      "Epoch 205/1000, Loss: 1.5214760464809607, Val Loss: 1.5366944309749146, Val Acc: 74.21%\n",
      "Epoch 206/1000, Loss: 1.5244777361111628, Val Loss: 1.5598468614856233, Val Acc: 72.98%\n",
      "Epoch 207/1000, Loss: 1.5242964924372464, Val Loss: 1.5536735642326245, Val Acc: 73.49%\n",
      "Epoch 208/1000, Loss: 1.509729208625915, Val Loss: 1.5723166433988485, Val Acc: 73.04%\n",
      "Epoch 209/1000, Loss: 1.520875809034152, Val Loss: 1.554495429385621, Val Acc: 72.95%\n",
      "Epoch 210/1000, Loss: 1.5180693025156402, Val Loss: 1.5472988122705111, Val Acc: 72.65%\n",
      "Epoch 211/1000, Loss: 1.5222410895982939, Val Loss: 1.5598172178118066, Val Acc: 72.89%\n",
      "Epoch 212/1000, Loss: 1.5220733947205742, Val Loss: 1.5346840063234553, Val Acc: 73.46%\n",
      "Epoch 213/1000, Loss: 1.5053117945336238, Val Loss: 1.5671730522683625, Val Acc: 72.71%\n",
      "Epoch 214/1000, Loss: 1.5179866290736397, Val Loss: 1.5415451297279896, Val Acc: 73.76%\n",
      "Epoch 215/1000, Loss: 1.5155436360604875, Val Loss: 1.5397962191797367, Val Acc: 73.52%\n",
      "Epoch 216/1000, Loss: 1.5307941871831952, Val Loss: 1.5588483558646353, Val Acc: 72.62%\n",
      "Epoch 217/1000, Loss: 1.5096035064092304, Val Loss: 1.5415519719199653, Val Acc: 73.82%\n",
      "Epoch 218/1000, Loss: 1.528968020753517, Val Loss: 1.558174688824649, Val Acc: 73.58%\n",
      "Epoch 219/1000, Loss: 1.5123255263900492, Val Loss: 1.527586716499644, Val Acc: 73.88%\n",
      "Epoch 220/1000, Loss: 1.5214311086900347, Val Loss: 1.5485801031166013, Val Acc: 73.04%\n",
      "Epoch 221/1000, Loss: 1.5242563472560238, Val Loss: 1.5926454586771606, Val Acc: 71.93%\n",
      "Epoch 222/1000, Loss: 1.527711771300625, Val Loss: 1.5805770593130706, Val Acc: 72.77%\n",
      "Epoch 223/1000, Loss: 1.5243941272890138, Val Loss: 1.5558628984007363, Val Acc: 72.35%\n",
      "Epoch 224/1000, Loss: 1.5057941327134658, Val Loss: 1.5262488066748447, Val Acc: 73.58%\n",
      "Epoch 225/1000, Loss: 1.5241402494114853, Val Loss: 1.5311610954153472, Val Acc: 74.00%\n",
      "Epoch 226/1000, Loss: 1.5040723581723558, Val Loss: 1.5524180383022956, Val Acc: 72.83%\n",
      "Epoch 227/1000, Loss: 1.5350951423935613, Val Loss: 1.562372727236769, Val Acc: 72.92%\n",
      "Epoch 228/1000, Loss: 1.5172859770356784, Val Loss: 1.5435392814136364, Val Acc: 73.07%\n",
      "Epoch 229/1000, Loss: 1.5224915827740593, Val Loss: 1.5558056940569214, Val Acc: 72.71%\n",
      "Epoch 230/1000, Loss: 1.5157602928351828, Val Loss: 1.579417423300973, Val Acc: 72.02%\n",
      "Epoch 231/1000, Loss: 1.5193630320708837, Val Loss: 1.546212620696038, Val Acc: 73.46%\n",
      "Epoch 232/1000, Loss: 1.5159649359139709, Val Loss: 1.5115250046776663, Val Acc: 74.15%\n",
      "Epoch 233/1000, Loss: 1.522030872981634, Val Loss: 1.5585792666695426, Val Acc: 72.86%\n",
      "Epoch 234/1000, Loss: 1.511036112219343, Val Loss: 1.6012931688614185, Val Acc: 72.17%\n",
      "Epoch 235/1000, Loss: 1.5227100799123336, Val Loss: 1.543014207688527, Val Acc: 73.10%\n",
      "Epoch 236/1000, Loss: 1.5214390664714856, Val Loss: 1.5341509565737512, Val Acc: 73.58%\n",
      "Epoch 237/1000, Loss: 1.5168769979410885, Val Loss: 1.530123972232126, Val Acc: 73.73%\n",
      "Epoch 238/1000, Loss: 1.5237407635453666, Val Loss: 1.5244295961655059, Val Acc: 73.97%\n",
      "Epoch 239/1000, Loss: 1.4995636936370025, Val Loss: 1.5603190934193536, Val Acc: 73.22%\n",
      "Epoch 240/1000, Loss: 1.5152085241848743, Val Loss: 1.5441123016782667, Val Acc: 73.34%\n",
      "Epoch 241/1000, Loss: 1.5239466123303549, Val Loss: 1.5448414895449403, Val Acc: 73.55%\n",
      "Epoch 242/1000, Loss: 1.5201794203133465, Val Loss: 1.5488366354501162, Val Acc: 73.88%\n",
      "Epoch 243/1000, Loss: 1.529382755419554, Val Loss: 1.5726980272721691, Val Acc: 72.41%\n",
      "Epoch 244/1000, Loss: 1.5113092951497213, Val Loss: 1.5764401270146635, Val Acc: 72.14%\n",
      "Epoch 245/1000, Loss: 1.5104677337688752, Val Loss: 1.5787614387100357, Val Acc: 71.99%\n",
      "Epoch 246/1000, Loss: 1.516757882450426, Val Loss: 1.5210722892611253, Val Acc: 74.18%\n",
      "Epoch 247/1000, Loss: 1.5087038072852876, Val Loss: 1.546665328696877, Val Acc: 72.53%\n",
      "Epoch 248/1000, Loss: 1.530877438658162, Val Loss: 1.55630303935163, Val Acc: 72.77%\n",
      "Early stopping at epoch 248\n"
     ]
    }
   ],
   "source": [
    "# Configuración de los hiperparámetros\n",
    "input_dim = n_features\n",
    "num_classes = len(torch.unique(y))\n",
    "num_heads = 8\n",
    "num_layers = 4\n",
    "hidden_dim = 256\n",
    "lstm_hidden_dim = 128\n",
    "dropout = 0.1\n",
    "sequence_length = 18\n",
    "\n",
    "# Crear el modelo con los nuevos hiperparámetros\n",
    "model = TransformerLSTMClassifier(input_dim, num_classes, num_heads, num_layers, hidden_dim, lstm_hidden_dim, dropout, sequence_length).to(device)\n",
    "\n",
    "# Hiperparámetros de entrenamiento\n",
    "learning_rate = 0.0001\n",
    "batch_size = 8\n",
    "num_epochs = 1000\n",
    "patience = 50   # Para early stopping\n",
    "incomplete_weight = 1.25\n",
    "weight_decay = 0.01\n",
    "incomplete_prob = 0.75\n",
    "\n",
    "criterion = CustomLoss(nn.CrossEntropyLoss(), incomplete_weight=incomplete_weight)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay = weight_decay)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Early stopping y guardado del mejor modelo\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_acc = 0.0\n",
    "early_stopping_counter = 0\n",
    "\n",
    "train_dataset = CustomDataset(X_train, y_train, max_length=sequence_length, incomplete_prob=incomplete_prob)\n",
    "val_dataset = CustomDataset(X_val, y_val, max_length=sequence_length, incomplete_prob=incomplete_prob)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels, seq_lengths in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels, seq_lengths, sequence_length)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Evaluar en el conjunto de validación\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels, seq_lengths in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels, seq_lengths, sequence_length)\n",
    "            val_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    val_acc = correct / total\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}, Val Loss: {val_loss/len(val_loader)}, Val Acc: {val_acc * 100:.2f}%')\n",
    "\n",
    "    # Guardar el mejor modelo\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        early_stopping_counter = 0\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "    \n",
    "    # Ajustar la tasa de aprendizaje basada en la pérdida de validación\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Early stopping\n",
    "    if early_stopping_counter >= patience:\n",
    "        print(f'Early stopping at epoch {epoch+1}')\n",
    "        break\n",
    "\n",
    "# Cargar el mejor modelo\n",
    "model.load_state_dict(best_model_wts)\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': running_loss,\n",
    "}, 'models/ModeloPaddingLTSML2.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a35e126b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence Length: 1, Accuracy: 46.07%\n",
      "Sequence Length: 2, Accuracy: 46.30%\n",
      "Sequence Length: 3, Accuracy: 47.36%\n",
      "Sequence Length: 4, Accuracy: 48.10%\n",
      "Sequence Length: 5, Accuracy: 49.29%\n",
      "Sequence Length: 6, Accuracy: 48.84%\n",
      "Sequence Length: 7, Accuracy: 50.78%\n",
      "Sequence Length: 8, Accuracy: 53.14%\n",
      "Sequence Length: 9, Accuracy: 56.97%\n",
      "Sequence Length: 10, Accuracy: 60.84%\n",
      "Sequence Length: 11, Accuracy: 64.37%\n",
      "Sequence Length: 12, Accuracy: 67.81%\n",
      "Sequence Length: 13, Accuracy: 70.94%\n",
      "Sequence Length: 14, Accuracy: 74.75%\n",
      "Sequence Length: 15, Accuracy: 78.82%\n",
      "Sequence Length: 16, Accuracy: 83.05%\n",
      "Sequence Length: 17, Accuracy: 88.50%\n",
      "Sequence Length: 18, Accuracy: 98.67%\n"
     ]
    }
   ],
   "source": [
    "# Evaluación del modelo en función de la longitud de la secuencia\n",
    "model.eval()\n",
    "accuracies = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for seq_len in range(1, sequence_length + 1):\n",
    "        test_outputs = model(X_test[:, :seq_len, :])\n",
    "        _, predicted = torch.max(test_outputs, 1)\n",
    "        accuracy = (predicted == y_test).sum().item() / len(y_test)\n",
    "        accuracies.append(accuracy)\n",
    "        print(f'Sequence Length: {seq_len}, Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
