{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f22a11d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mobfot import MobFot\n",
    "from datetime import date, timedelta\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "213d6b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generar_fechas(anio_inicio, anio_fin):\n",
    "  \"\"\"\n",
    "  Genera una lista de fechas en formato YYYYMMDD\n",
    "  entre dos años (inclusive).\n",
    "\n",
    "  Args:\n",
    "    anio_inicio (int): Año de inicio (ej. 2017)\n",
    "    anio_fin (int): Año de fin (ej. 2024)\n",
    "\n",
    "  Returns:\n",
    "    lista_fechas (list): Lista de strings con las fechas.\n",
    "  \"\"\"\n",
    "  lista_fechas = []\n",
    "  fecha_inicio = date(anio_inicio, 1, 1)\n",
    "  fecha_fin = date(anio_fin, 12, 31)\n",
    "  while fecha_inicio <= fecha_fin:\n",
    "    fecha_str = fecha_inicio.strftime(\"%Y%m%d\")\n",
    "    lista_fechas.append(fecha_str)\n",
    "    fecha_inicio += timedelta(days=1)\n",
    "  return lista_fechas\n",
    "\n",
    "# Ejemplo de uso\n",
    "anio_inicio = 2015\n",
    "anio_fin = 2023\n",
    "lista_fechas = generar_fechas(anio_inicio, anio_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "675b121c",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MobFot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7f76d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_matches(date):\n",
    "    client = MobFot()\n",
    "    ids = []\n",
    "    day_matches = client.get_matches_by_date(date)\n",
    "    for leag in range(len(day_matches[\"leagues\"])):\n",
    "        for match_leg in range(len(day_matches[\"leagues\"][leag][\"matches\"])):\n",
    "            ids.append(day_matches[\"leagues\"][leag][\"matches\"][match_leg][\"id\"])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2f2df47b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3287/3287 [03:00<00:00, 18.23it/s]\n"
     ]
    }
   ],
   "source": [
    "workers = 12\n",
    "results = Parallel(n_jobs=workers)(delayed(day_matches)(d) for d in tqdm(lista_fechas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a94d9ae2-f1e7-4d8b-9517-3fc54efdff7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "471436"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = []\n",
    "for r in results:\n",
    "    ids.extend(r)\n",
    "len(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d752b3db-9f62-4d3d-8b0c-e2ffde5f443c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(id):\n",
    "    try : \n",
    "        client = MobFot()\n",
    "        info = client.get_match_details(id)\n",
    "        if info[\"general\"][\"started\"] == False or info[\"general\"][\"finished\"] == False :\n",
    "            print(\"Partido Cancelado\")\n",
    "            return False\n",
    "        data = {\n",
    "            \"id\": [id],\n",
    "            'date': [info[\"general\"][\"matchTimeUTCDate\"][:10]],\n",
    "            'leagueName': [info[\"general\"][\"leagueName\"]],\n",
    "            'homeTeam': [info[\"general\"][\"homeTeam\"][\"name\"]],\n",
    "            'awayTeam': [info[\"general\"][\"awayTeam\"][\"name\"]],\n",
    "            'homeTeam_score': [info[\"header\"][\"teams\"][0][\"score\"]],\n",
    "            'awayTeam_score': [info[\"header\"][\"teams\"][1][\"score\"]],\n",
    "            'homeIdTeam': [info[\"general\"][\"homeTeam\"][\"id\"]],\n",
    "            'awayIdTeam': [info[\"general\"][\"awayTeam\"][\"id\"]],\n",
    "            'parentLeagueId': [info[\"general\"][\"parentLeagueId\"]],\n",
    "            'leagueId': [info[\"general\"][\"leagueId\"]],\n",
    "        }\n",
    "        if info[\"content\"][\"stats\"] != None:\n",
    "            for i in range(len(info[\"content\"][\"stats\"][\"Periods\"][\"All\"][\"stats\"])):\n",
    "                for j in range(len(info[\"content\"][\"stats\"][\"Periods\"][\"All\"][\"stats\"][i][\"stats\"])):\n",
    "                    key = info[\"content\"][\"stats\"][\"Periods\"][\"All\"][\"stats\"][i][\"stats\"][j][\"key\"]\n",
    "                    data[key+\"_home\"] = info[\"content\"][\"stats\"][\"Periods\"][\"All\"][\"stats\"][i][\"stats\"][j][\"stats\"][0]\n",
    "                    data[key+\"_away\"] = info[\"content\"][\"stats\"][\"Periods\"][\"All\"][\"stats\"][i][\"stats\"][j][\"stats\"][1]\n",
    "    \n",
    "        df_id = pd.DataFrame(data)\n",
    "        return df_id\n",
    "    except Exception as error:\n",
    "        print(error)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78812d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▊     | 135/278 [5:00:01<5:17:48, 133.35s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m\n\u001b[0;32m     12\u001b[0m trabajo_por_division \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(filtro)\u001b[38;5;241m/\u001b[39mdivision_global)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m div \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, trabajo_por_division\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 15\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mworkers\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_info\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m                                       \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfiltro\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdivision_global\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdiv\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\u001b[43mdivision_global\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     17\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([i \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m results \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(i, pd\u001b[38;5;241m.\u001b[39mDataFrame)], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df,dfs], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gfuen\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# La primera vez, hay que crear un data frame de 0 y la busqueda con todos los\n",
    "# df = pd.DataFrame()\n",
    "# filtro = ids\n",
    "# Después leer el archivo guardado, y ver que ids faltan.\n",
    "df_completo = pd.read_csv('datos_fotmob_completo.csv')\n",
    "filtro = list(set(ids)-set(df_completo[\"id\"]))\n",
    "random.shuffle(filtro)\n",
    "df = df_completo.head(1)\n",
    "\n",
    "workers = 12\n",
    "division_global = 1000\n",
    "trabajo_por_division = int(len(filtro)/division_global)\n",
    "\n",
    "for div in tqdm(range(1, trabajo_por_division+1)):\n",
    "    results = Parallel(n_jobs=workers)(delayed(get_info)(id)\n",
    "                                       for id in filtro[division_global*(div-1):division_global*div])\n",
    "    dfs = pd.concat([i for i in results if isinstance(i, pd.DataFrame)], ignore_index=True)\n",
    "    df = pd.concat([df,dfs], ignore_index=True)\n",
    "\n",
    "    # Por temas de como funciona trabajar con datos grandes, esto hace que sea más rápido\n",
    "    if div % 5 == 0:\n",
    "        # Unir el trabajo y resetear el work\n",
    "        df_completo = pd.concat([df_completo, df], ignore_index=True)\n",
    "        # Guardar toda la info\n",
    "        df_completo.to_csv('datos_fotmob_completo.csv', header=True, index=False)\n",
    "        # Reinicio del trabajo\n",
    "        df = df_completo.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e5428",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('datos_fotmob_completo.csv')\n",
    "df_sin_duplicados = df.drop_duplicates()\n",
    "df_sin_duplicados = df_sin_duplicados.sort_values(by=\"date\",ignore_index=True)\n",
    "# Guardar la primera corrida de datos scrapeados\n",
    "df_sin_duplicados.to_csv('datos_fotmob_primer_filtro.csv', header=True, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
